[
  {
    "objectID": "Data.html#field-sites",
    "href": "Data.html#field-sites",
    "title": "Field Sites & Data",
    "section": "Field Sites",
    "text": "Field Sites\nThis web-app shows the sites where operate Eddy Covariance towers and Flux Chamber sites in wetlands across the Metro Vancouver.\n\nClick the points to sites to see more information.\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\nView map in separate window\n\nBurns Bog 1 (BB)\n\n\n\nData Preview Graph (CA-DBB) (2014 - *, Delta, BC, Canada)\nDownload Data for CA-DBB from Ameriflux.\nData from the web plots are available here\n\nprefix BB. note that this is raw data – no filtering or QCQA. Database codes are available here.\n\nBB1 Site Photos (also see Dr. Andreas Christen’s previous photos from when he held his position at UBC)\n\n\n\n\n\n\n\n\n\n\n\nBurns Bog 2 (BB2)\n\n\n\nData Preview Graph (CA-DB2) (2019 - *, Delta, BC, Canada)\nDownload Data for CA-DB2 will soon be available from Ameriflux.\nData from the web plots are available here\n\nprefix BB2. note that this is raw data – no filtering or QCQA. Database codes are available here.\n\nBB2 Site Photos\n\n\n\n\n\n\n\n\n\n\n\nBurns Bog Seedling (BBS)\n\n\n\nFlux chambers were installed in spring 2023\nA temporary flux tower was installed in summer 2023\n\nmore info to come\n\n\n\n\n\n\n\n\n\n\n\n\nDelta Salt Marsh (DSM)\n\n\n\nData Preview Graph (CA-DSM) (2021 - *, Delta, BC, Canada)\nData from CA-DSM will soon be available on AmeriFlux.\nData from the web plots are available here\n\nprefix DSM. note that this is raw data – no filtering or QCQA.\n\nDSM Site Photos\n\n\n\n\n\n\n\n\n\n\n\nRichmond Brackish Marsh (RBM)\n\n\n\nData Preview Graph (CA-RBM) (2021 - *, Richmond, BC, Canada)\nData from CA-RBM will soon be available on AmeriFlux.\nData from the web plots are available here\n\nprefix RBM. note that this is raw data – no filtering or QCQA.\n\nRBM Site Photos\n\n\n\n\n\n\n\n\n\n\n\nUBC Climate Station on Totem Field\n\nAccess to custom data download of standard measured meteorological observations (air temperature, humidity, precipitation, soil temperatures, solar irradiance, wind) measured since 1958. Data can be downloaded in .csv format."
  },
  {
    "objectID": "Data.html#code-and-documentation",
    "href": "Data.html#code-and-documentation",
    "title": "Field Sites & Data",
    "section": "Code and Documentation",
    "text": "Code and Documentation\nOur lab github hosts code for flux processing, README docs for our lab, and various other bits of information.\nMore documentation related to maintenance procedures for our sites can be found here."
  },
  {
    "objectID": "People.html",
    "href": "People.html",
    "title": "People",
    "section": "",
    "text": "Name\nDr. Sara Helen Knox\n\n\nPronouns\nShe/Her/Hers\n\n\nEmail\nsara.knox@mcgill.ca\n\n\nGitHub\nhttps://github.com/sknox01\n\n\n\n\n\nI am broadly interested in the impacts of climate variability and land-use change on land-atmosphere exchanges of water, energy, and trace gases. I also seek to understand how ecosystem responses to global change can feedback to slow or accelerate future climate change. My research and training is in micrometeorology, hydrology, and ecosystem ecology. I focus on biosphere-atmosphere interactions in a variety of climates and ecosystems.\n\nEducation\n\nUniversity of California, Berkeley, PhD\n\n\nCarleton University, MSc\n\n\nMcGill University, BSc\n\n\n\n\n\n\n\n\n\n\nName\nRick Ketler - Project Manager (Geography)\n\n\nEmail\nrick.ketler@ubc.ca\n\n\nOffice\nPCMH B1100\n\n\n\n\n\nCurrently Research Projects Manager for UBC Geography Physical Labs. I have designed and installed research sites in terrain ranging from temperate bogs to arctic tundra. I primarily support professors, PDF’s and grad students with physical scientific measurements in the field and in the lab.\n\nEducation\n\nB.Sc. Physics Co-op University of Victoria, B.C.\n\n\n\n\n\n\n\n\n\n\nName\nZoran Nesic - Senior Research Engineer\n\n\nEmail\nzoran.nesic@ubc.ca\n\n\nOffice\nMCML 137\n\n\n\n\n\nI am responsible for management of numerous research and equipment design projects for various faculties and departments at UBC. I design eddy covariance and soil respiration systems that are used at various North American universities as well as by other research institutions. My main research interests are (1) the design of automated measurement systems for long-term environmental measurements and (2) the standardization of measurement and data processing procedures to ensure high quality and reproducibility of research results.\n\nEducation\n\nB.Sc. in Electrical Engineering, University of Belgrade\n\n\nM.A.Sc. in Electrical Engineering, University of British Columbia",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#faculty-and-staff",
    "href": "People.html#faculty-and-staff",
    "title": "People",
    "section": "",
    "text": "Name\nDr. Sara Helen Knox\n\n\nPronouns\nShe/Her/Hers\n\n\nEmail\nsara.knox@mcgill.ca\n\n\nGitHub\nhttps://github.com/sknox01\n\n\n\n\n\nI am broadly interested in the impacts of climate variability and land-use change on land-atmosphere exchanges of water, energy, and trace gases. I also seek to understand how ecosystem responses to global change can feedback to slow or accelerate future climate change. My research and training is in micrometeorology, hydrology, and ecosystem ecology. I focus on biosphere-atmosphere interactions in a variety of climates and ecosystems.\n\nEducation\n\nUniversity of California, Berkeley, PhD\n\n\nCarleton University, MSc\n\n\nMcGill University, BSc\n\n\n\n\n\n\n\n\n\n\nName\nRick Ketler - Project Manager (Geography)\n\n\nEmail\nrick.ketler@ubc.ca\n\n\nOffice\nPCMH B1100\n\n\n\n\n\nCurrently Research Projects Manager for UBC Geography Physical Labs. I have designed and installed research sites in terrain ranging from temperate bogs to arctic tundra. I primarily support professors, PDF’s and grad students with physical scientific measurements in the field and in the lab.\n\nEducation\n\nB.Sc. Physics Co-op University of Victoria, B.C.\n\n\n\n\n\n\n\n\n\n\nName\nZoran Nesic - Senior Research Engineer\n\n\nEmail\nzoran.nesic@ubc.ca\n\n\nOffice\nMCML 137\n\n\n\n\n\nI am responsible for management of numerous research and equipment design projects for various faculties and departments at UBC. I design eddy covariance and soil respiration systems that are used at various North American universities as well as by other research institutions. My main research interests are (1) the design of automated measurement systems for long-term environmental measurements and (2) the standardization of measurement and data processing procedures to ensure high quality and reproducibility of research results.\n\nEducation\n\nB.Sc. in Electrical Engineering, University of Belgrade\n\n\nM.A.Sc. in Electrical Engineering, University of British Columbia",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#current-students-and-postdocs",
    "href": "People.html#current-students-and-postdocs",
    "title": "People",
    "section": "Current Students and PostDocs",
    "text": "Current Students and PostDocs\n\n\n\n\n\n\n\nName\nDr. June Skeeter\n\n\nPosition\nPostdoctoral Scholar\n\n\nPronouns\nThey/Them/Theirs\n\n\nEmail\njune.skeeter@ubc.ca\n\n\nOffice\nGeography Room 127\n\n\nGitHub\nhttps://github.com/June-Skeeter\n\n\n\n\n\nMy name is June. I am a geographer, researcher, and educator who has been living as an uninvited guest on unceded Coast Salish Territory since 2015. I received a PhD in geography from UBC in 2022 and am now working as a postdoctoral researcher for the UBC Micrometeorology group. I study greenhouse gas exchange in wetland ecosystems and associated feedback mechanisms that influence the earth’s climate system. My research employs a variety of methods from different fields, including: micrometeorology, remote sensing, spatial analysis, and machine learning.\n\n\n\n\n\n\n\n\nName\nDr. Joyson Ahongshangbam\n\n\nPosition\nPostdoctoral Scholar\n\n\nEmail\njoyson.ahongshangbam@mcgill.ca\n\n\nOrcid\nhttps://orcid.org/0000-0002-2678-6879\n\n\n\n\n\nI am interested in studying the carbon and water cycle in different ecosystems (forest, urban vegetation and wetlands) using observations such as eddy covariance, sap flow and remote sensing techniques. My research includes understanding the responses of carbon and water dynamics with the land cover change, management activities and climate change (drought and heatwave).\n\nPrevious experience\n\nUniversity of Helsinki, Finland, Postdoctoral researcher\n\nEducation\n\nUniversity of Göttingen, Germany, PhD\n\n\nIndian Institute of Remote sensing, India, Masters\n\n\n\n\n\n\n\n\n\n\nName\nTzu-Yi Lu\n\n\nLevel of Study\nPh.D. Candidate\n\n\n\n\n\nI received my MS in Geography from National Taiwan University in 2017. I am interested in understanding the response of the wetland ecosystem to climate change, especially in quantifying the net exchange of carbon. My previous research investigated the relationship between environmental controls and CO2 flux in low-latitude wetland ecosystems, applying an Artificial Neural Network technique to simulate the variance of carbon exchange by meteorological variables.\n\n\n\n\n\n\n\n\nName\nKatrina Poppe\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nPh.D. Candidate\n\n\nEmail\npoppek@student.ubc.ca\n\n\n\n\n\nI earned an MS in Environmental Science from Western Washington University in 2016 and continued at WWU as a Research Associate for several years. My previous research has focused primarily on blue carbon, studying soil carbon sequestration rates in Pacific Northwest estuaries and in United Arab Emirates mangroves, in addition to monitoring and modeling vegetation and sediment dynamics in relation to estuary restoration and sea level rise. I am currently interested in studying greenhouse gas fluxes in Pacific Northwest tidal wetlands – particularly how they respond to ecosystem restoration and climate change – to ultimately better understand the value of tidal wetland management actions as natural climate solutions.\n\n\n\n\n\n\n\n\nName\nSarah Russell\n\n\nLevel of Study\nPh.D. Student\n\n\n\n\n\nI received a BS in Biological Sciences from Wellesley College in 2017, then worked as an ecosystem ecology field technician and research assistant before moving to Vancouver. I am interested in land-atmosphere carbon dynamics and am particularly interested in quantifying the terrestrial carbon sink. My research at UBC involves modeling greenhouse gas fluxes from restored tidal wetlands in the Sacramento-San Joaquin River Delta.\n\n\n\n\n\n\n\n\nName\nTed Scott\n\n\nPronouns\nHe/Him/His\n\n\nLevel of Study\nPh.D. Student\n\n\nEmail\ntedjs@student.ubc.ca\n\n\n\n\n\nPrior to UBC, while at the University of Minnesota, I earned a BS in Computer Science (1997), then a MS (2000) and PhD in Geophysics (2006) under David Kohlstedt, investigating the dynamic properties of partially molten peridotites. My work helped constrain the composition and behavior of highly molten planetary interiors such as Jupiter’s moon Io. I then worked at Microsoft for several years as a data scientist and engineer, and more recently as a high-school science and math teacher. My current research interests include studying greenhouse gas fluxes in wetland environments. I then synthesize the data collected across multiple sites in various environments to better understand how individually and collectively they can contribute as a natural climate solution. As in my prior PhD, I hope my work can be used to better constrain climate models and lead to sound policy as we address climate change. In my free time, I enjoy trail running and rock climbing.\n\n\n\n\n\n\n\n\nName\nHehan (Zoe) Zhang\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nM.Sc. Student\n\n\nEmail\nhehanzha@student.ubc.ca\n\n\n\n\n\nI received a BSc in Environmental Science from the University of British Columbia in 2022. My undergraduate thesis research investigated the impacts of fire on greenhouse gas (GHG) fluxes (CO2, CH4, and N2O) within different burned zones of a bog. The methodology incorporated chamber measurements and gas chromatography for the analysis. Currently, as a second-year MSc student, I am examining the impacts of seedling removal — a post-fire management practice — on the GHG fluxes (CO2 and CH4) within the 2016 burned zone of the Burns Bog, Delta, BC. This research utilizes a smart chamber with a portable analyzer. Additionally, I am interested in quantifying the net carbon exchange at the ecosystem scale using eddy covariance techniques and understanding carbon dynamics across varying ecosystem types.\n\n\n\n\n\n\n\n\nName\nVanessa Valenti\n\n\nLevel of Study\nM.Sc. Student\n\n\n\n\n\nI earned a Bachelor’s degree in Geography/Environmental Studies from the University of California, Los Angeles in 2019. Before coming to UBC, I worked as a scientific programmer at the NASA Goddard Space Flight Center, providing visualization and computation support to earth system and atmosphere climate models. I am interested in modelling land-atmosphere exchanges and projecting responses of wetland and forested ecosystems to climate change.\n\n\n\n\n\n\n\n\nName\nKelsey McGuire\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nB.Sc. Student\n\n\nEmail\nkmcgu@student.ubc.ca\n\n\n\n\n\nI am currently doing a B.Sc in Geographical Sciences, with concentrations in Climatology and Geographic Information Systems. Within the lab, I am supporting various graduate students with their research on how wetland and tidal sites can act as natural climate solutions, where I will help to write code for data processing, and assist on field site visits. I hope in later years to conduct more research on land-atmosphere exchanges, specifically around the carbon cycle, and find ways to incorporate different GIS technologies to help in visualizing it.\n\n\n\n\n\n\n\n\nName\nHimari Honda\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nB.Sc. Student\n\n\nEmail\nhhonda02@student.ubc.ca\n\n\n\n\n\nI am an undergraduate student at UBC majoring in Geographical Sciences with a concentration in climatology. My work in the lab consists of facilitating graduate students with their research and field visits to the flux towers. I am currently working with Zoe Zhang to pursue further research on the effects of heatwaves on carbon dioxide and methane fluxes at restored peatlands. In the near future, I am interested in studying how greenhouse gas exchange is influenced by varying biophysical and meteorological factors.",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#lab-alumni",
    "href": "People.html#lab-alumni",
    "title": "People",
    "section": "Lab Alumni",
    "text": "Lab Alumni\n\n\n\nDarian Ng\n\n(M.Sc. Student)\n\n\n\n\n\nAdin Litman\n\n(B.Sc. Student)\n\n\n\n\n\n\n\n\nTin Satriawan\n\n(M.Sc. Student)\n\n\n\n\n\nDr. Sung-Ching (Nick) Lee\n\n(Postdoctoral Scholar)\n\n\n\n\n\n\n\n\nMarion Nyberg\n\n(M.Sc. Student)\n\n\n\n\n\nAylin Barreras-Apodaca\n\n(Visiting International M.Sc. Student)\n\n\n\n\n\n\n\n\nNicole Choi\n\n(B.Sc. Student)\n\n\n\n\n\nCristina Mace\n\n(B.Sc. Student)\n\n\n\n\n\n\n\n\nAzumi Konaka\n\n(B.Sc. Student)\n\n\n\n\n\nWeiwen Fu\n\n(B.Sc. Student)",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "Meetings.html",
    "href": "Meetings.html",
    "title": "Lab Meetings",
    "section": "",
    "text": "Lab meetings are held weekly on Mondays from 12:30-1:30 Pacific time, the schedule is listed below. You can sign up for a day/topic using the google sheet link (contact june for access).\n\n\n\n\nTable 1: Schedule for Fall 2023 Lab meetings.\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nLead\nTopic\n4\n\n\n\n\nJan 12\nJune\nDiscussing WTD corrections at BB\nNA\n\n\nJan 19\n–\nQuick Check-in\nNA\n\n\nJan 26\nJune\nEddy Pro Automation Overview\nNA\n\n\nFeb 2\n–\nQuick Check-in\nNA\n\n\nFeb 9\nZoe\nHow to recompute chamber fluxes + Growing season analysis\nNA\n\n\nFeb 16\n–\nQuick Check-in\nNA\n\n\nFeb 23\nNo Lab Meeting\nReading Break\nNA\n\n\nMar 1\nTed\nprelim gap-filling via SSA results 🤞\nNA\n\n\nMar 8\nNo Lab Meeting\nNA\nNA\n\n\nMar 15\nVanessa\nprobably research proposal discussion/feedback\nNA\n\n\nMar 22\nTzu-Yi\nEGU Practice Presentation\nNA\n\n\nMar 29\nNo Lab Meeting\nGood Friday\nNA\n\n\nApr 5\n–\nQuick Check-in\nNA\n\n\nApr 12\nKatrina\npossibly soil sequestration results\nNA\n\n\nApr 19\nJoyson\nWetland cooling potential - first results\nNA\n\n\nApr 26\n–\nQuick Check-in\nNA\n\n\nMay 3\n–\nQuick Check-in\nNA\n\n\nMay 10\nSarah R\nparitioning results\nNA\n\n\nMay 17\nJune\nQuick Check-in\nNA\n\n\nMay 24\nZoe\nCGU Practice Presentation\nNA\n\n\nMay 31\n–\nQuick Check-in\nVanessa and Tzu-Yi away (field school TA)",
    "crumbs": [
      "Home",
      "Lab Meetings"
    ]
  },
  {
    "objectID": "Join.html",
    "href": "Join.html",
    "title": "Join the Lab!",
    "section": "",
    "text": "Thank you for your interest in joining our lab! I am always happy to hear from students and postdocs interested in our ongoing research activities.\nPlease contact me by email (sara.knox@ubc.ca) for more details. The deadline for applications in geography is mid-December for MSc students and early January for PhD students, however, I encourage you to contact me well before this deadline. Also, please keep in mind the deadlines for external funding opportunities through NSERC. While graduate student support at UBC is available through teaching assistantships and UBC fellowships, I strongly encourage prospective students to apply for external funding.",
    "crumbs": [
      "Home",
      "Join the lab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_Data_Visualization.html",
    "href": "PipelineDocumentation/7_Data_Visualization.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Previously, we briefly mentioned some ways to visualize your data (section 5.1). This current section gives details on our recommended methods for viewing and analyzing your data, both during cleaning, and once you have real-time data flowing.\n\n\nThere are several Matlab functions for visualizing data:\n\nplotApp: a Matlab graphical user interface (GUI) which we recommend using during database set up and cleaning of your data (see section 7.1 for details).\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons. A working example is provided in section 5.1.\n\nguiPlotTraces: this is an older function with less utility than the plotApp GUI, but it still displays your data for quick viewing.\n\n\n\n\nThere is also an R Shiny App which is useful for viewing your flux data in real-time (see section 7.2 for details). You can host this on an RShiny server to make your data publicly viewable.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_Data_Visualization.html#data-visualization-apps",
    "href": "PipelineDocumentation/7_Data_Visualization.html#data-visualization-apps",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Previously, we briefly mentioned some ways to visualize your data (section 5.1). This current section gives details on our recommended methods for viewing and analyzing your data, both during cleaning, and once you have real-time data flowing.\n\n\nThere are several Matlab functions for visualizing data:\n\nplotApp: a Matlab graphical user interface (GUI) which we recommend using during database set up and cleaning of your data (see section 7.1 for details).\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons. A working example is provided in section 5.1.\n\nguiPlotTraces: this is an older function with less utility than the plotApp GUI, but it still displays your data for quick viewing.\n\n\n\n\nThere is also an R Shiny App which is useful for viewing your flux data in real-time (see section 7.2 for details). You can host this on an RShiny server to make your data publicly viewable.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_Quick_Start_Create_Database.html",
    "href": "PipelineDocumentation/5_Quick_Start_Create_Database.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section contains quick-start instructions on how to create an initial database from your raw data, which can subsequently be used in the pipeline.\n\n\nSample_Data_For_Tutorial \nDownload the sample data above, unzip the contents, and keep them handy.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "href": "PipelineDocumentation/5_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section contains quick-start instructions on how to create an initial database from your raw data, which can subsequently be used in the pipeline.\n\n\nSample_Data_For_Tutorial \nDownload the sample data above, unzip the contents, and keep them handy.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_Motivation.html",
    "href": "PipelineDocumentation/1_Motivation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Collecting and handling eddy covariance data presents challenges on many front, including site and instrument selection, high frequency data processing, data post-processing, and QA/QC. While there are many resources online resources that can help with all of these different challenges of eddy covariance measurements (see examples below), here we focus on the approach that the EcoFlux Lab uses for flux data post-processing and QA/QC. Here we provide a detailed outline of our procedures for data post-processing QA/QC, gap-filling, and CO2 flux partitioning. We also provide data visualization tools. We draw on expertise from regional networks and FLUXNET, and leverage widely used tools in the field such as REddyProc and other recently developed machine learning-based gap-filling resources . By following these procedures, you will be able to develop a robust data pipeline to ensure high quality data that you can submit to your regional flux networks. Data standardization and reproducibility helps minimize errors, enhances data reliability, and enables the integration of data sets from diverse ecosystems into global networks like FLUXNET.\nNote that in this pipeline, we are focused on data post-processing, rather than the processing of high frequency (e.g., 20 Hz) data. While we do provide resources on high frequency flux processing, this is not within the main scope of this data pipeline. For more information on site set-up, instrument selection, and processing of high frequency data, please contact Xiangmin Sun from the FLUXNET CH4 and N2O processing committee.\nWhile the data pipeline described here requires some coding knowledge, the whole pipeline can be run without having to modify any existing code. You only need to edit some initialization files, and then execute the code.\nAdditional resources:\n\nFlux Data Post-Processing and QA/QC\nFluxcourse Educational Materials\nVideos on the FLUXNET website",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_Motivation.html#motivation-the-importance-of-flux-data-standardization-and-reproducibility",
    "href": "PipelineDocumentation/1_Motivation.html#motivation-the-importance-of-flux-data-standardization-and-reproducibility",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Collecting and handling eddy covariance data presents challenges on many front, including site and instrument selection, high frequency data processing, data post-processing, and QA/QC. While there are many resources online resources that can help with all of these different challenges of eddy covariance measurements (see examples below), here we focus on the approach that the EcoFlux Lab uses for flux data post-processing and QA/QC. Here we provide a detailed outline of our procedures for data post-processing QA/QC, gap-filling, and CO2 flux partitioning. We also provide data visualization tools. We draw on expertise from regional networks and FLUXNET, and leverage widely used tools in the field such as REddyProc and other recently developed machine learning-based gap-filling resources . By following these procedures, you will be able to develop a robust data pipeline to ensure high quality data that you can submit to your regional flux networks. Data standardization and reproducibility helps minimize errors, enhances data reliability, and enables the integration of data sets from diverse ecosystems into global networks like FLUXNET.\nNote that in this pipeline, we are focused on data post-processing, rather than the processing of high frequency (e.g., 20 Hz) data. While we do provide resources on high frequency flux processing, this is not within the main scope of this data pipeline. For more information on site set-up, instrument selection, and processing of high frequency data, please contact Xiangmin Sun from the FLUXNET CH4 and N2O processing committee.\nWhile the data pipeline described here requires some coding knowledge, the whole pipeline can be run without having to modify any existing code. You only need to edit some initialization files, and then execute the code.\nAdditional resources:\n\nFlux Data Post-Processing and QA/QC\nFluxcourse Educational Materials\nVideos on the FLUXNET website",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_1_Matlab_plotApp.html",
    "href": "PipelineDocumentation/7_1_Matlab_plotApp.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "plotApp is a Matlab graphical user interface (GUI) made to facilitate visualization of database traces as:\n(i) Basic time series;\n(ii) Statistical summary;\n(iii) Comparison amongst traces; and\n(iv) Comparisons amongst stages.\nWe recommend using this plotApp during database creation and data cleaning.\n\n\n\nIn Matlab, run plotApp on the command line (figure 7.1).\n\nFigure 7.1. How to launch the plotApp GUI in Matlab.\nThe GUI in figure 7.2 will launch; specify the folder containing the database you would like to plot by typing the folder path or by clicking the folder button and navigating to the desired folder.\n\nYou need to specify the \\Database\\ folder (immediately beneath your project folder) rather than the individual year or site folder.\nThe Year and Site will be chosen from the drop-down menus which are automatically populated based on what is available in the folder.\nIf you type in the folder path manually or paste it into the Folder field, press ‘Enter’ when complete.\nOn subsequent uses of plotApp, it will remember the last Folder that was entered.\n\n\nFigure 7.2. plotApp GUI showing where to enter the Database folder to view and analyze data.\nNext, select the Year, Site, and Data type (Flux, Met, Clean, etc.) that you want to plot.\nFlux and Met data type are associated with Raw and Stage1 traces. Clean data type is associated with Stage2 and higher.\nOnce these selections are made, the ‘Traces’ list will populate; then click on a trace to select it and plot the time series (figure 7.3).\n\nFigure 7.3. Example of plotApp GUI showing time series plotted after selecting data.\nYou can use the Matlab window navigation tools (i.e., zoom, pan, data tips) to explore the traces.\nTwo traces can be plotted at once for quick and easy reference, or for qualitative visual comparisons.\n\n\n\n\nThe Trace analysis drop-down menu has four options:\n(i) Single trace;\n(ii) Plot parents;\n(iii) Compare traces; and\n(iv) Compare Stages (coming soon…).\n\nOptions (i), (ii), and (iv) operate on a single trace, which is defined by the ‘Trace’ selector  (i.e., 1 or 2, upper or lower panel, respectively).\nOption (iv) compares the selected trace with its previous stage.\nOptions (iii) operates on both Trace 1 and 2.\n\nSingle Trace\nA basic summary of a single trace can be plotted using the Trace analysis drop-down menu.\n\nFor the single trace summary, first select Trace ‘1’ (upper panel) or ‘2’ (lower panel) using the up/down buttons.\nFrom the Trace analysis drop-down menu, select ‘Single trace’.\nA new window will open with the trace summary (figure 7.4).\n\nFigure 7.4. Example of single-trace summary in plotApp GUI.\n\nThe four panel ’Single trace’ summary includes (i) upper left – raw half-hourly data and daily average; (ii) upper right – diurnal trend using boxplots on an hourly basis; (iii) lower left – Q-Q plot; and (iv) lower right – cumulative distribution with 1st and 99th percentile, mean, and median printed.\n\nPanels (ii)–(iv) are a summary of the raw data in panel (i).\nUsers can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\n\nCompare traces\nA basic comparison of Trace ‘1’ and ‘2’ can be plotted using the Trace analysis drop-down menu and selecting ‘Compare traces’.\n\nA new window will open with the trace comparison (figure 7.5).\nAny two traces can be compared. There are no restrictions on Site, Data type or Level, but currently the Year must be the same for each trace.\nAn error message will appear if only one trace is plotted or if Year doesn’t match.\n\nFigure 7.5. Example of ‘Compare trace’ summary in plotApp GUI.\n\nThe four panel ‘Plot comparison’ summary includes (i) upper left – raw half-hourly residuals and their daily average; (ii) upper right – normalized cross-covariance for +/- 48 measurement intervals; (iii) scatter plot comparing the two traces; and (iv) lower right – cumulative distribution of the residuals.\n\nAs for the single-trace analysis, users can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\nPanel (ii) displays the lag with the highest cross-covariance between the two traces for data visible in panel (i).\nPanel (iii) displays the slope and intercept of the linear regression between the two traces, along with the r2 value for data visible in panel (i).\nPanel (iv) displays the root mean squared error (RMSE), mean absolute deviation (MAD), mean, and median of the residuals in panel (i).\n\nPlot parents\n\nThe Trace analysis option is meant to aid the assessment of data cleaning amongst stages. For example, if more data is removed due to data cleaning than expected, Plot parents can be used to help identify which parent trace resulted in data removal.\nThe ‘Trace’ selector (i.e., 1 or 2) defines which trace will be analyzed and displays the analysis in a new window (figure 7.6).\n\nFigure 7.6. Example of ‘plot parents’ display in plotApp GUI.\nFor a particular stage, ‘Plot parents’ shows the data from the current stage (orange circles, upper panel) and contrasts with the previous stage (i.e. blue circles, upper panel).\nThe minMax=[] bounds for the trace defined in the .ini file for the particular site are shown in the upper panel in yellow and purple.\nThe lower panel shows all the parents of a particular trace (i.e., those traces which include that trace as a dependent=’’ in the INI file). Note, dependents are often called using ‘tags’.\nIn the lower panel, 0 - SELF refers to data which are missing before cleaning based on dependents.\nFor all other parents, a large dot indicates a point which was removed from the selected trace which was not missing in the previous stage. A small dot indicates the data from the selected trace would be removed based on the parent, but was already missing from the previous stage.\nUsers can interact with the figure using the Matlab navigation tools (e.g., zoom and pan) to view a subset of the data.\n\nIt is recommended that you only zoom/pan the top panel.\nThe x-axis of the top and bottom panels are linked, so navigating in one panel will automatically update the x-axis on the corresponding panel.\n\n\nCompare traces\nThis option in Trace analysis is intended to quickly assess what changes occurred between stages for a particular trace.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization",
      "7.1 &nbsp; Matlab plotApp"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_1_Matlab_plotApp.html#matlab-plotapp-for-in-depth-analysis",
    "href": "PipelineDocumentation/7_1_Matlab_plotApp.html#matlab-plotapp-for-in-depth-analysis",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "plotApp is a Matlab graphical user interface (GUI) made to facilitate visualization of database traces as:\n(i) Basic time series;\n(ii) Statistical summary;\n(iii) Comparison amongst traces; and\n(iv) Comparisons amongst stages.\nWe recommend using this plotApp during database creation and data cleaning.\n\n\n\nIn Matlab, run plotApp on the command line (figure 7.1).\n\nFigure 7.1. How to launch the plotApp GUI in Matlab.\nThe GUI in figure 7.2 will launch; specify the folder containing the database you would like to plot by typing the folder path or by clicking the folder button and navigating to the desired folder.\n\nYou need to specify the \\Database\\ folder (immediately beneath your project folder) rather than the individual year or site folder.\nThe Year and Site will be chosen from the drop-down menus which are automatically populated based on what is available in the folder.\nIf you type in the folder path manually or paste it into the Folder field, press ‘Enter’ when complete.\nOn subsequent uses of plotApp, it will remember the last Folder that was entered.\n\n\nFigure 7.2. plotApp GUI showing where to enter the Database folder to view and analyze data.\nNext, select the Year, Site, and Data type (Flux, Met, Clean, etc.) that you want to plot.\nFlux and Met data type are associated with Raw and Stage1 traces. Clean data type is associated with Stage2 and higher.\nOnce these selections are made, the ‘Traces’ list will populate; then click on a trace to select it and plot the time series (figure 7.3).\n\nFigure 7.3. Example of plotApp GUI showing time series plotted after selecting data.\nYou can use the Matlab window navigation tools (i.e., zoom, pan, data tips) to explore the traces.\nTwo traces can be plotted at once for quick and easy reference, or for qualitative visual comparisons.\n\n\n\n\nThe Trace analysis drop-down menu has four options:\n(i) Single trace;\n(ii) Plot parents;\n(iii) Compare traces; and\n(iv) Compare Stages (coming soon…).\n\nOptions (i), (ii), and (iv) operate on a single trace, which is defined by the ‘Trace’ selector  (i.e., 1 or 2, upper or lower panel, respectively).\nOption (iv) compares the selected trace with its previous stage.\nOptions (iii) operates on both Trace 1 and 2.\n\nSingle Trace\nA basic summary of a single trace can be plotted using the Trace analysis drop-down menu.\n\nFor the single trace summary, first select Trace ‘1’ (upper panel) or ‘2’ (lower panel) using the up/down buttons.\nFrom the Trace analysis drop-down menu, select ‘Single trace’.\nA new window will open with the trace summary (figure 7.4).\n\nFigure 7.4. Example of single-trace summary in plotApp GUI.\n\nThe four panel ’Single trace’ summary includes (i) upper left – raw half-hourly data and daily average; (ii) upper right – diurnal trend using boxplots on an hourly basis; (iii) lower left – Q-Q plot; and (iv) lower right – cumulative distribution with 1st and 99th percentile, mean, and median printed.\n\nPanels (ii)–(iv) are a summary of the raw data in panel (i).\nUsers can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\n\nCompare traces\nA basic comparison of Trace ‘1’ and ‘2’ can be plotted using the Trace analysis drop-down menu and selecting ‘Compare traces’.\n\nA new window will open with the trace comparison (figure 7.5).\nAny two traces can be compared. There are no restrictions on Site, Data type or Level, but currently the Year must be the same for each trace.\nAn error message will appear if only one trace is plotted or if Year doesn’t match.\n\nFigure 7.5. Example of ‘Compare trace’ summary in plotApp GUI.\n\nThe four panel ‘Plot comparison’ summary includes (i) upper left – raw half-hourly residuals and their daily average; (ii) upper right – normalized cross-covariance for +/- 48 measurement intervals; (iii) scatter plot comparing the two traces; and (iv) lower right – cumulative distribution of the residuals.\n\nAs for the single-trace analysis, users can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\nPanel (ii) displays the lag with the highest cross-covariance between the two traces for data visible in panel (i).\nPanel (iii) displays the slope and intercept of the linear regression between the two traces, along with the r2 value for data visible in panel (i).\nPanel (iv) displays the root mean squared error (RMSE), mean absolute deviation (MAD), mean, and median of the residuals in panel (i).\n\nPlot parents\n\nThe Trace analysis option is meant to aid the assessment of data cleaning amongst stages. For example, if more data is removed due to data cleaning than expected, Plot parents can be used to help identify which parent trace resulted in data removal.\nThe ‘Trace’ selector (i.e., 1 or 2) defines which trace will be analyzed and displays the analysis in a new window (figure 7.6).\n\nFigure 7.6. Example of ‘plot parents’ display in plotApp GUI.\nFor a particular stage, ‘Plot parents’ shows the data from the current stage (orange circles, upper panel) and contrasts with the previous stage (i.e. blue circles, upper panel).\nThe minMax=[] bounds for the trace defined in the .ini file for the particular site are shown in the upper panel in yellow and purple.\nThe lower panel shows all the parents of a particular trace (i.e., those traces which include that trace as a dependent=’’ in the INI file). Note, dependents are often called using ‘tags’.\nIn the lower panel, 0 - SELF refers to data which are missing before cleaning based on dependents.\nFor all other parents, a large dot indicates a point which was removed from the selected trace which was not missing in the previous stage. A small dot indicates the data from the selected trace would be removed based on the parent, but was already missing from the previous stage.\nUsers can interact with the figure using the Matlab navigation tools (e.g., zoom and pan) to view a subset of the data.\n\nIt is recommended that you only zoom/pan the top panel.\nThe x-axis of the top and bottom panels are linked, so navigating in one panel will automatically update the x-axis on the corresponding panel.\n\n\nCompare traces\nThis option in Trace analysis is intended to quickly assess what changes occurred between stages for a particular trace.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization",
      "7.1 &nbsp; Matlab plotApp"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_2_Download_Biomet_Library.html",
    "href": "PipelineDocumentation/2_2_Download_Biomet_Library.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The Biomet.net repository contains libraries of computer code (in Matlab, R, and Python) related to running the data cleaning pipeline. It contains (a) the scripts to run the first, second, and third cleaning stages, as well as conversion to AmeriFlux format, and (b) many functions to help analyse and visualize data throughout all the stages of data processing and cleaning.\nIn principle, users should not need to edit this code library. Instead, as far as possible, you will interface with it using various configuration files unique to your own project(s) and site(s). This process is described later.\nFirst, you need to either download or clone the repository. Instructions for both are given below; if you do not have Git installed on your computer and do not anticipate adding any of your own code to the Biomet.net library, then you can download the repository, as follows:\n\n\n\nGo to the Biomet.net library webpage.\nClick on the green “Code” button, and then click “Download ZIP”:\n\nScreenshot of Biomet.net repository on Github, showing how to download.\nMove the downloaded Biomet.net-main.zip file to a convenient location on your computer, e.g., within your C: drive for PCs, or within /Users/&lt;username&gt;/ for Macs.\nImportantly, after unzipping the file, rename the unzipped directory from Biomet.net-main to Biomet.net.\nWe recommend repeating these steps periodically so that you remain up to date with our pipeline developments, keeping only the most recently downloaded Biomet.net folder on your computer. It should always be in the same location so that Matlab knows where to look for the library (see section 2.4 for details).\n\n\nAlternatively, if you prefer to clone the directory, you will need to have Git installed on your computer (section 2.1), then you can follow these instructions:\n\n\n\n\nMake sure you are in the directory (folder) where you wish the repository to live, for example, your computer C: drive for PCs, or /Users/&lt;username&gt;/ for Macs, or something similar depending on your preferences.\nNext, follow the instructions on this website to clone the repository.\nIf you have any difficulties or are unsure of git procedures in general, see this online tutorial.\nAlways keep your local copy of the Biomet.net code up to date by periodically using the git pull command every few days, or when you know there has been a change to the code. You must be in the repository directory for this to work.\n\nNOTE: Editing existing files or saving new ones to the main branch of Biomet.net should generally be avoided. Reminder: if you wish to contribute your own code to the Biomet.net library, see section 2.1.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.2 &nbsp; Download Biomet Library"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_2_Download_Biomet_Library.html#download-or-clone-the-biomet.net-library-git-repository-to-your-local-computer",
    "href": "PipelineDocumentation/2_2_Download_Biomet_Library.html#download-or-clone-the-biomet.net-library-git-repository-to-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The Biomet.net repository contains libraries of computer code (in Matlab, R, and Python) related to running the data cleaning pipeline. It contains (a) the scripts to run the first, second, and third cleaning stages, as well as conversion to AmeriFlux format, and (b) many functions to help analyse and visualize data throughout all the stages of data processing and cleaning.\nIn principle, users should not need to edit this code library. Instead, as far as possible, you will interface with it using various configuration files unique to your own project(s) and site(s). This process is described later.\nFirst, you need to either download or clone the repository. Instructions for both are given below; if you do not have Git installed on your computer and do not anticipate adding any of your own code to the Biomet.net library, then you can download the repository, as follows:\n\n\n\nGo to the Biomet.net library webpage.\nClick on the green “Code” button, and then click “Download ZIP”:\n\nScreenshot of Biomet.net repository on Github, showing how to download.\nMove the downloaded Biomet.net-main.zip file to a convenient location on your computer, e.g., within your C: drive for PCs, or within /Users/&lt;username&gt;/ for Macs.\nImportantly, after unzipping the file, rename the unzipped directory from Biomet.net-main to Biomet.net.\nWe recommend repeating these steps periodically so that you remain up to date with our pipeline developments, keeping only the most recently downloaded Biomet.net folder on your computer. It should always be in the same location so that Matlab knows where to look for the library (see section 2.4 for details).\n\n\nAlternatively, if you prefer to clone the directory, you will need to have Git installed on your computer (section 2.1), then you can follow these instructions:\n\n\n\n\nMake sure you are in the directory (folder) where you wish the repository to live, for example, your computer C: drive for PCs, or /Users/&lt;username&gt;/ for Macs, or something similar depending on your preferences.\nNext, follow the instructions on this website to clone the repository.\nIf you have any difficulties or are unsure of git procedures in general, see this online tutorial.\nAlways keep your local copy of the Biomet.net code up to date by periodically using the git pull command every few days, or when you know there has been a change to the code. You must be in the repository directory for this to work.\n\nNOTE: Editing existing files or saving new ones to the main branch of Biomet.net should generally be avoided. Reminder: if you wish to contribute your own code to the Biomet.net library, see section 2.1.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.2 &nbsp; Download Biomet Library"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html",
    "href": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Before completing this step, you must have the Biomet.net repository cloned to your local computer (section 2.2). This step is important for ensuring the pipeline scripts and tools run successfully on your local computer.\n\n\n\nOnce Biomet.net is successfully cloned, open Matlab.\nIn “Home” tab, click “Set Path” menu option. Figure 2.1 shows what your screen should be displaying when you do this, for both Windows and Mac users:\n\nFigure 2.1. View of the Set Path menu option, for (a) Windows and (b) Mac.\n\nSelect “Add Folder…” and navigate to Biomet.net/matlab/Startup. Click “Select Folder” (PC) or “Open” (Mac):\n\nFigure 2.2. View of Add Folder to Path menu option, adding the Startup directory, for (a) Windows and (b) Mac.\n\nAt this point the Matlab path should look very similar to Figure 2.3: only one folder in the path does not belong to Matlab nor the /user/Documents/Matlab folder, and that is the Startup folder you just added.\nNext, click “Save” to save these settings for the future. We recommend that no other folders are saved in the default path to avoid unintentional consequences when running the data cleaning. If the saving fails, if possible you should restart Matlab in admin mode and repeat the process (it is often a permissions issue at this point; see Troubleshooting section 8 for more information).\n\nFigure 2.3. View of Matlab path after adding the Startup directory but before restarting Matlab, for (a) Windows and (b) Mac.\n\nNow, you can restart Matlab. Following the restart, check the Matlab path (click “Set Path”) and it should have the Biomet.net library included, as shown in Figure 2.4. In some instances you might also see “UBC_PC_Setup” but this may not appear in all installations - this will be automatically added only when required. Once confirmed, you can close this window, taking care not to change anything further.\n\nFigure 2.4. View of Matlab path after adding the Startup directory, and after restarting Matlab, for (a) Windows and (b) Mac.\n\n\nMatlab is now ready to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.4 &nbsp; Configure Matlab for Biomet"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html#configure-matlab-for-biomet.net-library",
    "href": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html#configure-matlab-for-biomet.net-library",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Before completing this step, you must have the Biomet.net repository cloned to your local computer (section 2.2). This step is important for ensuring the pipeline scripts and tools run successfully on your local computer.\n\n\n\nOnce Biomet.net is successfully cloned, open Matlab.\nIn “Home” tab, click “Set Path” menu option. Figure 2.1 shows what your screen should be displaying when you do this, for both Windows and Mac users:\n\nFigure 2.1. View of the Set Path menu option, for (a) Windows and (b) Mac.\n\nSelect “Add Folder…” and navigate to Biomet.net/matlab/Startup. Click “Select Folder” (PC) or “Open” (Mac):\n\nFigure 2.2. View of Add Folder to Path menu option, adding the Startup directory, for (a) Windows and (b) Mac.\n\nAt this point the Matlab path should look very similar to Figure 2.3: only one folder in the path does not belong to Matlab nor the /user/Documents/Matlab folder, and that is the Startup folder you just added.\nNext, click “Save” to save these settings for the future. We recommend that no other folders are saved in the default path to avoid unintentional consequences when running the data cleaning. If the saving fails, if possible you should restart Matlab in admin mode and repeat the process (it is often a permissions issue at this point; see Troubleshooting section 8 for more information).\n\nFigure 2.3. View of Matlab path after adding the Startup directory but before restarting Matlab, for (a) Windows and (b) Mac.\n\nNow, you can restart Matlab. Following the restart, check the Matlab path (click “Set Path”) and it should have the Biomet.net library included, as shown in Figure 2.4. In some instances you might also see “UBC_PC_Setup” but this may not appear in all installations - this will be automatically added only when required. Once confirmed, you can close this window, taking care not to change anything further.\n\nFigure 2.4. View of Matlab path after adding the Startup directory, and after restarting Matlab, for (a) Windows and (b) Mac.\n\n\nMatlab is now ready to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.4 &nbsp; Configure Matlab for Biomet"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html",
    "href": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The third stage cleaning generally requires the least amount of work by the user, but is usually the most computationally intensive stage, as it includes running models for gap-filling fluxes and flux partitioning. The following example assumes you have already completed first and second stage cleaning for one site.\n\nOpen your site-specific SITEID1_config.yml for editing (figure 6.5):\n\nFigure 6.5. Directory tree showing location of third stage custom YAML file that must be copied (green highlighted text) and edited (yellow highlighted text).\nAt the top of your site-specific configuration file (i.e., SITEID1_config.yml), input the site ID, the year that measurements at the site began, and the metadata for your site (figure 6.6; yellow highlighted text):\n\nFigure 6.6. Third stage site-specific custom YAML file showing which fields to edit in yellow highlighted text.\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the TraceAnalysis_ini directory, and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs; these site-specific settings will overwrite those in the global_config.yml if they are also defined there.\nNote on gap-filling FCH4: currently the predictors for all random forest models used to fill gaps are set to: Predictors: SW_IN_1_1_1,TA_1_1_1,VPD_1_1_1. However, for FCH4, these inputs should be changed to prioritize soil variables such as soil temperature, soil moisture, and water table depth. You can change these settings under “Optional parameters” (figure 6.7; peach highlighting). \n\nFigure 6.7. Third stage site-specific custom YAML file showing where to change inputs for FCH4 random forest gap-filling.\nNext, test the third stage data cleaning in Matlab; remember that it can take a lot longer to run than first and second stages. Note that the cleaning stage argument for third stage cleaning is 7 (not 3; this is a legacy artifact), as follows:\n fr_automated_cleaning(yearIn,siteID,7)  % third stage\nThe output will appear in two directories: ThirdStage and ThirdStage_Default_Ustar within the Clean directory, where the second stage output is; again, we recommend that you inspect this data using the visualization tools. \n\n\n\nThe standalone flux variable names (i.e., FCH4, FC, H, LE) are copied directly from the second stage output, then wind sector and precipitation filters are applied to comply with eddy-covariance measurement theory, with no change to the variable name. For the variable names with suffixes following the flux variables, these suffixes represent different algorithms that we have applied sequentially, in the order that they appear. For now, this description provides only definitions, and more detailed information on each output variable will be provided on this webpage soon.\n\n\n\n\n\n\n\nSuffix\nDefinition\n\n\n\n\n_PI_SC\nStorage flux Correction\n\n\n_PI_SC_JSZ\nPlus z-score filter\n\n\n_PI_SC_JSZ_MAD\nPlus Median of Absolute Deviation (about the median) filter\n\n\n_PI_SC_JSZ_MAD_RP\nPlus REddyProc applied (u-star filtering)\n\n\n\nThis link provides descriptions of suffixes applied to REddyProc output, e.g., _uStar, U95, _orig, and _f.\n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux:\nfr_automated_cleaning(yearIn,SITEID,8)  % Ameriflux CSV output\nThe output will appear in an Ameriflux directory within the Clean directory, where the second and third stage output is.\nNote that you can run all stages at once (or a subset, provided the previous stages to the subset have already been run, i.e., the data exists):\nfr_automated_cleaning(yearIn,SITEID,[1 2 7 8])  % all three stages plus Ameriflux output",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.3 &nbsp; Quick Start: Third Stage and Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html#quick-start-third-stage-cleaning-and-converting-to-ameriflux-output",
    "href": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html#quick-start-third-stage-cleaning-and-converting-to-ameriflux-output",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The third stage cleaning generally requires the least amount of work by the user, but is usually the most computationally intensive stage, as it includes running models for gap-filling fluxes and flux partitioning. The following example assumes you have already completed first and second stage cleaning for one site.\n\nOpen your site-specific SITEID1_config.yml for editing (figure 6.5):\n\nFigure 6.5. Directory tree showing location of third stage custom YAML file that must be copied (green highlighted text) and edited (yellow highlighted text).\nAt the top of your site-specific configuration file (i.e., SITEID1_config.yml), input the site ID, the year that measurements at the site began, and the metadata for your site (figure 6.6; yellow highlighted text):\n\nFigure 6.6. Third stage site-specific custom YAML file showing which fields to edit in yellow highlighted text.\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the TraceAnalysis_ini directory, and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs; these site-specific settings will overwrite those in the global_config.yml if they are also defined there.\nNote on gap-filling FCH4: currently the predictors for all random forest models used to fill gaps are set to: Predictors: SW_IN_1_1_1,TA_1_1_1,VPD_1_1_1. However, for FCH4, these inputs should be changed to prioritize soil variables such as soil temperature, soil moisture, and water table depth. You can change these settings under “Optional parameters” (figure 6.7; peach highlighting). \n\nFigure 6.7. Third stage site-specific custom YAML file showing where to change inputs for FCH4 random forest gap-filling.\nNext, test the third stage data cleaning in Matlab; remember that it can take a lot longer to run than first and second stages. Note that the cleaning stage argument for third stage cleaning is 7 (not 3; this is a legacy artifact), as follows:\n fr_automated_cleaning(yearIn,siteID,7)  % third stage\nThe output will appear in two directories: ThirdStage and ThirdStage_Default_Ustar within the Clean directory, where the second stage output is; again, we recommend that you inspect this data using the visualization tools. \n\n\n\nThe standalone flux variable names (i.e., FCH4, FC, H, LE) are copied directly from the second stage output, then wind sector and precipitation filters are applied to comply with eddy-covariance measurement theory, with no change to the variable name. For the variable names with suffixes following the flux variables, these suffixes represent different algorithms that we have applied sequentially, in the order that they appear. For now, this description provides only definitions, and more detailed information on each output variable will be provided on this webpage soon.\n\n\n\n\n\n\n\nSuffix\nDefinition\n\n\n\n\n_PI_SC\nStorage flux Correction\n\n\n_PI_SC_JSZ\nPlus z-score filter\n\n\n_PI_SC_JSZ_MAD\nPlus Median of Absolute Deviation (about the median) filter\n\n\n_PI_SC_JSZ_MAD_RP\nPlus REddyProc applied (u-star filtering)\n\n\n\nThis link provides descriptions of suffixes applied to REddyProc output, e.g., _uStar, U95, _orig, and _f.\n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux:\nfr_automated_cleaning(yearIn,SITEID,8)  % Ameriflux CSV output\nThe output will appear in an Ameriflux directory within the Clean directory, where the second and third stage output is.\nNote that you can run all stages at once (or a subset, provided the previous stages to the subset have already been run, i.e., the data exists):\nfr_automated_cleaning(yearIn,SITEID,[1 2 7 8])  % all three stages plus Ameriflux output",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.3 &nbsp; Quick Start: Third Stage and Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_6_Install_Python_Optional.html",
    "href": "PipelineDocumentation/2_6_Install_Python_Optional.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. Python can be used in a number of eddy covariance data packages (e.g., some of these flux data post-processing and QA/QC resources).\nPython is also necessary for those interested in running high-frequency data reprocessing described in this EddyPro API or gap-filling fluxes using the approach described in the paper “Gap-filling eddy covariance methane fluxes: Comparison of machine learning model predictions and uncertainties at FLUXNET-CH4 wetlands”.\nHowever, you can skip this step if you do no plan to use any of the resources above.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.6 &nbsp; Install Software: Python (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_6_Install_Python_Optional.html#python-installation-on-your-local-computer-optional",
    "href": "PipelineDocumentation/2_6_Install_Python_Optional.html#python-installation-on-your-local-computer-optional",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. Python can be used in a number of eddy covariance data packages (e.g., some of these flux data post-processing and QA/QC resources).\nPython is also necessary for those interested in running high-frequency data reprocessing described in this EddyPro API or gap-filling fluxes using the approach described in the paper “Gap-filling eddy covariance methane fluxes: Comparison of machine learning model predictions and uncertainties at FLUXNET-CH4 wetlands”.\nHowever, you can skip this step if you do no plan to use any of the resources above.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.6 &nbsp; Install Software: Python (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html",
    "href": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to start cleaning your data for the first stage. Note that the example assumes you have already created Flux and Met databases for the same site.\n\n\n\n\nOpen your site-specific SITEID1_FirstStage.ini file for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better), your site ID, and the timezone information. Difference_GMT_to_local_time is the timezone that your site is located in, and Timezone is the timezone that your data is stored in (UTC, standard time, or daylight saving time). Both these parameters are currently necessary in order to output the correct timezone for Ameriflux requirements (local standard time).\nNext, scroll down (skipping “Global Variables” sections for now) to where you see:\n %----------------------------------------------------\n %--&gt; Insert met variables here\n %----------------------------------------------------\nBeginning with air temperature, define the input data to each trace given in the INI file. For example, if you have an output variable that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 6.2; yellow highlighted text). During this first cleaning stage this trace will be renamed as TA_1_1_1 using the variableName parameter, following the Ameriflux naming convention.\n\nFigure 6.2. Air temperature trace (TA_1_1_1) as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab’s datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs). \n\nOnce you have done this for a few variables, test the data cleaning in Matlab, using the fr_automated_cleaning.m function as follows:\n fr_automated_cleaning(yearIn,'SITEID',1)\nwhere yearIn is a numeric array containing the year(s) of data you want to clean (it is possible to clean multiple years at once but we will keep things simple with one year for now), SITEID would be SITEID1 from the earlier example, and ‘1’ represents the first stage of data cleaning.\nProvided everything worked with no errors, you should now see output data matching the variables you defined in a new Clean folder within your Met folder.\n\n\n\nIf you do NOT have a four-component net radiometer such as a CNR4 at your site, you can skip all of step 5 and go to step 6. If you do have a CNR4 or similar, we have provided an “include” INI file that loads relevant radiation variables for you. We strongly advise that you do not edit this or any other include file. Instead, scroll to the bottom of your site-specific INI file where the include files are called.\n\nUncomment* (e.g., delete % - NOT #) the line containing RAD_FirstStage_include.ini, so that the block of code looks like this:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n%#include EddyPro_Common_FirstStage_include.ini\n%#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n%#include EddyPro_LI7700_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\n*Recall that this file is read by Matlab, so comments are created using %; the # sign must be present to call the include file.\nNext, scroll almost to the top of the file where you see:\n%----------------------------------------------------------\n% Global variable specification (trace-specific)\n%----------------------------------------------------------\n%--&gt; Radiation sensor information: \n%--&gt; If using CNR4 with RAD_FirstStage_include file, add inputFileNames etc., for example:\n%globalVars.Trace.SW_IN_1_1_1.inputFileName     = {'MET_CNR4_SWi_Avg'}\n\nglobalVars.Trace.SW_IN_1_1_1.inputFileName              = {''}\nglobalVars.Trace.SW_IN_1_1_1.inputFileName_dates        = [datenum(1900,1,1) datenum(2999,12,31)]\nglobalVars.Trace.SW_IN_1_1_1.instrument                 = 'CNR4'\nglobalVars.Trace.SW_IN_1_1_1.instrumentSN               = ''\nFor SW_IN_1_1_1 (incoming shortwave radiation), add:\n\nThe inputFileName from your database;\nEdit the start date for your data record in inputFileName_dates;\nEdit instrument (if necessary); and\nAdd the serial number (instrumentSN).\n\nAll these parameters are used for the other radiation variables defined below this one, apart from inputFileName, which you will need to input for each variable. Also, if you have a PAR radiometer you can use the PPFD global variables in this radiation section in the same way.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this change. This step will add the radiation variables into the same Met folder as in step 4.\n\nNext, we will clean your Flux variables. As previously mentioned if you followed step 5, we have provided “include” files that load most information on common traces for you.\n\nTo use these files scroll to the bottom of your first stage INI file. For anyone with EddyPro output, you need to use EddyPro_Common_FirstStage_include.ini, which contains all variables common to all IRGAs; then, also uncomment the other line(s) for your specific IRGA(s).\nFor example, let’s assume you are using a LiCor LI-7200 and LI-7700 at your site. Then you would uncomment the relevant include files, as follows:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n#include EddyPro_LI7700_FirstStage_include.ini\n%#include RAD_FirstStage_include.ini\nNow scroll up to:\n%---------------------------------------\n% Global variables (instrument-specific)\n%---------------------------------------\n% The instrument traces will all need to have the same\n% instrumentType field 'LI7200', 'LI7700'...\n% and come from the same software (EddyPro)\n\n% If using LI7200 with EddyPro_LI7200_FirstStage_include file:\nglobalVars.Instrument.IRGA.Enable                       = 1     % Required variable 0/1\nglobalVars.Instrument.IRGA.instrument                   = 'LI-7200' % Edit if using LI7500\nglobalVars.Instrument.IRGA.instrumentSN                 = ''    \nglobalVars.Instrument.IRGA.inputFileName_dates          = [datenum(1900,1,1) datenum(2999,12,31)]\nEnter/edit the relevant parameter details for your IRGA(s) (instrument, serial number, start date of measurements). Do the same for your anemometer and full eddy-covariance (EC) system instruments (e.g., CSAT3 plus IRGA). ‘otherTraces’ includes all instrument types not previously defined within global variables, and must be enabled, with measurement record dates defined.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this addition. You should see your first-stage cleaned data appear in a new Clean folder within your Flux folder.\n\nAt this stage, you can start to add any Met or Flux variables that may not already be included in the template files. You can do this by copy-pasting the [Trace]...[End] code block (like the one in figure 6.2) and editing the parameter inputs accordingly.\nAlso, for example, if you have more than one air temperature measurement, you would create more traces to assign these, and use the Ameriflux naming convention to distinguish and define their relative positions (figure 6.3).\n\nFigure 6.3. Second air temperature trace (TA_1_2_1) defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\nTIPS\n\n\nPay attention to the output display, it is informative.\n\n\nWe recommend using the “quick-look” visualization tools at any stage of cleaning to check that your data looks as expected (e.g., the filenames are correct and the retained values conform to your minMax bounds).",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.1 &nbsp; Quick Start: First Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html#quick-start-create-your-first-stage-ini-file-for-data-cleaning",
    "href": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html#quick-start-create-your-first-stage-ini-file-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to start cleaning your data for the first stage. Note that the example assumes you have already created Flux and Met databases for the same site.\n\n\n\n\nOpen your site-specific SITEID1_FirstStage.ini file for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better), your site ID, and the timezone information. Difference_GMT_to_local_time is the timezone that your site is located in, and Timezone is the timezone that your data is stored in (UTC, standard time, or daylight saving time). Both these parameters are currently necessary in order to output the correct timezone for Ameriflux requirements (local standard time).\nNext, scroll down (skipping “Global Variables” sections for now) to where you see:\n %----------------------------------------------------\n %--&gt; Insert met variables here\n %----------------------------------------------------\nBeginning with air temperature, define the input data to each trace given in the INI file. For example, if you have an output variable that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 6.2; yellow highlighted text). During this first cleaning stage this trace will be renamed as TA_1_1_1 using the variableName parameter, following the Ameriflux naming convention.\n\nFigure 6.2. Air temperature trace (TA_1_1_1) as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab’s datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs). \n\nOnce you have done this for a few variables, test the data cleaning in Matlab, using the fr_automated_cleaning.m function as follows:\n fr_automated_cleaning(yearIn,'SITEID',1)\nwhere yearIn is a numeric array containing the year(s) of data you want to clean (it is possible to clean multiple years at once but we will keep things simple with one year for now), SITEID would be SITEID1 from the earlier example, and ‘1’ represents the first stage of data cleaning.\nProvided everything worked with no errors, you should now see output data matching the variables you defined in a new Clean folder within your Met folder.\n\n\n\nIf you do NOT have a four-component net radiometer such as a CNR4 at your site, you can skip all of step 5 and go to step 6. If you do have a CNR4 or similar, we have provided an “include” INI file that loads relevant radiation variables for you. We strongly advise that you do not edit this or any other include file. Instead, scroll to the bottom of your site-specific INI file where the include files are called.\n\nUncomment* (e.g., delete % - NOT #) the line containing RAD_FirstStage_include.ini, so that the block of code looks like this:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n%#include EddyPro_Common_FirstStage_include.ini\n%#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n%#include EddyPro_LI7700_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\n*Recall that this file is read by Matlab, so comments are created using %; the # sign must be present to call the include file.\nNext, scroll almost to the top of the file where you see:\n%----------------------------------------------------------\n% Global variable specification (trace-specific)\n%----------------------------------------------------------\n%--&gt; Radiation sensor information: \n%--&gt; If using CNR4 with RAD_FirstStage_include file, add inputFileNames etc., for example:\n%globalVars.Trace.SW_IN_1_1_1.inputFileName     = {'MET_CNR4_SWi_Avg'}\n\nglobalVars.Trace.SW_IN_1_1_1.inputFileName              = {''}\nglobalVars.Trace.SW_IN_1_1_1.inputFileName_dates        = [datenum(1900,1,1) datenum(2999,12,31)]\nglobalVars.Trace.SW_IN_1_1_1.instrument                 = 'CNR4'\nglobalVars.Trace.SW_IN_1_1_1.instrumentSN               = ''\nFor SW_IN_1_1_1 (incoming shortwave radiation), add:\n\nThe inputFileName from your database;\nEdit the start date for your data record in inputFileName_dates;\nEdit instrument (if necessary); and\nAdd the serial number (instrumentSN).\n\nAll these parameters are used for the other radiation variables defined below this one, apart from inputFileName, which you will need to input for each variable. Also, if you have a PAR radiometer you can use the PPFD global variables in this radiation section in the same way.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this change. This step will add the radiation variables into the same Met folder as in step 4.\n\nNext, we will clean your Flux variables. As previously mentioned if you followed step 5, we have provided “include” files that load most information on common traces for you.\n\nTo use these files scroll to the bottom of your first stage INI file. For anyone with EddyPro output, you need to use EddyPro_Common_FirstStage_include.ini, which contains all variables common to all IRGAs; then, also uncomment the other line(s) for your specific IRGA(s).\nFor example, let’s assume you are using a LiCor LI-7200 and LI-7700 at your site. Then you would uncomment the relevant include files, as follows:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n#include EddyPro_LI7700_FirstStage_include.ini\n%#include RAD_FirstStage_include.ini\nNow scroll up to:\n%---------------------------------------\n% Global variables (instrument-specific)\n%---------------------------------------\n% The instrument traces will all need to have the same\n% instrumentType field 'LI7200', 'LI7700'...\n% and come from the same software (EddyPro)\n\n% If using LI7200 with EddyPro_LI7200_FirstStage_include file:\nglobalVars.Instrument.IRGA.Enable                       = 1     % Required variable 0/1\nglobalVars.Instrument.IRGA.instrument                   = 'LI-7200' % Edit if using LI7500\nglobalVars.Instrument.IRGA.instrumentSN                 = ''    \nglobalVars.Instrument.IRGA.inputFileName_dates          = [datenum(1900,1,1) datenum(2999,12,31)]\nEnter/edit the relevant parameter details for your IRGA(s) (instrument, serial number, start date of measurements). Do the same for your anemometer and full eddy-covariance (EC) system instruments (e.g., CSAT3 plus IRGA). ‘otherTraces’ includes all instrument types not previously defined within global variables, and must be enabled, with measurement record dates defined.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this addition. You should see your first-stage cleaned data appear in a new Clean folder within your Flux folder.\n\nAt this stage, you can start to add any Met or Flux variables that may not already be included in the template files. You can do this by copy-pasting the [Trace]...[End] code block (like the one in figure 6.2) and editing the parameter inputs accordingly.\nAlso, for example, if you have more than one air temperature measurement, you would create more traces to assign these, and use the Ameriflux naming convention to distinguish and define their relative positions (figure 6.3).\n\nFigure 6.3. Second air temperature trace (TA_1_2_1) defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\nTIPS\n\n\nPay attention to the output display, it is informative.\n\n\nWe recommend using the “quick-look” visualization tools at any stage of cleaning to check that your data looks as expected (e.g., the filenames are correct and the retained values conform to your minMax bounds).",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.1 &nbsp; Quick Start: First Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_5_Install_R_Studio.html",
    "href": "PipelineDocumentation/2_5_Install_R_Studio.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "As with all the other software installations for use in data cleaning, first check which versions are recommended at present.\n\nInstallation instructions for R on Windows and Mac can be found here: cran.rstudio.com.\nInstallation instructions for RStudio on Windows and Mac: posit.co/download/rstudio-desktop.\nAny packages that you need and do not already have will be automatically installed by our pipeline scripts.\nNext, install the following libraries in RStudio by running the code below. Note this code will only install packages that you don’t already have installed:\n\npackages &lt;- c(\"tidyverse\", \"caret\", \"REddyProc\", \"dplyr\", \"lubridate\", \"data.table\", \"fs\", \"yaml\", \"rlist\", \"zoo\", \"reshape2\", \"stringr\", \"ranger\", \"caret\", \"ggplot2\")\n  \ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n  if (any(installed_packages == FALSE)) {\n    install.packages(packages[!installed_packages])\n  }",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.5 &nbsp; Install Software: R/RStudio"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_5_Install_R_Studio.html#install-r-and-rstudio-on-your-local-computer",
    "href": "PipelineDocumentation/2_5_Install_R_Studio.html#install-r-and-rstudio-on-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "As with all the other software installations for use in data cleaning, first check which versions are recommended at present.\n\nInstallation instructions for R on Windows and Mac can be found here: cran.rstudio.com.\nInstallation instructions for RStudio on Windows and Mac: posit.co/download/rstudio-desktop.\nAny packages that you need and do not already have will be automatically installed by our pipeline scripts.\nNext, install the following libraries in RStudio by running the code below. Note this code will only install packages that you don’t already have installed:\n\npackages &lt;- c(\"tidyverse\", \"caret\", \"REddyProc\", \"dplyr\", \"lubridate\", \"data.table\", \"fs\", \"yaml\", \"rlist\", \"zoo\", \"reshape2\", \"stringr\", \"ranger\", \"caret\", \"ggplot2\")\n  \ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n  if (any(installed_packages == FALSE)) {\n    install.packages(packages[!installed_packages])\n  }",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.5 &nbsp; Install Software: R/RStudio"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_2_Recently_Added_Features.html",
    "href": "PipelineDocumentation/8_2_Recently_Added_Features.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This page contains significant features recently added to the data cleaning pipeline - most recent at the top. Details can also be found in the full documentation (updated on this website soon).\nWatch this space for third stage changes coming soon…\n\n\nNew Overwrite feature:\nSometimes we need to overwrite multiple properties for one or more traces that have already been created e.g. in an include file. The global variables feature allows this, however, once this section becomes long with many trace property tweaks, it can become very hard to troubleshoot, and in these cases having all the information for one trace together is more desirable.\nInstead of using global variables you can duplicate the full trace ([Trace] ... [End]), and in your site-specific first stage INI file, put this duplicate after the line of code where the include file is called that contains your original trace. Note the additional Overwrite property highlighted in yellow:\n\nLocation within site-specific INI file to put duplicate trace for overwriting a trace previously defined in an include INI file. In this case, we want to overwrite the CH4_MIXING_RATIO trace that was originally defined in EddyPro_LI7700_FirstStage_include.ini. Yellow highlighting shows the syntax for the “overwrite” property.\nThere are three overwrite options:\n\n0 = do not overwrite with this trace. This is the default setting. If you do not include the Overwrite parameter, the pipeline assumes this option. If you have a duplicate trace you will get an error during cleaning (Example 1):\n\n1 = overwrite traces having the same variableName and also with Overwrite = 0 setting. This puts the duplicate data where the original data was, i.e., complete overwrite, first trace gone. Use this setting if you want your duplicate T2 available to use in a later variable such as T3, T4, T5, etc. (Example 2):\n\n2 = overwrite traces having the same variableName and with Overwrite = 0 setting. This takes advantage of the “position” of the duplicate trace. Use this setting if you want a later variable such as T3 or T4 available to use in T2.\n\n\n\nNew postEvaluate property:\nThe new postEvaluate property can be applied in the first stage. It is executed in the first stage after all other cleaning is done, e.g., minMax, calibration, etc.. This is in contrast to the Evaluate property which is executed for all traces before any other cleaning properties.\nGenerally speaking, the order of operations is: Evaluate –&gt; other cleaning (e.g., minMax, calibration) –&gt; postEvaluate. This is regardless of the order that they appear within [Trace] ... [End]. The following series of examples show how the Evaluate and postEvaluate statements work in relation to the other cleaning properties. For each example the input (Original) is an array of ones, and both the “Original” and first-stage “Clean” data are plotted in the result.\nExample 1: Only minMax and calibration, no Evaluate or postEvaluate statements\n\nResult: x = 2x [calibrated]\n\n\nExample 2: Evaluate, minMax and calibration, no postEvaluate statement\n\nResult: x = (x - 1)*2 [Evaluate first, then calibrate]\n\n\nExample 3: minMax and calibration, postEvaluate statement, no Evaluate statement\n\nResult: x = 2x - 1 [calibrate, then postEvaluate]\n\n\nExample 4: minMax and calibration, both Evaluate and postEvaluate statements\n\nResult: x = (x - 1)*2 - 1 [Evaluate, then calibrate, then postEvaluate]",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ",
      "8.2 &nbsp; Recently Added Features"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_2_Recently_Added_Features.html#recently-added-features",
    "href": "PipelineDocumentation/8_2_Recently_Added_Features.html#recently-added-features",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This page contains significant features recently added to the data cleaning pipeline - most recent at the top. Details can also be found in the full documentation (updated on this website soon).\nWatch this space for third stage changes coming soon…\n\n\nNew Overwrite feature:\nSometimes we need to overwrite multiple properties for one or more traces that have already been created e.g. in an include file. The global variables feature allows this, however, once this section becomes long with many trace property tweaks, it can become very hard to troubleshoot, and in these cases having all the information for one trace together is more desirable.\nInstead of using global variables you can duplicate the full trace ([Trace] ... [End]), and in your site-specific first stage INI file, put this duplicate after the line of code where the include file is called that contains your original trace. Note the additional Overwrite property highlighted in yellow:\n\nLocation within site-specific INI file to put duplicate trace for overwriting a trace previously defined in an include INI file. In this case, we want to overwrite the CH4_MIXING_RATIO trace that was originally defined in EddyPro_LI7700_FirstStage_include.ini. Yellow highlighting shows the syntax for the “overwrite” property.\nThere are three overwrite options:\n\n0 = do not overwrite with this trace. This is the default setting. If you do not include the Overwrite parameter, the pipeline assumes this option. If you have a duplicate trace you will get an error during cleaning (Example 1):\n\n1 = overwrite traces having the same variableName and also with Overwrite = 0 setting. This puts the duplicate data where the original data was, i.e., complete overwrite, first trace gone. Use this setting if you want your duplicate T2 available to use in a later variable such as T3, T4, T5, etc. (Example 2):\n\n2 = overwrite traces having the same variableName and with Overwrite = 0 setting. This takes advantage of the “position” of the duplicate trace. Use this setting if you want a later variable such as T3 or T4 available to use in T2.\n\n\n\nNew postEvaluate property:\nThe new postEvaluate property can be applied in the first stage. It is executed in the first stage after all other cleaning is done, e.g., minMax, calibration, etc.. This is in contrast to the Evaluate property which is executed for all traces before any other cleaning properties.\nGenerally speaking, the order of operations is: Evaluate –&gt; other cleaning (e.g., minMax, calibration) –&gt; postEvaluate. This is regardless of the order that they appear within [Trace] ... [End]. The following series of examples show how the Evaluate and postEvaluate statements work in relation to the other cleaning properties. For each example the input (Original) is an array of ones, and both the “Original” and first-stage “Clean” data are plotted in the result.\nExample 1: Only minMax and calibration, no Evaluate or postEvaluate statements\n\nResult: x = 2x [calibrated]\n\n\nExample 2: Evaluate, minMax and calibration, no postEvaluate statement\n\nResult: x = (x - 1)*2 [Evaluate first, then calibrate]\n\n\nExample 3: minMax and calibration, postEvaluate statement, no Evaluate statement\n\nResult: x = 2x - 1 [calibrate, then postEvaluate]\n\n\nExample 4: minMax and calibration, both Evaluate and postEvaluate statements\n\nResult: x = (x - 1)*2 - 1 [Evaluate, then calibrate, then postEvaluate]",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ",
      "8.2 &nbsp; Recently Added Features"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_SoftwareInstallation.html",
    "href": "PipelineDocumentation/2_SoftwareInstallation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides instructions on installing the software and configuring the libraries needed for running the data-cleaning pipeline.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_SoftwareInstallation.html#software-installation",
    "href": "PipelineDocumentation/2_SoftwareInstallation.html#software-installation",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides instructions on installing the software and configuring the libraries needed for running the data-cleaning pipeline.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation"
    ]
  },
  {
    "objectID": "Pubs_list.html#theses",
    "href": "Pubs_list.html#theses",
    "title": "Publications",
    "section": "Theses",
    "text": "Theses"
  },
  {
    "objectID": "Pubs_list.html#research-talks-poster-presentations",
    "href": "Pubs_list.html#research-talks-poster-presentations",
    "title": "Publications",
    "section": "Research Talks & Poster Presentations",
    "text": "Research Talks & Poster Presentations"
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html",
    "href": "Documentation/Fieldwork/Cleaning.html",
    "title": "Cleaning Procedures",
    "section": "",
    "text": "This page details the steps for cleaning the 7700 and 7200. Always do the 7700 first, followed by the 7200. You must clean the sensors before calibration.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#getting-started",
    "href": "Documentation/Fieldwork/Cleaning.html#getting-started",
    "title": "Cleaning Procedures",
    "section": "Getting Started",
    "text": "Getting Started\n\nConnect the laptop to the site system using the ethernet cable and dongle to the top box on the outside of the scaffolding.\n\nNOTE: remote connection via vinimet & mobile hotspot can also work in a pinch.\n\n\nTO DO: Add pics of logger box at each each site for reference.\n\nRetract the boom to access the flux sensors. Loosen the two bolts securing the boom &gt; pull quick release tab&gt; slide boom in &gt; Be mindful of cables\n\nFor BB1 & DSM: place stepladder on plywood on the ground. Make sure it’s solid.\nFor BB2 & RBM: loosen the wing nut &gt; remove the fastening boot &gt; lift up sensors to reduce force needed to pull out the bolt &gt; rotate the arm to bring the sensors within reach from platform. Be careful this can be quite difficult.\n\n\n\n\nBB1 & DSM Cleaning\n\n\n\n\n\n\nBB2 & RBM Cleaning\n\n\n\n\n\n\n\n\nDo a visual inspection of the sensors and note/address any obvious issues.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7700",
    "href": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7700",
    "title": "Cleaning Procedures",
    "section": "Cleaning the LI 7700",
    "text": "Cleaning the LI 7700\n\nOn the laptop, open the program ‘LI-7700’. Click “Connect” &gt;&gt; Ethernet &gt;&gt; Select Instrument &gt;&gt; Connect. Open 2nd data page and look at the single graph with signal strength\n\nTO DO: Add Screenshot\n\nWipe top and bottom windows with windshield cleaner (wet then dry cloth) until signal strength is sufficient:\n\n\n\n\n\n\nTO DO: Replace w/ another (different) image on cleaning mirrors?\n\nDesired LI 7700 Signal Strength by Site\n\n\n\nSite\nValue\n\n\n\n\nBB1\nHigh 50’s\n\n\nBB2\nAround 80\n\n\nDMS\nAround 70\n\n\nRBM\n…\\\n\n\n\n\nDisconnect from the 7700 and close the LI-7700 program.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7200",
    "href": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7200",
    "title": "Cleaning Procedures",
    "section": "Cleaning the LI 7200",
    "text": "Cleaning the LI 7200\n\nPower down the flux system. Inside the power box, flip the 3 switches as shown in the table below. The power is off when the red light is on.\n\n\nEC Sensor Power Channels\n\n\n\nSite\nLI-7200 flow module (pump)\nLI-7700\nLI-7550\n\n\n\n\nBB1\n12\n7\n2\n\n\nBB2\n3\n4\n5\n\n\nDMS\n3\n4\n5\n\n\nRBM\nTBD\nTBD\nTBD\n\n\n\n\nTake the intake tube off by loosening the nut with a wrench. The tube can then just hang loosely from the tower, disconnected from the 7200. Then loosen the knobs on top of 7200 to release the top and take side part off to open the 7200 sensor head. This can can hang next to the instrument while cleaning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWipe top and bottom windows with windshield cleaner (wet then dry cloth). Then clean intake filter with a wipe.\n\nYou may need to replace with a spare clean cap intake. If so: remove the foil wrap, loosen the metal ring with pliers and push it all the way to the tube, remove and replace the cap, use pliers to put the metal ring back on the cap\n\nPower system back up using switches listed in the table above.\n\nNOTE: The start up sequence is: 1) LI-7200 pump, 2) LI-7700, 3) LI-7550. Allow 90 seconds between each sensor when powering up. This order is important for proper start up.\nInside the Li-7550 box, the USB logging light should be flashing\n\nOn the laptop open the LI 7x00 program and connect the the 7500 to check that the signal strength is sufficient:\n\n\n\nDesired LI 7200 Signal Strength by Site\n\n\n\nSite\nValue\n\n\n\n\nBB1\nAbout 100\n\n\nBB2\nAbout 103\n\n\nDMS\nAbout 102\n\n\nRBM\n…",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DataEntry.html",
    "href": "Documentation/Fieldwork/DataEntry.html",
    "title": "Data Entry",
    "section": "",
    "text": "This page features links to various spots to input the hand-collected data when out in the field (e.g., Water Table Depths). The sheets are all hosted on the Lab’s Google Drive under the Project’s tab.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Data entry"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DataEntry.html#sites",
    "href": "Documentation/Fieldwork/DataEntry.html#sites",
    "title": "Data Entry",
    "section": "Sites",
    "text": "Sites\n\nBurns Bog 1\n\n\n\n\n\n\n\nWater Table Height\n\nAnother Link?\n\n\n\n\nBurns Bog 2\n\n\n\n\n\n\n\nWater Table Height\n\nAnother Link?",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Data entry"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html",
    "href": "Documentation/Fieldwork/DailyMonitoring.html",
    "title": "Daily Site Monitoring",
    "section": "",
    "text": "Below are instructions for visual checks of the data that should be done every day. Web plots for each site can be found by going to:",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html#check-critical-variables",
    "href": "Documentation/Fieldwork/DailyMonitoring.html#check-critical-variables",
    "title": "Daily Site Monitoring",
    "section": "Check Critical Variables",
    "text": "Check Critical Variables\n\nMotor Power (%) & Flow Fate for LI-7200 pump\n\nKeep checking if this one changes over time (over weeks).\n\nFlow drive % should be 50 - 90%\nHigher power means more air-flow resistance in the tubing indicating either clogged up filter (if we use those), dirty rain cap wire-mesh or damaged tubing\n\nWe aim for 30-min averages flow rate of slpm = 15 L/min;\n\nBelow 13 L/min we begin to worry, (data quality drops);\nBelow 10 L/min we need to fix it ASAP (data quality questionable or data missing) by cleaning up the intake tube cap\n\n\n\n\n\nGood Flow at BB1\n\n\n\n\n\nBad Flow at BB2\n\n\n\n\nThermocouples\n\nLI-7200: these two values should be very close to each other (&lt;1 C difference)\n\nIn (t_in_LI_7200)\nOut (t_out_LI_7200)\n\nLI-7700\n\nAir temperature\n\n\n\n\nClimate Data\n\nCheck if there are new data & if the values are reasonable (for all sites)\n\nClick on “last day” / “last week” to filter the time series\nIf you see BB1 and BB2 precip data don’t align with each other, there’s a possibility that the tipping bucket is clogged.\n\n\n\n\nFlux Data\nFor Fluxes and Gas Concentrations check if there are new data & if the values are reasonable. These will only be calculated if smart flux is running properly.\nGas Concentrations:\n\nCO2 mixing ratio (dry) - during daytime should be around 380-420 ppm (umol/mol), during night time it can be quite high depending on the site (sometimes &gt; 600 ppm)\nCH4 mixing ratio (dry) - should be around 2 ppm\n\nFluxes:\n\nSensible & Latent heat - Typically between -100 and + 450 w m-2\n\nShould generally sum ~ 70%-80% of net radiation\n\nFCO2 & FCH4 - site dependent, but look for gaps, extreme spikes, systematic jumps\n\nSignal strength\n\nView over a longer period to check for any trends. Schedule a site visit if it’s decreasing rapidly (&lt;10%). It’s normal for 7700’s signal strength to drop during rainy periods, but it’s not normal for 7200’s to drop that frequently (if it does, check the 7200 head’s o-ring).\n\nVoltage\n\nLook at data over the past month and tell June or Rick if the battery voltage goes below 24 V at midnight.\n\nThis usually happens in the winter when we have a few days of clouds & rain.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html#remote-login-procedures",
    "href": "Documentation/Fieldwork/DailyMonitoring.html#remote-login-procedures",
    "title": "Daily Site Monitoring",
    "section": "Remote Login Procedures",
    "text": "Remote Login Procedures\nConnect to VPN, go to Remote Desktop & connect to vinimet.geog.ubc.ca (or it’s better to do this from your personal computer so you won’t kick off or get kicked off by other people using Vinimet). Note that doing this on your personal computer requires a PC.\n\nLI 7200\nOpen the LI-7x00 software on desktop and connect using the right IP Address\n\n\n\nConnecting to the BB1 7200\n\n\n\nOnce on the main page check:\n\nThe logger status → should be logging\nUSB Free Space → make sure there’s still enough space in the USB (usually the USB stick can handle 6-weeks data)\nThe SmartFlux module has to be connected to the system, make sure it’s not “none”\nHead pressure - should be between -0.8 to -3.8 kPa\nFlow drive - should be around 50% to 90%\nFlow rate - should be around 13-15 l/m\n\n\n\n\n\nChecking the BB1 7200\n\n\nClick on the “Diagnostics” tab - check if all parameters are “OK”\n\n\n\nChecking the BB1 7200 Diagnostics\n\n\n\n\nLI 7700\nOpen the LI-7700 software on desktop and use the same IP address as for the 7200\n\n\n\nConnecting to the BB1 7200\n\n\nCheck 7700 Optics RH. If it’s &gt;15%, we should replace the internal chemicals.\n\n\n\nChecking the BB1 7700\n\n\n\nCheck SmartFlux internal memory every 2-3 months\n\nInstructions to check memory are under Note_SmartFlux internal memory_ssh connection.docx\nInstructions to clear memory are under Note_SmartFlux internal memory_updater.docx\n\n\nInfo from here",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html",
    "href": "Documentation/LabInfo.html",
    "title": "Information for Current Lab Members",
    "section": "",
    "text": "This page provides information and links to documentation that are relevant to current members of the lab",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#information-for-new-lab-members",
    "href": "Documentation/LabInfo.html#information-for-new-lab-members",
    "title": "Information for Current Lab Members",
    "section": "Information for new lab members",
    "text": "Information for new lab members\n\nAdd your info to the ‘Lab contact information’ doc in Google Drive\nReview the Grad Expectations document \nCoordinate a day/time for your bi-weekly lab meeting with Sara\nMake sure you’ve been added to the Google Drive account and Slack Workspace\nSend bio and picture to Sara so that she can add you to the lab website\nReview program requirements (see below). More information on potential courses can be found here.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#program-requirements",
    "href": "Documentation/LabInfo.html#program-requirements",
    "title": "Information for Current Lab Members",
    "section": "Program requirements",
    "text": "Program requirements\n\nPhD (typical timeline)\n\nFall term Year 1\n\nComplete memo of expectations  in September.\nCourses\nGEOS 500 - develop literature Review and preliminary proposal\n\nSpring term Year 1\n\nCourses\nWork with supervisor to refine preliminary proposal/research gap. Identify committee members\nComplete spring review - present preliminary proposal, assess progress, discuss timeline for comprehensive exams, set tentative topics, confirm committee, confirm any further coursework\n\nSummer Year 1\n\nFieldwork/research\nFormalise comps reading lists and topics. More details on the comps policy can be found here.\n\nFall term Year 2\n\nComplete memo of expectations  in September.\nFieldwork/research\nPrepare and distribute proposal\n\nSpring term Year 2\n\nFieldwork/research\nComprehensive Exam (written exam followed by oral examination two weeks later)\n\nComplete annual review by end of spring/early summer\n\n\nSummer Year 2\n\nFieldwork/research\nFormal meeting to approve Proposal (or previous spring)\n\nFall term Year 3\n\nComplete memo of expectations  in September.\nFieldwork/research\n\nSpring term Year 3\n\nComplete memo of expectations by end of May\nComplete annual review by end of spring/early summer\nFieldwork/research\n\nSummer term Year 3\n\nFieldwork/research\n\nFall term Year 4\n\nComplete memo of expectations  in September.\nResearch\nCheck doctoral deadlines to plan out timing of defense\n\nBe mindful of the time required to complete the examination process\nThe External Examiner nomination form needs to be submitted ~5 months before the expected defense date. Nomination is the supervisor’s responsibility. Candidates should keep tabs on the process to ensure timely submission, but are not allowed to know who the examiner is.\n\n\nSpring term Year 4\n\nComplete annual review by end of spring/early summer\nResearch\nShare thesis with Supervisory Committee\n\nCommittee may take a while read/approve document\nIncorporate changes/edits\n\nNominate University Examiners - Supervisor’s responsibility, but candidate should keep tabs to ensure the\nSubmit thesis to GPS for external examination\n\nPlan for a defense 6-8 weeks after submitting\n\nShould anticipate that the examiner will take 4-6 weeks to review the thesis.\nThe earliest possible defense date is 1 week after the external report is submitted.\n\nSummer term Year 4\n\nOral exam\n\nNote: Exam must be scheduled at least 4 weeks in advance. *Make necessary revisions/edits\nDue within 1 month of defense\n\n\n\n\n\nMSc (typical timeline)\n\nFall term Year 1\n\nComplete memo of expectations  in September.\nCourses\nGEOS 500 - develop literature Review and preliminary proposal\n\nSpring term Year 1\n\nCourses\nWork with supervisor to refine preliminary proposal/research gap. Identify committee members\nComplete spring review - present preliminary proposal, assess progress\n\nSummer Year 1\n\nFieldwork/research\n\nFall term Year 2\n\nComplete memo of expectations  in September.\nFieldwork/research\n\nSpring term Year 2\n\nFieldwork/research\nComplete annual review by end of spring/early summer\nPresent thesis at Spring Symposium\n\nSummer Year 2\n\nThesis defense & submit final thesis (guidelines for the thesis can be found here). And see ‘msc-thesis-defence-revised.pdf’ doc for details.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#leaving-the-lab-checklist",
    "href": "Documentation/LabInfo.html#leaving-the-lab-checklist",
    "title": "Information for Current Lab Members",
    "section": "Leaving the lab checklist",
    "text": "Leaving the lab checklist\nWe’re sad to see you go, but we’re excited for your next adventure! However, before you leave, make sure to:\n\nUpload your code to github (or share it with Sara directly). Make sure to follow the guidelines for reproducible data flow so that we can run all your code after you leave!\nMake sure to share all relevant data with Sara. Ideally this would be in the data folder in each of your project/chapter folders.\nReturn all keys, equipment, field gear that belong to be lab/university.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/DataWorkflows.html",
    "href": "Documentation/DataWorkflows.html",
    "title": "Data Pipeline",
    "section": "",
    "text": "Data flow schematic\nMicromet Data Flow (currently for BB1, BB2, DSM, and RBM)  Note: Manitoba sites have SmartFlux stream only\n\n\n\n\n\n\n\nDrive/Folder structure on vinimet\n\nP:\\Sites\\Site (data as received from the sites)\n              \\Met\n                  \\5min (for BB only)\n              \\Flux\n              \\HighFrequencyData\nP:\\Database (everything in read_bor format)\n            \\yyyy\\Sites\n                       \\Met (straight from CRBasic -&gt; read_bor)\n                           \\Clean (First stage clean)\n                       \\Flux (from EddyPro summary, site files -&gt; read_bor & then overwrite with EddyPro recalculated output) \n                           \\Clean (First stage clean)\n                       \\Clean\\SecondStage \n                             \\ThirdStage\n                             \\Ameriflux\n                             \\ThirdStage_REddyProc_RF_Fast (simple uncertainty analysis)\n                             \\ThirdStage_REddyProc_RF_Full (full uncertainty analysis)\nP:\\Micromet_web (note that this mirrors what’s on host remote.geog.ubc.ca)\n                \\www \n                    \\data (html & javascript for site level web plotting)\n                    \\webdata\\resources\\csv (csv files for web plotting)\nGoogle drive contains:\n\nPictures (to remove and host on google photos)\nDocuments\n\n\n\n\nData stream notes\nThe data processing is run from LoggerNet Task manager through batch files stored under c:\\UBC_Flux\\BiometFTPsite. The batch files are named: Site_automatic_processing.bat. The generic Matlab scripts that’s called from these batch files is: run_BB_db_update([],{‘Site’},0)\nMore Details\n\nMET Files\n\nEvery 4 hrs (e.g., BB1 4:20, BB2 4:10) Loggernet Task Master downloads MET data to P:\\Sites\\Site\\Met\\Site_MET.dat (or Site_biomet.dat)\n\nNote Site is used generically to refer to each site (e.g., BB, BB2, DSM, RBM, Hogg, Young)\nOnce per day 1:10 am\n\n‘Rename files for all sites’ - Called from Loggernet Task Master\n\nRenames Site_MET.dat to Site_MET.yyyyddd\n-nosplash /r “renam_csi_dat_files”('P:Sites\\Site\\Met');…for each site; exit;”\n\nSite_Process_CSI_to_database - Called from Loggernet Task Master\n\n-nosplash -minimize /r “run_BB_db_update([],{‘Site’});”\n  function run_BB_db_update(yearln)\n    dv = datevec(now);\n    arg_defualt('yearln', dv(1));\n    sites = {'Site'};\n    db_update_BB_site(yearln, sites);\n    exit\n\ndb_update_BB_site\n\nUpdate Site matlab database with new logger files to P:\\Database\\yyyy\\Site\\Met\nCall C:\\UBC_PC_Setup\\PC_specific\\BB_webupdate to create .csv files used by webplots -&gt; `P:_web\nCall C:\\Ubc_flux\\BiometFTPsite\\BB_Web_Update.bat to move csv files to webserver\n\n\nLocation of data\n\nP: - local drive on vinimet\nP:\\Micromet_web\\www\\webdata\\resources\\csv - location of CSV files\nP:\\Micromet_web\\www\\data - local versions of js, html files\nP:\\Sites\\Sites - raw data\n\n\n\n\nWeb-plotting pipeline\nFirst, go from the bottom of the chart in the first page upwards, find out which step stopped working then use the followiwng instructions to troubleshoot.\nEC Data\n\nSmartFlux files (*_EP-Summary.txt) arrive in the folders as below: \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB\\Flux \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB2\\Flux\n\ninsert image \n\nTroubleshooting for Step 1:\n\nIf there is NO recent data files (.zip or .txt) at all, then:\n\nCheck if you can still connect to LI-7200 to determine if it is an Internet issue or not.\nCheck if the LoggerNet task for data downloads failed. When all tasks are executed properly, the status of the tasks should be shown like this:\n\ninsert image \n\nCheck if the SmartFlux system at site skips processing or not by using WinSCP connecting to it (e.g., BB_site or BB2_site). The second figure below shows the BB2 SmartFlux have skipped processing of Feb. 1 (there is no summary file for that day):\n\ninsert images \n\n\n\nIf SmartFlux files arrive, the task “XX_automatic_processing” (as shown in the figure above) that runs Matlab program will process all data (CSI and SmartFlux) and update the web files too.\n\nTroubleshooting for Step 2:\n\nIf you suspect that this task failed, then:\n\nCheck the dates of the last updated web csv files (\\\\VINIMET.GEOG.UBC.CA\\Micromet_web\\www\\webdata\\resources\\csv).\nCheck a few flux-related csv files to see if the last datestamps are less than 6 hours old.\nIf the files are NOT up-to-date, manually rerun the Matlab program by using run_BB_db_update([]{'BB2},0)\n\nIf you confirm the task is working fine or you’ve re-run the Matlab program, then:\n\nCheck if those files were uploaded to the web server by starting WinSCP to connect to ‘remote.geog.ubc.ca’ and see if the files are there.\n\n\n\n\nMet Data\nData from dataloggers goes to \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB\\Met\\ or \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB2\\Met\\.\n\nTroubleshooting:\n\nCheck if the files have arrived or not.\nCheck the LoggerNet task for data downloads.\nCheck connection to the logger\nCheck the data of the last updates Biomet database files (\\\\VINIMET.GEOG.UBC.CA\\Database)\nCheck a few MET-related csv files to see if the last datestamps are less than 6 hours old.\nManually re-run the task in Matlab by using run_BB_db_update([]{'BB2},0).\nCheck if those csv files were uploaded to the web server.\n\n\nContinue adding info from here\n\n\nOther Documentation & Tasks\nThe following documents in the General procedures, settings and protocols folder in Google Drive may also be of use:\n\nUpdating webplots → Micromet web plotting.docx\nDocuments for dealing with full internal memory for SmartFlux → Note_SmartFlux internal memory_ssh connection.docx & Note_SmartFlux internal memory_updater.docx\n7200 lab calibration → Note_LI-7200 Lab calibration procedure.docx\nGas tank calibration → Report_Gas tank calibration.docx\n\nNOTE: Note that the troubleshooting reports folder contains documentation on troubleshooting that is relevant to all sites.\n\n\nCreating a local copy of the database\nTo create a local copy of the Micromet Lab database or create your own database following the Micromet Lab database structure, following the instructions provided here",
    "crumbs": [
      "Documentation",
      "Data pipeline"
    ]
  },
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "Research",
    "section": "",
    "text": "Our research focuses on measuring and modelling greenhouse gas, water, and energy fluxes across a range of spatial and temporal scales. We combine field-based measurements, remote sensing, and modelling to investigate land-atmosphere interactions in our rapidly changing world, with a current emphasis on wetland ecosystems.",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Research.html#the-integrated-ghg-research-and-observations-in-wetlands-igrow-research-program",
    "href": "Research.html#the-integrated-ghg-research-and-observations-in-wetlands-igrow-research-program",
    "title": "Research",
    "section": "The integrated GHG Research and Observations in Wetlands (iGROW) Research Program",
    "text": "The integrated GHG Research and Observations in Wetlands (iGROW) Research Program\nAmong the numerous ecosystem services provided by wetlands climate regulation is identified as one of their most important benefits to society. Wetland ecosystems play an important role in the global carbon cycle; they provide the ideal environment for long-term storage of atmospheric carbon dioxide, yet they are also the largest single source of methane. Climate change has the potential to increase greenhouse gas (GHG) emissions from wetlands, however, the consequences of rising temperatures on wetland GHG exchange remains uncertain. Furthermore, preventing further wetland loss and restoring wetland ecosystems has been identified by researchers and governments as important in limiting future emissions to help meet climate goals. The integrated GHG Research and Observations in Wetlands (iGROW) research program’, takes an interdisciplinary approach to provide a better understanding of how wetland responses to climate variability and restoration can feedback to slow or accelerate future climate change. iGROW combines state-of-the-art field-based measurements, remote sensing, and modelling to provide new insights into the controls of wetland GHG fluxes across a range of spatial and temporal scales and quantify the potential climate benefits of wetland restoration and conservation. This research is key to better predicting current and future contributions of wetlands to climate change, which is highly relevant for policies aiming to limit the level of global temperature rise. Current iGROW research projects include:\n\nCarbon Fluxes in Restored Wetlands\n\nIn collaboration with several other research groups at UBC, my lab’s research is focused on measuring GHG, water, and energy fluxes over Burns Bog, a restored peatland in Metro Vancouver. This site is a raised domed peat bog that is undergoing re-wetting as a restoration management strategy following peat harvesting and associated drainage. While restoration can help recover important ecosystems services provided by wetlands, it can also affect the exchange of greenhouse gases, water and energy between the surface and the atmosphere.\n\nBy conducting year-round eddy covariance measurements, we are assessing the biogeochemical and biophysical impacts of peatland restoration, and the impacts of forest fire smoke on carbon cycling and energy fluxes in restored peatlands. This research is also being done in collaboration with researchers and staff at Metro Vancouver who are interested in quantifying the climate mitigation potential of wetland restoration and conservation. \nCollaborators:  UBC Ecohydrology Research Group  UBC Biometeorology and Soil Physics Group  Dan Moore’s Research Group, UBC Geography \nNews on Burns Bog:  April 20, 2023 - Up in smoke: Human activities are fuelling wildfires that burn essential carbon-sequestering peatlands  June 16, 2021 - David Suzuki: For climate’s sake, save the peat!  June 29, 2020 - Carbon Neutral Achievement Sets Stage for Bold Climate Action \n\n\nBlue Carbon Research\n\nBlue carbon is the carbon sequestered and stored in coastal ecosystems including tidal marshes, mangroves, and seagrasses. Coastal ecosystems are among the strongest carbon sinks in the biosphere. This coupled with their potential for low methane emissions, has generated widespread interest in these ecosystems for climate change mitigation and adaptation. However, measuring and modelling carbon exchanges in tidal wetlands presents unique challenges due to highly dynamic atmospheric and hydrological fluxes, as well as sensitivities to both terrestrial and marine influences. Through iGROW, my research group recently installed two eddy covariance flux towers (CA-DSM and CA-RBM) in tidal wetlands along the Pacific Coast of Canada to quantify and model the net carbon and GHG balance of these marshes, which represent an important data gap in ecosystem-scale measurements of GHG exchange from tidal marshes. In partnership West Coast Environmental Law, we are also working to translate this blue carbon science into policy-relevant information for municipalities and provincial governments. \n\nOur research group is also involved in the NSF funded Coastal Carbon Research Coordination Network. Specifically, we are involved in the Methane Working Group, which aims to compile all methane flux data from coastal habitats (excluding mangroves) in the CONUS to parameterize and validate a set of nested process-based methane models. \nAdditionally, we are part of a U.S. Department of Energy grant “High-frequency Data Integration for Landscape Model Calibration of Carbon Fluxes Across Diverse Tidal Marshes” focused on leveraging eddy covariance observations to improve landscape-scale models of carbon fluxes across tidal marshes. This grant brings together university and government researchers from across the U.S. \nI am also a co-investigator on a collaborative research effort to model the current and future mitigation capacity of Canada’s blue carbon ecosystems, which was recently funded by an NSERC Alliance grant. The project is led by Julia Baum (University of Victoria), and includes four co-principal investigators and seventeen collaborators from across the country which are contributing to this national natural ocean climate solutions research effort. The project brings together a consortium of four universities, three government agencies, and four eNGOs. \nPrimary Collaborators:  Oikawa Lab, California State University, East Bay  The U.S. Geological Survey  The Baum Lab, University of Victoria  O’Connor Lab, The University of British Columbia \n\n\nCarbon Cycling in the Prairie Pothole Region of Canada\n\nWith growing interest in wetland management and restoration as a Natural Climate Solution, improved estimates of wetland carbon sequestration and GHG fluxes across Canadian wetland types are strongly needed. In partnership with Ducks Unlimited Canada, we focus on wetlands in the Prairie Pothole Region of western Canada since these ecosystems are understudied relative to other wetland types in Canada, yet they play important roles in carbon cycling and climate regulation. Specifically, this study leverages existing funding and infrastructure to investigate carbon and GHG dynamics across two wetland sites, with the aims of measuring and modeling GHG fluxes in prairie wetlands in Manitoba.\nPrimary Collaborators:  Ducks Unlimited Canada - Pascal Badiou \nPrairie Wetlands in the News:  May 19, 2022 - Project intended to demonstrate wetlands’ greenhouse gas impact  April 20, 2022 - DUC analyzing wetlands on farms and ranches for carbon capture  April, 2022 - DUC Carbon Tower Project",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Research.html#past-igrow-research-projects",
    "href": "Research.html#past-igrow-research-projects",
    "title": "Research",
    "section": "Past iGROW Research Projects",
    "text": "Past iGROW Research Projects\n\nFLUXNET-CH4 – a global database of eddy covariance CH4 flux measurements and wetland synthesis for CH4\nNatural wetlands emit approximately 30% of global CH4 emissions, as their waterlogged soils create ideal conditions for CH4 production. They are also the largest, and most uncertain, natural source of CH4 to the atmosphere. Direct observations of local CH4 emissions with high measurement frequency are important for constraining CH4 budgets, for understanding the responses of CH4 fluxes to environmental factors and climate, and for creating validation datasets for the land-surface models used to infer global CH4 budgets. However, unlike well-coordinated efforts for synthesizing CO2 flux-tower observations (e.g., FLUXNET), no such network and data synthesis effort existed previously for CH4. \n\nFLUXNET-CH4 is an initiative coordinated through the Global Carbon Project in close partnership with AmeriFlux and EuroFlux, to compile a global database of eddy covariance CH4 flux measurements to answer regional and global questions related to the global CH4 cycle. Through this activity, we coordinated the collection, aggregation, standardization, and post-processing of global CH4 data from the flux tower community. FLUXNET-CH4 Version 1.0 includes data from 81 sites, representing freshwater, coastal, upland, natural, and managed ecosystems. \nAdditionally, through a USGS Powell Center Working Group, we leveraged the FLUXNET-CH4 to provide novel insights into the controls and timing of wetland CH4 emissions for North America and globally, inform and validate biogeochemical models, and upscale wetland CH4 flux measurements globally. \nCollaborators:  Jackson Lab, Stanford University  NASA Goddard  Global Carbon Project  Stanford Machine Learning Group \n\n\nQuantifying carbon benefits of tidal wetland restoration in the Delta: Decision support using a robust, integrated and data-driven model\n\nThis research was funded through the California Department of Fish and Wildlife and focused on advancing remote sensing data in tidally flooded marshes and using the data to improve modeling of greenhouse gases in these systems. The modeling tool will be made publicly available through Google Earth Engine. This research was a collaboration between UBC, California State University, East Bay, UC Berkeley, and the U.S. Geological Survey.\nCollaborators:  Oikawa Lab, California State University, East Bay  The U.S. Geological Survey  The Landscape Research Group, UC Berkeley",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html",
    "href": "Documentation/UsingGit.html",
    "title": "Using Git",
    "section": "",
    "text": "We use git/github to manage our codebase. Version control is an essential to successfully managing of any project with multiple contributors! There can be a steep learning curve when you are first getting started with Git, but the time and hassle you will save your future self by learning will be enormous!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#what-is-a-repository",
    "href": "Documentation/UsingGit.html#what-is-a-repository",
    "title": "Using Git",
    "section": "What is a Repository?",
    "text": "What is a Repository?\nA repository (or repo) is just a collection of code, files, data, etc. that are being tracked by Git. Generally, all components of a repo should work towards a common end, whether that is an expansive goal such as managing all code used in our processing pipeline, maintaining the versions of code running on our data loggers, or the backend of our organizations website",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#what-is-a-branch",
    "href": "Documentation/UsingGit.html#what-is-a-branch",
    "title": "Using Git",
    "section": "What is a Branch?",
    "text": "What is a Branch?\n\n\nBranches can be thought of like limbs on a tree, or perhaps a more accurate analogy can be taken from hydrology, by comparing them to an anbranching river channel.\n\nBranches are separate “streams” of code that break off of the main “channel”. Some branches may be dead ends that don’t end up going anywhere. While some branches may lead lead to improvements that can be incorporated back into the main branch of the codebase. Others may break off from the main project entirely.\n\n\n\n\n\nAn anabranching section of the Mackenzie River Delta.\n\n\n\n\n\nBranch Protection Rules\nWe use branch protection rules to protect the main codebase. All changes to the main branch of a protected repository, must be submitted via a “pull request” using a separate branch. This allows for review of the changes by other users before changes are incorporated. It adds an extra level of security by protecting the main branch of code from the erroneous keystrokes of novice and expert users alike! These rules are act as a “gate” that helps ensure any changes to operational code are intentional and agreed upon by relevant parties. If you try to push (force) a change to the main branch of a protected repository on our GitHub, you’ll be given an error message and told to submit a pull request instead! More info can be found here",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#common-git-tasks",
    "href": "Documentation/UsingGit.html#common-git-tasks",
    "title": "Using Git",
    "section": "Common Git Tasks",
    "text": "Common Git Tasks\n\nCreating a Repo From Scratch\nSay you have a collection of code that you want to add to a local repository. The syntax would be as follows:\ngit init -b main\ngit add some_program.py\ngit commit -am \"First Commit: Adding my initial code to repo\"\nWhat is happening here?\ninit: Creates an empty repository with a “branch” (-b) named “main”\nadd: indicates that “some_program.py” will be tracked within the repo. Other files in the repo will not be tracked unless you explicitly add them. Alternatively bash git add . would all all files within the repo. There are more complex patterns that can be setup to explicitly exclude/include some files using “.gitignore” files, but that’s a topic for another day.\ncommit: Tells git to “save” a permanent copy of all (-a) tracked files in the repo in there current state and leave a commit message (-m) to leave important notes about the commit. Commit messages are extremely helpful, both for reminding yourself what happened, and indicating to collaborators what’s included in a commit.\n\nYou should avoid leaving short/nondescript commit messages such as “updated code”, because that doesn’t help anyone\nIt is good practice to make a commit any time you make an important change to your project!\n\n\nAdding Your New Repository to GitHub\nGo to github.com and follow the steps outlined in the images bellow:\n\n\n\n\n\n1. Create a new repository\n\n\n\n\n\n\n2. Give it the same name as your local repo. Accept all defaults then click create.\n\n\n\n\n\n\n\n3. The next window will open an empty repo. Copy the url go back to your local git repo and execute the commands listed bellow.\n\n\ngit remote add origin https://github.com/June-Skeeter/SomeTest.git\ngit push --set-upstream origin main\nThese commands will tell your local git where the remote (on the cloud) version of your repository is located and then “push” your local copy to the cloud. If you go back to the github repo and refresh the webpage, you’ll see that whatever files were in your local copy are now visible remotely!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#working-with-an-existing-repository",
    "href": "Documentation/UsingGit.html#working-with-an-existing-repository",
    "title": "Using Git",
    "section": "Working With an Existing Repository",
    "text": "Working With an Existing Repository\nMore often than not, you’ll end up working with an existing repository. You may need to make some changes, which can be added back to the main branch (following the branch protection rules!) or you may just need to run some fully developed code locally on your computer. For example, maybe you need to do some processing for the HOGG site in Manitoba. You can “download” a local copy of the repo and create a branch for your local changes as follows:\ngit clone https://github.com/ubc-micromet/Calculation_Procedures.git\ngit checkout -b MyHoggUpdates\nWhat is happening here?\nclone:: Tells git to download a local copy a repository from a remote location (i.e. a github URL) to your computer.\ncheckout: Tells git to create an new branch (-b) with the name “MyHoggUpdates”. This branch will contain all of your own adjustments, but the main branch on your local machine will not be altered! If you break something (e.g., delete an important code block), its easy to go back to the main branch. Just type bash git checkout main to go back in to the main branch. Note only need to add bash -b after bash checkout when creating a new branch. Git will automatically revert all files in the repo the the main version.\n\nContributing your Changes\nProvided you have permission to write to the repository (this is granted on a per-user basis by repo “owners”), you can contribute any changes you make by pushing a copy of your branch to github. Say you added a new file “new_HOGG_file.ini”, you could add it, commit it, then push to git hub as follows:\ngit add new_HOGG_file.ini\ngit commit -am \"A relevant description of your changes\"\ngit push --set-upstream origin MyHoggUpdates\nNote you only need to run the --set-upstream origin Test once per branch. The second, third, hundredth push you can simply type git push\n\n\nPulling Remote Changes\nSay there were some important updates to the main remote branch that you need to incorporate to code on your local branch. Starting from within MyHoggUpdates You could:\ngit checkout main\ngit pull\ngit checkout MyHoggUpdates\ngit merge main\npull downloads all the changes from the main branch on github to your local machine.\nmerge will automatically incorporate any changes for files that don’t conflict with changes you’ve made to files locally. i.e., if you’re working on “new_HOGG_file.ini”, but someone else changed “existing_BB_file.ini” then the updates to “existing_BB_file.ini” will be brought in without causing any “merge conflicts”. If you also made an edit to “existing_BB_file.ini” that does not match the incoming change, it will create a merge conflict, that needs to be resolved manually.",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#other-useful-commands-and-features",
    "href": "Documentation/UsingGit.html#other-useful-commands-and-features",
    "title": "Using Git",
    "section": "Other Useful Commands and Features",
    "text": "Other Useful Commands and Features\ngit status\nstatus will list which if any files have been changed since the last commit and any files that have been added but are not tracked.\ngit restore existing_BB_file.ini\nrestore: will revert “existing_BB_file.ini” back to it’s state in the most recent commit. This is useful if you broke something and want to “go back” to the way things were. You can give a specific file name after\ngit revert HEAD~n\nrevert allows you go back any “n” number of commits, bash Head~2 would go back to the state you had two commits ago.\nIssues: Opening issues on GitHub allows us to delegate work, discuss task and keep notes on various features.\n\nThey can be labeled as bugs, upgrade requests, etc.\n\nIssues can be assigned to one or more members of the organization, and you can leave comments to discuss issues.\nAn issue remains open until they are “closed”.\n\nGitHub Pages: GitHub can be used to render static sites (like this one). Its a super useful skill/resource. But that’s a whole separate tutorial for another time!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#i-broke-something",
    "href": "Documentation/UsingGit.html#i-broke-something",
    "title": "Using Git",
    "section": "I broke something",
    "text": "I broke something\nSometimes you break something and need to know how to fix it. Pardon the profanity … but Oh Shit, Git is a really helpful resource.",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/index.html",
    "href": "Documentation/index.html",
    "title": "Overview",
    "section": "",
    "text": "This page is intended to serve as a catchall with general information for the lab.\n\nResources for current members of the lab.\nTips and tricks for coding and using github.\nLinks to relevant information.\nOverview of the data pipeline and database.",
    "crumbs": [
      "Documentation",
      "Overview"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/index.html",
    "href": "Documentation/Fieldwork/index.html",
    "title": "Fieldwork maintenance and procedures",
    "section": "",
    "text": "This page covers general procedures for daily monitoring and field site maintenance/calibration. If you need to sechedule a field visit - the availability of lab members by day is shown below in Table 1.\n\nUnder normal operating conditions, maintenance trips should occur every 4-6 weeks for each site. Further trips can be scheduled as needed. Ensure you are review the key information before commencing field work. You can checklists below to help prepare for field visits and ensure you are following standard procedures while in the field.\n\n\n\n\n\nTable 1: Availability of Lab Members for Fieldwork.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\n9\n\n\n\n\nJune\nNot Available\nAvailable\nNot Available\nAvailable\nIf Necessary\nAvailable\nAvailable\nNA\n\n\nTed\nBefore noon\nBefore 2pm*\nNot Available\nBefore 2pm*\nAvailable\nAvailable\nNot Available\n*after Feb 26, before that “Not Available”\n\n\nVanessa\nNot Available\nBefore 1pm\nNot Available\nNot Available\nAvailable\nAvailable\nAvailble\nNA\n\n\nSarah R\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nNA\n\n\nTzu-Yi\nAvailable\nAvailable\nAvailable every other week\nNot Available\nIf Necessary\nAvailable\nAvailable\nNA\n\n\nHimari\nAvailable\nNot Available\nNot Available\nNot Available\nNot Available\nAvailable\nAvailable\nNA",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html",
    "href": "Documentation/Fieldwork/Calibration.html",
    "title": "Calibration Procedures",
    "section": "",
    "text": "This page details the steps for calibrating the 7200 and 7700. Always do the 7200 first, followed by the 7700. You must clean the sensors before calibration.\nTO DO: Should we ‘formally’ require calibration on each site visit?",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#getting-started",
    "href": "Documentation/Fieldwork/Calibration.html#getting-started",
    "title": "Calibration Procedures",
    "section": "Getting Started",
    "text": "Getting Started\n\nMake sure the flow module, tubing, 7700 calibration shroud, and necessary wrenches are ready.\n\nNOTE: At BB1 & BB2, it is in the equipment box, for DSM & RBM you must bring to the field.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#calibrating-the-7200",
    "href": "Documentation/Fieldwork/Calibration.html#calibrating-the-7200",
    "title": "Calibration Procedures",
    "section": "Calibrating the 7200",
    "text": "Calibrating the 7200\n\nOn the laptop open the program ‘Li 7X00’ and connect to instrument. Click the LI-7200 icon and choose ‘calibration’.\nSave the old calibration coefficients. Click “Config Files” tab &gt;&gt; “Save Configuration” &gt;&gt; check all and “continue”.\n\nPlace the file in this directory “C:”. The LI 7200 serial numbers are listed in the table below.\n\nThe naming template is:  yyyymmdd(site visit date)_Configuration_Before_calibrations.l7x\n\nScreenshots should also be taken before and after calibrating and saved in the same directory.\n\n\n\nLI 7200 Serial Numbers\n\n\n\nSite\nValue\n\n\n\n\nBB1\n0816\n\n\nBB2\n0815\n\n\nDMS\n1029\n\n\nRBM\n…\n\n\n\n\nCheck the head serial number information in “LI-7200” &gt; “Calibration” &gt; “Coefficients”. Make sure the head serial number on the sensor matches that in the software (see table above).\n\n\n\n\n\n\n\nConnect the calibration flow tube to the 7200 head. At BB2, DSM & RMB - there is a special intake novel that is always connected to the intake tube? At BB1 you need to do manually disconnect the intake tube first?\n\nTO DO: Add pics for each site?\n\n\nSet CO2 zero\n\nConnect the flow meter to the N2 gas tank and open main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow.\n\nNOTE: REVERSED directions (on is right, off is left)\n\nWatch plot on the program: CO2 should drop to 0. On the calibration screen – green flags show that CO2 concentrations are steady. You may need to change the scale on the chart.\n\nClick “Zero CO2” &gt; “OK”\nClick “Zero H2O &gt; “OK”\nCheck graph values continuously throughout the process to make sure values are correct\n\nClose regulator then close the main N2 tank value and disconnect the flow meter.\n\n\n\nSet CO2 span\n\nMove flow meter to CO2 tank and open the main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank) — To Do: Confirm value?\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow.\n\nNOTE: REVERSED directions (on is right, off is left)\n\nEnter CO2 concentration (ppm), Confirm the exact value on the tank. Then Watch plot on the program: CO2 should increase until it stabilizes. On the calibration screen – green flags show that CO2 concentrations are steady. You may need to change the scale on the chart again.\n\nClick Span CO2” &gt; “OK”\n\nDO NOT Click Span H2O\n\n\nClose regulator then close the main CO2 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\n\n\n\nCheck the Coefficients\n\nYou can find the calibration results in “LI-7200” &gt; “Calibration” &gt; “Manual” tab. Reference the table below for acceptable values. Take a screenshot (alt + print screen) of the manual tab and save it to the Micromet Google drive:\nMicromet Lab/Projects/(**Flux Site**)/Flux-tower/Calibrations/LI-7200/SNXXXX (reference serial number table above)\n\n\nOptimal Calibration Constant Values\n\n\n\n\n\n\n\n\nConstant\nValue\nNotes\n\n\n\n\nCO2 Zero\n0.85 ~ 1.1\nZero is primarily affected by temperature, and the state of the internal chemicals\n\n\nCO2 Span\n0.97 ~ 1.03\nA value outside this range indicates is a warning sign for me that something is not correct with either the instrument (wrong head, bad sensor) or with the tank (not accurately calibrated).\n\n\nH2O Zero\n0.9 ~ 1.2\nSet in lab (dobule check frequency?)\n\n\nH2O Span\n0.9 ~ 1.1\nSet in lab (dobule check frequency?)\n\n\n\n\n\n\n\n\n\nSave the after calibration coefficients. Click “Config Files” tab &gt;&gt; “Save Configuration” &gt;&gt; check all and “continue”. Use same naming/saving convention as above.\nDisconnect calibration tubing from 7200 intake. All the connections on the tubing that stays connected to the ‘T’ on the 7200 head should be wrench tight when finished with calibrations.\nExit the program",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#calibrating-the-7700",
    "href": "Documentation/Fieldwork/Calibration.html#calibrating-the-7700",
    "title": "Calibration Procedures",
    "section": "Calibrating the 7700",
    "text": "Calibrating the 7700\nTO DO: Ask - should we save screenshots for 7700 calibrations?\n\nOpen the program ‘LI-7700’ and go to the data page 1 &gt;&gt; 1 chart CH4 (umol/mol).\nUse calibration cylinder to cover the 7700. Make sure to remove 7700 head cap and washer tube first then orient the tube with the straps on top and slide tube over the instrument.\nAttach the small black tube into the LI-7700 cover and attach other end to the black tube with the flow meter (same one as used for 7200). Make sure to check union connections.\n\n\nSet CH4 zero\n\nConnect the flow meter to the N2 gas tank and open main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow. NOTE: REVERSED directions (on is right, off is left)\nWatch plot on the program: CH4 should drop to 0 (or ~.13). On the calibration screen – green flags show that it’s steady Change scale if needed to check that the trace is flat/steady\n\nClick “Zero CH4” &gt; “Apply”\n\nClose regulator then close the main N2 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\n\n\n\nSet CH4 span\n\nMove flow meter to CH4 tank and open the main valve (on is left, off is right).\n\nNote: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\n\nTO DO: Confirm value?\n\nEnter CH4 concentration (ppm), Confirm the exact value on the tank. Then Watch plot on the program: CH4 should increase until it stabilizes. On the calibration screen – green flags show that CH4 concentrations are steady. You may need to change the scale on the chart again.\n\nClick “Span CH4” &gt; “Apply”\n\nClose regulator then close the main CH4 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\nRemove the calibration shield, make sure to disconnect the tubing first. Then make sure to replace the 7700 head cap and washer tube first.\n\nTO DO: Check the Coefficients",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html",
    "title": "Fieldwork Protocol & Procedures",
    "section": "",
    "text": "This page covers general procedures for doing maintenance and calibration at our flux sites. Under normal operating conditions, maintenance trips should occur every 4-6 weeks for each site. Further trips can be scheduled as needed. Ensure you are review the key information before commencing field work. You can checklists below to help prepare for field visits and ensure you are following standard procedures while in the field.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#planning-a-trip",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#planning-a-trip",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Planning a Trip",
    "text": "Planning a Trip\nMake a clear list of tasks & goals. If you are going to one of the one of the flux stations you can check the field logs. See what was done on the last visit and check for notes to see if there is anything that needs to be done.\n\nBB1&2\nDSM\nRBM\nUse the checklists below to make sure you have all the supplies you need and complete all crucial tasks",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#scheduling-communication",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#scheduling-communication",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Scheduling & Communication",
    "text": "Scheduling & Communication\nLet the lab know when you will be going into the field and make sure you have at least one person joining you.\n\nYou can use the fieldwork slack channel to discuss scheduling\nBook the Micromet Truck using the google calendar\n\nContact Rick for calendar access or to request alternative vehicle options.\n\nIf you are going into Burns Bog:\n\nBoth participants must be permit holders\nYou must also call Metro Vancouver to check in and check out: 604-520-6442",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#safety",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#safety",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Safety",
    "text": "Safety\nEnsure that you are familiar with the lab’s Safety Plan\n\nIn Summer: Be cautions of fire safety:\n\nDon’t park on gras, bring water sprayer spray and fire extinguisher.\n\nIn Winter: Be mindful of the weather forecasts:\n\nDress appropriately, bring extra socks, a head lamp, gloves, etc.\nCheck the tides when going to RBM or DSM",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/ReproducibleDataFlow.html",
    "href": "Documentation/ReproducibleDataFlow.html",
    "title": "Tips for Reproducible Data Flow",
    "section": "",
    "text": "R resources\nHere is some helpful info on setting up a project in R. I’ve adapted the figure from that link to better suit the needs of our research group (see below).\n\n\n\n\n\nOther helpful resources can be found in the ‘R_dynamic_document_overview_html.html’ and ‘0_set_up_workspace.html’ doc in the ‘Coding resources/R resources’ in the Google Drive. These were written by Dan Moore, but are not meant to be distributed publicly.",
    "crumbs": [
      "Documentation",
      "Tips for Reproducible Data Flow"
    ]
  },
  {
    "objectID": "acknowledgement.html",
    "href": "acknowledgement.html",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "",
    "text": "The University of British Columbia and the city of Vancouver are on the traditional, ancestral, and unceded territory of the Coast Salish Peoples. Specifically the UBC Vancouver campus is on xʷməθkʷəy̓əm (Musqueam) land.",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "acknowledgement.html#field-sites",
    "href": "acknowledgement.html#field-sites",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "Field Sites",
    "text": "Field Sites\nOur field sites are located in the Statl̕əẃ (Sto:lo) River Delta (aka the Fraser River Delta). This map below shows the overlapping territories and conveys the multiplicity of occupancy of First Nations wh in and around the Statl̕əẃ delta. This area encompass the traditional territories of a number of First Nations, including the:\n\n\n\nxʷməθkʷəy̓əm\nStó:lō\nsc̓əwaθenaɁɬ təməxʷ (Tsawwassen)\nkwantlen\nSemiahmoo\nStz’uminus\nÁ,LEṈENEȻ ȽTE (W̱SÁNEĆ)\nsq̓əc̓iy̓aɁɬ təməxʷ (Katzie)\nAnd others",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "acknowledgement.html#what-does-it-mean",
    "href": "acknowledgement.html#what-does-it-mean",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "What does it mean?",
    "text": "What does it mean?\n\n\nTraditional and Ancestral Recognize by whom the lands were traditionally used and/or occupied and the cultures have been handed down from generation to generation. The area around UBC was used by many different people, including the xʷməθkʷəy̓əm, əl̓ilwətaɁɬ, and Skwxwú7mesh-ulh Nations. The map to the left shows the overlapping territories of these nations, along with others in the region.\nThese people are are part of a broader linguistic / cultural group of Coast Salish speaking people. The xʷməθkʷəy̓əm and əl̓ilwətaɁɬ speak dialects of Hul’q’umi’num’ / Halq’eméylem / hən̓q̓əmin̓əm̓ and the Skwxwú7mesh-ulh speak Sḵwx̱wú7mesh sníchim.\n\n\n\n\n\n\n\nUnceded: Refers to land that was not turned over to the government by a treaty or other agreement. Over 95% of the land in BC, and many lands elsewhere in the world were never ceded by treaty. Without treaties, these lands remain the sovereign territory of the First Nations that call them home. Yet at the same time, the lands have been claimed by Canada and these First Nations living on these lands lack a framework to express their sovereignty. This by no means absolves the Canadian government of their crimes where lands were “ceded” by treaty. Treaties were more frequently reached by coercion than negotiation. The RCMP was created specifically to force indigenous people off their lands by any means necessary.",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_3_Install_Matlab.html",
    "href": "PipelineDocumentation/2_3_Install_Matlab.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Currently, it is necessary to use Matlab to run the data cleaning scripts.\n\nFirst, check which Matlab version is currently recommended for the pipeline code, in section 8.2. While the data pipeline will most likely work with other recent versions of Matlab, we make no guarantees of cross compatibility.\nVisit this website for details on how to install Matlab, or contact your systems administrator for help.\nMake sure you install Matlab with the following toolboxes:\n\nCurve fitting\nGlobal Optimization\nOptimization\nSignal Processing\nStatistics and Machine Learning.\n\nAdditional toolboxes may be available depending on your institutional/personal licence. However, having every available toolbox installed is likely a waste of your disk space, and is not required for the data pipeline.\nIf you already have the correct version of Matlab on your local computer, you can install the necessary toolboxes using the “Add-Ons” button in the “Home” tab within Matlab.\nWhen you have Matlab along with the relevant toolboxes installed, you are ready to configure it to use with the Biomet.net library.\n\nNOTE: you can have more than one version of Matlab on your computer in case you prefer to use another version of Matlab for other projects.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.3 &nbsp; Install Software: Matlab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_3_Install_Matlab.html#install-matlab-on-your-local-computer",
    "href": "PipelineDocumentation/2_3_Install_Matlab.html#install-matlab-on-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Currently, it is necessary to use Matlab to run the data cleaning scripts.\n\nFirst, check which Matlab version is currently recommended for the pipeline code, in section 8.2. While the data pipeline will most likely work with other recent versions of Matlab, we make no guarantees of cross compatibility.\nVisit this website for details on how to install Matlab, or contact your systems administrator for help.\nMake sure you install Matlab with the following toolboxes:\n\nCurve fitting\nGlobal Optimization\nOptimization\nSignal Processing\nStatistics and Machine Learning.\n\nAdditional toolboxes may be available depending on your institutional/personal licence. However, having every available toolbox installed is likely a waste of your disk space, and is not required for the data pipeline.\nIf you already have the correct version of Matlab on your local computer, you can install the necessary toolboxes using the “Add-Ons” button in the “Home” tab within Matlab.\nWhen you have Matlab along with the relevant toolboxes installed, you are ready to configure it to use with the Biomet.net library.\n\nNOTE: you can have more than one version of Matlab on your computer in case you prefer to use another version of Matlab for other projects.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.3 &nbsp; Install Software: Matlab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/PipelineDocumentation.html",
    "href": "PipelineDocumentation/PipelineDocumentation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Compiled by: Rosie Howard, 2024\nEdits and contributions: Zoran Nesic, Sara Knox, June Skeeter, Paul Moore, and other EcoFlux Lab members and affiliates.\nRevisions:\n- February 2025: “Recently Added Features” section 8.2 added, linked below. Full documentation still in progress.\n- October 2024: these are the “quick-start” instructions for new users of the data-cleaning pipeline. The full documentation will be made available at a later date.\n\n\n\nRecently added pipeline features\nLink to Overview of Data Cleaning Pipeline slides presented during FLUXNET-CH4 V2.0 Workshop (October 21-23, 2024) on Day 1: Afternoon, and Day 2: Morning and Afternoon.\nLink to YouTube playlist of ALL presentations from FLUXNET-CH4 V2.0 Workshop.\n\n\n\n\n\nMotivation: The Importance of Flux Data Standardization and Reproducibility\n 1.1  Note on EddyPro Processing of High Frequency Data\n\nSoftware Installation\n 2.1  Install Software: Git, and Create Github account (optional)\n 2.2  Download Biomet.net Library\n 2.3  Install Software: Matlab\n 2.4  Configure Matlab for Biomet.net\n 2.5  Install Software: R/RStudio\n 2.6  Install Software: Python (optional)\nData Cleaning Principles\n\n\n 4  Quick Start: Project Directory Structure\n\n\n\n\nQuick Start: Create Database\n 5.1  Quick Start: Create Database and Visualize\n\n\n\n\nQuick Start: Create INI Files for Data Cleaning\n 6.1  Quick Start: First Stage INI File\n 6.2  Quick Start: Second Stage INI File\n 6.3  Quick Start: Third Stage and Ameriflux Output\n\nData Visualization\n 7.1  Matlab plotApp\n 7.2  R-Shiny App\n\nTroubleshooting and FAQ\n 8.1  Recommended Software Versions\n 8.2  Recently Added Features",
    "crumbs": [
      "Pipeline Documentation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/PipelineDocumentation.html#data-cleaning-pipeline-documentation-and-instructions",
    "href": "PipelineDocumentation/PipelineDocumentation.html#data-cleaning-pipeline-documentation-and-instructions",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Compiled by: Rosie Howard, 2024\nEdits and contributions: Zoran Nesic, Sara Knox, June Skeeter, Paul Moore, and other EcoFlux Lab members and affiliates.\nRevisions:\n- February 2025: “Recently Added Features” section 8.2 added, linked below. Full documentation still in progress.\n- October 2024: these are the “quick-start” instructions for new users of the data-cleaning pipeline. The full documentation will be made available at a later date.\n\n\n\nRecently added pipeline features\nLink to Overview of Data Cleaning Pipeline slides presented during FLUXNET-CH4 V2.0 Workshop (October 21-23, 2024) on Day 1: Afternoon, and Day 2: Morning and Afternoon.\nLink to YouTube playlist of ALL presentations from FLUXNET-CH4 V2.0 Workshop.\n\n\n\n\n\nMotivation: The Importance of Flux Data Standardization and Reproducibility\n 1.1  Note on EddyPro Processing of High Frequency Data\n\nSoftware Installation\n 2.1  Install Software: Git, and Create Github account (optional)\n 2.2  Download Biomet.net Library\n 2.3  Install Software: Matlab\n 2.4  Configure Matlab for Biomet.net\n 2.5  Install Software: R/RStudio\n 2.6  Install Software: Python (optional)\nData Cleaning Principles\n\n\n 4  Quick Start: Project Directory Structure\n\n\n\n\nQuick Start: Create Database\n 5.1  Quick Start: Create Database and Visualize\n\n\n\n\nQuick Start: Create INI Files for Data Cleaning\n 6.1  Quick Start: First Stage INI File\n 6.2  Quick Start: Second Stage INI File\n 6.3  Quick Start: Third Stage and Ameriflux Output\n\nData Visualization\n 7.1  Matlab plotApp\n 7.2  R-Shiny App\n\nTroubleshooting and FAQ\n 8.1  Recommended Software Versions\n 8.2  Recently Added Features",
    "crumbs": [
      "Pipeline Documentation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html",
    "href": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a generic, quick-start example that converts (1) EddyPro raw data output, and (2) Campbell Scientific TOA5 output, to a formatted database ready to be processed using the standardised libraries (i.e., Biomet.net). We also present some examples of how to quickly check the contents of your new database.\n\n\nAs previously mentioned, we will follow an example that focuses on flux-related EddyPro and met-related Campbell Scientific output data:\n\nFirst, in your newly created Sites directory, within the relevant SITEID directory, create a new Flux folder (figure 5.1).\n\nFigure 5.1. Directory tree showing file path to the location where raw flux data for site with SITEID1 should be stored.\nCopy your EddyPro raw output data to this Flux folder. This data will remain untouched so that you always have a local copy of the original data.\nNow, create a Met folder within the same SITEID directory (SITEID1 in this example), and copy your Campbell Scientific TOA5 data for this site to this Met folder.\nIn your &lt;projectPath&gt;/Matlab folder, create one new “main” Matlab file that will act as a “do-it-all” script. You can name this file however you like; we advise making it meaningful and including the word “Main”. The example given here (figure 5.2) is named DataCleaning_Main.m (you can make your filename less generic) — it will first create the database, and later you can add the code for data cleaning. This script can be copied section by section from the code block at the bottom of this page.\n\nFigure 5.2. Matlab code to create database from raw EddyPro output and Campbell Scientific logger meteorological data. Yellow highlighted text should be edited.\nNext, run your “Main” Matlab program; you should see some data output in your Database directory. Data is grouped by year, then by site, then by data type, e.g., Flux or Met (figure 5.3; Met data not shown here).\n\nFigure 5.3. Directory tree showing file path to output in Database directory following database conversion.\n\nYour data is now in a format ready for cleaning using the pipeline.\n\n\n\nHere are some quick tips to inspect the data in your newly created database, all within Matlab:\n\nplotApp function:\nSimply type plotApp on the Matlab command line, and it will open an app that can compare traces from the same and different cleaning stages (once you have completed those), databases, and also produce statistical plots and outputs. More details of this and other visualization tools are described at length in section 7.\ngui_Browse_folder function:\n pth = biomet_path(yearIn, 'SITEID1', 'Flux')  % define path to folder you wish to browse\n gui_Browse_Folder(pth)\n \nThis function opens a Matlab app that looks in the Flux folder for SITEID1 for a specific year (as defined in the biomet_path function input parameters) and plots each variable in turn. You can scroll through or use the dropdown in order to check that your data looks reasonable and as expected.\nLoad one trace, e.g., co2_mixing_ratio and plot it.\n %% Load one trace and plot it\n pth = biomet_path(yearIn,'SITEID1','Flux');   \n tv = read_bor(fullfile(pth,'clean_tv'),8);      % load the time vector (Matlab's datenum format)\n tv_dt = datetime(tv,'convertfrom','datenum');   % convert to Matlab's datetime object\n x = read_bor(fullfile(pth,'co2_mixing_ratio')); % load co2_mixing_ratio from SITEID1/Flux folder\n plot(tv_dt,x)                                   % plot data\n grid on;zoom on\n \n\n\n\n\nDataCleaning_Main.m template script for copying one section at a time (see Figure 5.2 for necessary edits, highlighted yellow):\n%% Main function for My_Micromet data processing\n% Created by &lt;author&gt; on &lt;date&gt;\n% \n% ============================\n% Setup the project and siteID\nprojectPath = '/Users/&lt;username&gt;/Project/My_Micromet';\nstructProject=set_TAB_project(projectPath);\nsiteID = 'SITEID1';\n\n% Create database from raw data\n%% Flux data from EddyPro output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Flux','MY_EDDYPRO_OUTPUT.csv');\n\n% Read the file \noptionsFileRead.flagFileType = 'fulloutput';    % select fulloutput, biomet, or summary\n[~, ~,tv,outStruct] = fr_read_EddyPro_file(fileName,[],[],optionsFileRead);\n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Flux'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1); \n\n%% Met data from Campbell Scientific TOA5 output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Met','MY_CS_TOA5_OUTPUT.dat');\n\n% Read the file \n[~,~,~,outStruct] = fr_read_TOA5_file(fileName); \n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Met'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1);",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database",
      "5.1 &nbsp; Quick Start: Create Database and Visualize"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "href": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a generic, quick-start example that converts (1) EddyPro raw data output, and (2) Campbell Scientific TOA5 output, to a formatted database ready to be processed using the standardised libraries (i.e., Biomet.net). We also present some examples of how to quickly check the contents of your new database.\n\n\nAs previously mentioned, we will follow an example that focuses on flux-related EddyPro and met-related Campbell Scientific output data:\n\nFirst, in your newly created Sites directory, within the relevant SITEID directory, create a new Flux folder (figure 5.1).\n\nFigure 5.1. Directory tree showing file path to the location where raw flux data for site with SITEID1 should be stored.\nCopy your EddyPro raw output data to this Flux folder. This data will remain untouched so that you always have a local copy of the original data.\nNow, create a Met folder within the same SITEID directory (SITEID1 in this example), and copy your Campbell Scientific TOA5 data for this site to this Met folder.\nIn your &lt;projectPath&gt;/Matlab folder, create one new “main” Matlab file that will act as a “do-it-all” script. You can name this file however you like; we advise making it meaningful and including the word “Main”. The example given here (figure 5.2) is named DataCleaning_Main.m (you can make your filename less generic) — it will first create the database, and later you can add the code for data cleaning. This script can be copied section by section from the code block at the bottom of this page.\n\nFigure 5.2. Matlab code to create database from raw EddyPro output and Campbell Scientific logger meteorological data. Yellow highlighted text should be edited.\nNext, run your “Main” Matlab program; you should see some data output in your Database directory. Data is grouped by year, then by site, then by data type, e.g., Flux or Met (figure 5.3; Met data not shown here).\n\nFigure 5.3. Directory tree showing file path to output in Database directory following database conversion.\n\nYour data is now in a format ready for cleaning using the pipeline.\n\n\n\nHere are some quick tips to inspect the data in your newly created database, all within Matlab:\n\nplotApp function:\nSimply type plotApp on the Matlab command line, and it will open an app that can compare traces from the same and different cleaning stages (once you have completed those), databases, and also produce statistical plots and outputs. More details of this and other visualization tools are described at length in section 7.\ngui_Browse_folder function:\n pth = biomet_path(yearIn, 'SITEID1', 'Flux')  % define path to folder you wish to browse\n gui_Browse_Folder(pth)\n \nThis function opens a Matlab app that looks in the Flux folder for SITEID1 for a specific year (as defined in the biomet_path function input parameters) and plots each variable in turn. You can scroll through or use the dropdown in order to check that your data looks reasonable and as expected.\nLoad one trace, e.g., co2_mixing_ratio and plot it.\n %% Load one trace and plot it\n pth = biomet_path(yearIn,'SITEID1','Flux');   \n tv = read_bor(fullfile(pth,'clean_tv'),8);      % load the time vector (Matlab's datenum format)\n tv_dt = datetime(tv,'convertfrom','datenum');   % convert to Matlab's datetime object\n x = read_bor(fullfile(pth,'co2_mixing_ratio')); % load co2_mixing_ratio from SITEID1/Flux folder\n plot(tv_dt,x)                                   % plot data\n grid on;zoom on\n \n\n\n\n\nDataCleaning_Main.m template script for copying one section at a time (see Figure 5.2 for necessary edits, highlighted yellow):\n%% Main function for My_Micromet data processing\n% Created by &lt;author&gt; on &lt;date&gt;\n% \n% ============================\n% Setup the project and siteID\nprojectPath = '/Users/&lt;username&gt;/Project/My_Micromet';\nstructProject=set_TAB_project(projectPath);\nsiteID = 'SITEID1';\n\n% Create database from raw data\n%% Flux data from EddyPro output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Flux','MY_EDDYPRO_OUTPUT.csv');\n\n% Read the file \noptionsFileRead.flagFileType = 'fulloutput';    % select fulloutput, biomet, or summary\n[~, ~,tv,outStruct] = fr_read_EddyPro_file(fileName,[],[],optionsFileRead);\n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Flux'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1); \n\n%% Met data from Campbell Scientific TOA5 output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Met','MY_CS_TOA5_OUTPUT.dat');\n\n% Read the file \n[~,~,~,outStruct] = fr_read_TOA5_file(fileName); \n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Met'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1);",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database",
      "5.1 &nbsp; Quick Start: Create Database and Visualize"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_1_Install_Git_Optional.html",
    "href": "PipelineDocumentation/2_1_Install_Git_Optional.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. It is useful if you think you may contribute to the Biomet.net library, which is a Git repository.\n\nDownload the latest version of Git for Windows or Mac here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\nNext, if you don’t already have a personal github account, go to Github and set one up.\nOnce these tasks are complete, if you wish to contribute your own code to the Biomet.net library, start by creating a separate git branch to work on. Once you are happy with your code, create a pull request, then someone will review and approve the new files or contact you if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.1 &nbsp; Install Software: Git (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_1_Install_Git_Optional.html#install-git-and-create-personal-github-account-optional",
    "href": "PipelineDocumentation/2_1_Install_Git_Optional.html#install-git-and-create-personal-github-account-optional",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. It is useful if you think you may contribute to the Biomet.net library, which is a Git repository.\n\nDownload the latest version of Git for Windows or Mac here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\nNext, if you don’t already have a personal github account, go to Github and set one up.\nOnce these tasks are complete, if you wish to contribute your own code to the Biomet.net library, start by creating a separate git branch to work on. Once you are happy with your code, create a pull request, then someone will review and approve the new files or contact you if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.1 &nbsp; Install Software: Git (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html",
    "href": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "For those users processing flux data in EddyPro, here we provide general guidelines as to what we recommend using for input parameters.\nThis EddyPro API helps you automate and parallelize EddyPro processing. Within this git repository, you can find our recommended input parameters for the following (linked here):\n\nLi-Cor closed-path systems;\nLi-Cor open-path systems.\n\nIf you are not using Li-Cor instruments or EddyPro then we suggest following this working group: FLUXNET CH4 and N2O processing committee.",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility",
      "1.1 &nbsp; Note: High Frequency Data Processing"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html#a-note-on-high-frequency-data-processing",
    "href": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html#a-note-on-high-frequency-data-processing",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "For those users processing flux data in EddyPro, here we provide general guidelines as to what we recommend using for input parameters.\nThis EddyPro API helps you automate and parallelize EddyPro processing. Within this git repository, you can find our recommended input parameters for the following (linked here):\n\nLi-Cor closed-path systems;\nLi-Cor open-path systems.\n\nIf you are not using Li-Cor instruments or EddyPro then we suggest following this working group: FLUXNET CH4 and N2O processing committee.",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility",
      "1.1 &nbsp; Note: High Frequency Data Processing"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_2_RShiny_App.html",
    "href": "PipelineDocumentation/7_2_RShiny_App.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "We recommend using this R-Shiny app for viewing your data in real-time, and if you wish to display your data publicly.\n\n\nInstructions for using the RShiny_flux_plots.R app (under /Biomet.net/R/RShiny_flux_plots/). To run the R-Shiny app, either locally or on an R-Shiny server, you will need to download, edit, rename and save two files.\n\nDownload the Excel sheet below to input your site coordinates (you can include multiple sites, each as a separate line):\nExample site coordinate file\n\nNote that the standard meridian is equal to your [UTC offset (without daylight savings) x 15]. For example, Vancouver is UTC-8 so the standard meridian is -120.\nAlso, latitude and longitude are in decimal degrees, and negative for West and South coordinates.\n\nRename the Excel file to include your specific Project Name (in place of “TUT” in this example file).\nCreate a new folder called RShiny_flux_plots_ini within &lt;projectPath&gt;/Database/Calculation_Procedures/, and save your Excel site coordinates file within this new folder.\nNext, download this R-Shiny ini file example:\nExample R-Shiny ini file\nEdit the ini file, renaming the two paths at the top of the file, to (a) the Biomet.net library and (b) your Database folder, so they match your local computer.\nSelect which level(s) of data you would like to plot. You can plot ThirdStage data by specifying level &lt;- \"Clean/ThirdStage\", or multiple levels of data using the concatenate command, e.g., level &lt;- c(\"Clean/ThirdStage\",\"Met\").\nAs with the Excel sheet, rename the ini file to include your specific Project Name and save it in under &lt;projectPath&gt;/Database/Calculation_Procedures/RShiny_flux_plots_ini/ (same location as the Excel file).\nNext, copy the code block below to a new R script, again saving it in your new RShiny_flux_plots_ini folder. You can name it as you wish; in this example it is named run_RShiny.R. Edit the filepaths at the top of your new script, as with your INI file, and also enter your RShiny ini filename. Alternatively, you can run these lines of code on the command line, but you must remember to edit the file paths and ini filename.\n # Edit these two file paths to point to biomet.net library and your database directory\n biomet_dir &lt;- \"/Users/rosie/Documents/Micromet/Biomet.net\"  # path to Biomet.net directory \n main_dir &lt;- \"/Users/rosie/Documents/Micromet/Projects/My_Micromet/Database\"   # database path \n\n # Edit to match your RShiny ini filename\n ini_filename &lt;- \"TUT_RShiny_ini.R\"\n\n source(file.path(main_dir,\"Calculation_Procedures/RShiny_flux_plots_ini\",ini_filename))\n source(file.path(biomet_dir,\"R/RShiny_flux_plots/load_save_data.R\"))\n shiny::runApp(file.path(biomet_dir,\"R/RShiny_flux_plots/RShiny_flux_plots.R\"))\nYou can now run the R-Shiny app by running your new script (run_RShiny.R), or as mentioned in step 9, you can run the individual lines of code within the R console. Importantly, remember to change the file paths to match those on your local computer.\n\n\nNote: if you get an error installing the package imager, you may need to try:\nFor imager, you may need to download quartz to be able to run this library properly (https://www.xquartz.org) & then use install.packages(\"igraph\", type=\"binary\")",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization",
      "7.2 &nbsp; R-Shiny App"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_2_RShiny_App.html#r-shiny-app-for-viewing-data-in-real-time",
    "href": "PipelineDocumentation/7_2_RShiny_App.html#r-shiny-app-for-viewing-data-in-real-time",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "We recommend using this R-Shiny app for viewing your data in real-time, and if you wish to display your data publicly.\n\n\nInstructions for using the RShiny_flux_plots.R app (under /Biomet.net/R/RShiny_flux_plots/). To run the R-Shiny app, either locally or on an R-Shiny server, you will need to download, edit, rename and save two files.\n\nDownload the Excel sheet below to input your site coordinates (you can include multiple sites, each as a separate line):\nExample site coordinate file\n\nNote that the standard meridian is equal to your [UTC offset (without daylight savings) x 15]. For example, Vancouver is UTC-8 so the standard meridian is -120.\nAlso, latitude and longitude are in decimal degrees, and negative for West and South coordinates.\n\nRename the Excel file to include your specific Project Name (in place of “TUT” in this example file).\nCreate a new folder called RShiny_flux_plots_ini within &lt;projectPath&gt;/Database/Calculation_Procedures/, and save your Excel site coordinates file within this new folder.\nNext, download this R-Shiny ini file example:\nExample R-Shiny ini file\nEdit the ini file, renaming the two paths at the top of the file, to (a) the Biomet.net library and (b) your Database folder, so they match your local computer.\nSelect which level(s) of data you would like to plot. You can plot ThirdStage data by specifying level &lt;- \"Clean/ThirdStage\", or multiple levels of data using the concatenate command, e.g., level &lt;- c(\"Clean/ThirdStage\",\"Met\").\nAs with the Excel sheet, rename the ini file to include your specific Project Name and save it in under &lt;projectPath&gt;/Database/Calculation_Procedures/RShiny_flux_plots_ini/ (same location as the Excel file).\nNext, copy the code block below to a new R script, again saving it in your new RShiny_flux_plots_ini folder. You can name it as you wish; in this example it is named run_RShiny.R. Edit the filepaths at the top of your new script, as with your INI file, and also enter your RShiny ini filename. Alternatively, you can run these lines of code on the command line, but you must remember to edit the file paths and ini filename.\n # Edit these two file paths to point to biomet.net library and your database directory\n biomet_dir &lt;- \"/Users/rosie/Documents/Micromet/Biomet.net\"  # path to Biomet.net directory \n main_dir &lt;- \"/Users/rosie/Documents/Micromet/Projects/My_Micromet/Database\"   # database path \n\n # Edit to match your RShiny ini filename\n ini_filename &lt;- \"TUT_RShiny_ini.R\"\n\n source(file.path(main_dir,\"Calculation_Procedures/RShiny_flux_plots_ini\",ini_filename))\n source(file.path(biomet_dir,\"R/RShiny_flux_plots/load_save_data.R\"))\n shiny::runApp(file.path(biomet_dir,\"R/RShiny_flux_plots/RShiny_flux_plots.R\"))\nYou can now run the R-Shiny app by running your new script (run_RShiny.R), or as mentioned in step 9, you can run the individual lines of code within the R console. Importantly, remember to change the file paths to match those on your local computer.\n\n\nNote: if you get an error installing the package imager, you may need to try:\nFor imager, you may need to download quartz to be able to run this library properly (https://www.xquartz.org) & then use install.packages(\"igraph\", type=\"binary\")",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization",
      "7.2 &nbsp; R-Shiny App"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html",
    "href": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a quick-start summary for setting up your project directory structure and configuring Matlab to work with this structure. \nYou need the file path to your project root folder for these next steps. This project directory must be on your local computer: avoid putting it on an external drive or cloud drive (like OneDrive or GDrive) as this will cause problems in the steps that follow. Additionally, avoid putting it within the Biomet.net directory.\nYou will also need to create a site ID that relates to the flux site you are working with. This is often an acronym based on the site name (e.g., Burns Bog has siteID = 'BB', Richmond Brackish Marsh has siteID = 'RBM'). Note that the siteID must be all upper case.\n\n\n\nIf you have multiple flux sites, choose one site to begin working with. Using the Matlab command line, define your projectPath and siteID as in the following example:\nFor example, if the name of your project is My_Micromet, and your chosen siteID is SITEID1, you would enter the following commands in the Matlab command window:\nMac example:\n projectPath = '/Users/&lt;username&gt;/Projects/My_Micromet/';\n siteID = 'SITEID1';\nPC example:\n projectPath = 'C:\\Projects\\My_Micromet\\';\n siteID = 'SITEID1';\nNext, in Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned), as follows:\n create_TAB_ProjectFolders(projectPath,siteID)\nBoth input arguments are of type “string”. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info. \nAs the function name suggests, this will create some folders on your computer, and also transfer some necessary small files. In your project root directory (e.g., My_Micromet in figure 4.1), you should now see three new directories with the following names: (1) Database, (2) Matlab, (3) Sites.\n\nFigure 4.1. Directory tree showing contents of project folder after running the create_TAB_ProjectFolders Matlab function.\n[Note: if you are working with an earlier version of Matlab than 2023b, you may get an error when running create_TAB_ProjectFolders (above). In that case, download this zip file, unzip, and put the contents of “My_Micromet_Folder” within your project directory (underneath My_Micromet); make sure your directory structure looks like figure 4.1.]\nFinally, run set_TAB_project(projectPath). This process sets up the Biomet.net toolbox to work with your project.\n\n\n\n\nFor now we will describe only the new directories and files necessary for you to begin data cleaning. \n\n\n\nThe new Database directory contains a Calculation_Procedures directory, and within Calculation_Procedures, there is one called TraceAnalysis_ini.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your siteID (e.g., SITEID1 in figure 4.2), as follows:\n\nFigure 4.2. Directory tree showing contents of TraceAnalysis_ini folder after running the create_TAB_ProjectFolders Matlab function.\nEventually (but not yet!), this Database directory will also contain the following:\n\nInitial database created using your raw data;\nYour site-specific INI files that configure how the pipeline scripts will clean the data;\nThe cleaned data once the INI files have been created and the pipeline has been run.\n\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory, under the appropriate siteID (covered shortly in section 5.1). The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 4.3. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created “behind the scenes” in step 2 above, then used during step 3;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes, and may also be useful to you later when you want to visualize your data.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start: Project Directory Structure"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html#quick-start-project-directory-structure-and-matlab-configuration",
    "href": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html#quick-start-project-directory-structure-and-matlab-configuration",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a quick-start summary for setting up your project directory structure and configuring Matlab to work with this structure. \nYou need the file path to your project root folder for these next steps. This project directory must be on your local computer: avoid putting it on an external drive or cloud drive (like OneDrive or GDrive) as this will cause problems in the steps that follow. Additionally, avoid putting it within the Biomet.net directory.\nYou will also need to create a site ID that relates to the flux site you are working with. This is often an acronym based on the site name (e.g., Burns Bog has siteID = 'BB', Richmond Brackish Marsh has siteID = 'RBM'). Note that the siteID must be all upper case.\n\n\n\nIf you have multiple flux sites, choose one site to begin working with. Using the Matlab command line, define your projectPath and siteID as in the following example:\nFor example, if the name of your project is My_Micromet, and your chosen siteID is SITEID1, you would enter the following commands in the Matlab command window:\nMac example:\n projectPath = '/Users/&lt;username&gt;/Projects/My_Micromet/';\n siteID = 'SITEID1';\nPC example:\n projectPath = 'C:\\Projects\\My_Micromet\\';\n siteID = 'SITEID1';\nNext, in Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned), as follows:\n create_TAB_ProjectFolders(projectPath,siteID)\nBoth input arguments are of type “string”. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info. \nAs the function name suggests, this will create some folders on your computer, and also transfer some necessary small files. In your project root directory (e.g., My_Micromet in figure 4.1), you should now see three new directories with the following names: (1) Database, (2) Matlab, (3) Sites.\n\nFigure 4.1. Directory tree showing contents of project folder after running the create_TAB_ProjectFolders Matlab function.\n[Note: if you are working with an earlier version of Matlab than 2023b, you may get an error when running create_TAB_ProjectFolders (above). In that case, download this zip file, unzip, and put the contents of “My_Micromet_Folder” within your project directory (underneath My_Micromet); make sure your directory structure looks like figure 4.1.]\nFinally, run set_TAB_project(projectPath). This process sets up the Biomet.net toolbox to work with your project.\n\n\n\n\nFor now we will describe only the new directories and files necessary for you to begin data cleaning. \n\n\n\nThe new Database directory contains a Calculation_Procedures directory, and within Calculation_Procedures, there is one called TraceAnalysis_ini.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your siteID (e.g., SITEID1 in figure 4.2), as follows:\n\nFigure 4.2. Directory tree showing contents of TraceAnalysis_ini folder after running the create_TAB_ProjectFolders Matlab function.\nEventually (but not yet!), this Database directory will also contain the following:\n\nInitial database created using your raw data;\nYour site-specific INI files that configure how the pipeline scripts will clean the data;\nThe cleaned data once the INI files have been created and the pipeline has been run.\n\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory, under the appropriate siteID (covered shortly in section 5.1). The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 4.3. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created “behind the scenes” in step 2 above, then used during step 3;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes, and may also be useful to you later when you want to visualize your data.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start: Project Directory Structure"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_Troubleshooting_FAQ.html",
    "href": "PipelineDocumentation/8_Troubleshooting_FAQ.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section presents some general troubleshooting tips outlining things to check to make sure the pipeline scripts will run smoothly. There are also some FAQ, and some guidance on special cases and how to handle them.\nNote that this section is a work in progress and will continue to be updated as the pipeline is further developed and tested.\n\n\n\nWhen running data cleaning (any stage), pay very close attention to the output on your screen. It tells you when everything runs smoothly, and if something goes wrong, in most cases it will be informative as to why and whereabouts things went wrong.\nAvoid having extra white space in your INI file between [TRACE] and [END]. Each parameter must be defined on a new line, with no wraparound. We recommend using a text editor that has line numbers (such as VS Code) to help you avoid and/or diagnose this issue.\nIf you downloaded the Biomet.net library (rather than cloned it), make sure that you renamed it Biomet.net since the download includes “main” in the folder name.\n\n\n\n\n\nI have multiple files containing data from one site, e.g., daily, monthly, or annual files. How do I create one database from all my files?\n\nSee section …\n\nI have multiple flux sites. Once I’ve added one site and created my database with data from that site (and cleaned it, etc.), what are the steps to move on to my next site?\n\nSee section …\n\nHow do I incorporate data from other sources such as nearby climate stations, e.g., for gap-filling, into my database?\n\nSee section …\n\nWhen I run the third stage cleaning, why does it finish so quickly (less than a minute), and/or why is there is no data output in the Database directory?\n\nCheck the log file that is produced automatically when running the third stage (SITEID1_ThirdStageCleaning.log). It is informative and will usually tell you the issue; it is located here:\n\nScreenshot showing the location of the third stage log file.\nCheck that all R packages are installed and loaded. Refer to the code provided in section 2.5 for this.\nCheck that after you downloaded and unzipped the Biomet.net library, you renamed it from Biomet.net-main to Biomet.net.\n\n\n\n\n\nThis section outlines some recurring special cases and how to deal with them:\n\nThere is a variable defined in an include INI file that I have no raw input data for.\nIn this case, you can add the following code to the global variables section of your first stage INI file. Remember to change the name of the “dummyVariable” to match the name of the variable you do not have.\n %--&gt;Avoiding errors due to missing input files \n dateRangeNoData = [datenum(1900,1,1) datenum(1900,12,31)]\n globalVars.Trace.dummyVariable.inputFileName_dates = dateRangeNoData\nE.g., if you are running data cleaning for the year 2023, this code essentially tells the pipeline that for this “dummyVariable”, no data exists for 2023 (and only exists for the year 1900), and the program will continue smoothly with no errors.\nI am using an earlier version of Matlab than 2023b, and I’m getting an error when running the create_TAB_ProjectFolders function. How do I fix this?\nDownload this zip file, unzip, and put the contents of the unzipped directory within your own project directory; make sure your directory structure looks like figure 4.1 in section 4.\nThe gitclone function is used within the create_TAB_ProjectFolders function to transfer (clone) the directory structure and files within. However, gitclone was only added to Matlab 2023b, so you need to download this project directory structure directly.\nIf it is an error related to gitclone, the error will occur on the line that gitclone is called, so you can check this in the error message, e.g., it may look like this:\nError: File: create_TAB_ProjectFolders.m Line: 66 Column: 32\n Incorrect use of '=' operator. To assign a value to a variable, use '='. To compare values for equality, use '=='.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_Troubleshooting_FAQ.html#troubleshooting-faq-and-special-cases",
    "href": "PipelineDocumentation/8_Troubleshooting_FAQ.html#troubleshooting-faq-and-special-cases",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section presents some general troubleshooting tips outlining things to check to make sure the pipeline scripts will run smoothly. There are also some FAQ, and some guidance on special cases and how to handle them.\nNote that this section is a work in progress and will continue to be updated as the pipeline is further developed and tested.\n\n\n\nWhen running data cleaning (any stage), pay very close attention to the output on your screen. It tells you when everything runs smoothly, and if something goes wrong, in most cases it will be informative as to why and whereabouts things went wrong.\nAvoid having extra white space in your INI file between [TRACE] and [END]. Each parameter must be defined on a new line, with no wraparound. We recommend using a text editor that has line numbers (such as VS Code) to help you avoid and/or diagnose this issue.\nIf you downloaded the Biomet.net library (rather than cloned it), make sure that you renamed it Biomet.net since the download includes “main” in the folder name.\n\n\n\n\n\nI have multiple files containing data from one site, e.g., daily, monthly, or annual files. How do I create one database from all my files?\n\nSee section …\n\nI have multiple flux sites. Once I’ve added one site and created my database with data from that site (and cleaned it, etc.), what are the steps to move on to my next site?\n\nSee section …\n\nHow do I incorporate data from other sources such as nearby climate stations, e.g., for gap-filling, into my database?\n\nSee section …\n\nWhen I run the third stage cleaning, why does it finish so quickly (less than a minute), and/or why is there is no data output in the Database directory?\n\nCheck the log file that is produced automatically when running the third stage (SITEID1_ThirdStageCleaning.log). It is informative and will usually tell you the issue; it is located here:\n\nScreenshot showing the location of the third stage log file.\nCheck that all R packages are installed and loaded. Refer to the code provided in section 2.5 for this.\nCheck that after you downloaded and unzipped the Biomet.net library, you renamed it from Biomet.net-main to Biomet.net.\n\n\n\n\n\nThis section outlines some recurring special cases and how to deal with them:\n\nThere is a variable defined in an include INI file that I have no raw input data for.\nIn this case, you can add the following code to the global variables section of your first stage INI file. Remember to change the name of the “dummyVariable” to match the name of the variable you do not have.\n %--&gt;Avoiding errors due to missing input files \n dateRangeNoData = [datenum(1900,1,1) datenum(1900,12,31)]\n globalVars.Trace.dummyVariable.inputFileName_dates = dateRangeNoData\nE.g., if you are running data cleaning for the year 2023, this code essentially tells the pipeline that for this “dummyVariable”, no data exists for 2023 (and only exists for the year 1900), and the program will continue smoothly with no errors.\nI am using an earlier version of Matlab than 2023b, and I’m getting an error when running the create_TAB_ProjectFolders function. How do I fix this?\nDownload this zip file, unzip, and put the contents of the unzipped directory within your own project directory; make sure your directory structure looks like figure 4.1 in section 4.\nThe gitclone function is used within the create_TAB_ProjectFolders function to transfer (clone) the directory structure and files within. However, gitclone was only added to Matlab 2023b, so you need to download this project directory structure directly.\nIf it is an error related to gitclone, the error will occur on the line that gitclone is called, so you can check this in the error message, e.g., it may look like this:\nError: File: create_TAB_ProjectFolders.m Line: 66 Column: 32\n Incorrect use of '=' operator. To assign a value to a variable, use '='. To compare values for equality, use '=='.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html",
    "href": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides quick-start instructions for creating your first and second stage INI files, editing the relevant third stage configuration files, and outputting your clean data in the correct format for submission to Ameriflux.\n\n\nTemplate and Sample INI/config files\n\nDownload the files above; unzip the contents, and copy the TEMPLATE files in the Template_INI_config_files folder to the location shown in figure 6.1:\n\nFigure 6.1. Directory tree showing location of template INI files inside relevant SITEID folder.\nRename the TEMPLATE files you just copied, replacing “TEMPLATE” with your site ID (SITEID1 in this example; this filename format is required).\nThe files in the Tutorial_filled_sample_files folder are completed, “filled-in” versions of the TEMPLATE files that work with the sample data available for download in section 5.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html#quick-start-create-ini-and-other-configuration-files-for-data-cleaning",
    "href": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html#quick-start-create-ini-and-other-configuration-files-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides quick-start instructions for creating your first and second stage INI files, editing the relevant third stage configuration files, and outputting your clean data in the correct format for submission to Ameriflux.\n\n\nTemplate and Sample INI/config files\n\nDownload the files above; unzip the contents, and copy the TEMPLATE files in the Template_INI_config_files folder to the location shown in figure 6.1:\n\nFigure 6.1. Directory tree showing location of template INI files inside relevant SITEID folder.\nRename the TEMPLATE files you just copied, replacing “TEMPLATE” with your site ID (SITEID1 in this example; this filename format is required).\nThe files in the Tutorial_filled_sample_files folder are completed, “filled-in” versions of the TEMPLATE files that work with the sample data available for download in section 5.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html",
    "href": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to create your second stage INI file, in order to carry out the second stage data cleaning. The following steps assume that you have already created a first stage INI file and run first stage cleaning for your flux site.\n\nOpen your site-specific SITEID1_SecondStage.ini for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better) and your site ID.\nThe TEMPLATE_SecondStage.ini file that you downloaded and copied already includes a set of fundamental climate variables and the bare minimum flux variables needed to run the third stage. We advise testing the file with only these variables first. Note that even if you do not change the content of any traces during the second stage, you must still include any trace that you wish to be available for the third stage (and beyond, e.g., Ameriflux submission), i.e., they must be passed along the pipeline.\nTest the second stage data cleaning in Matlab, as follows:\n fr_automated_cleaning(yearIn,'SITEID1',2)\nNote the third and final input parameter is 2 instead of 1, denoting second stage. You should see a new Clean folder directly under the site ID directory in your Database, for whichever year(s) you ran it for. Within that Clean folder, there will be a SecondStage folder (figure 6.4).\n\nFigure 6.4. Directory tree showing location of data output (in this example, variable1, variable2, …) from second stage cleaning.\nYou can add any other relevant variables into the second stage INI file. Again, use the visualization tools to check your data looks as expected, then you are ready to progress to third stage cleaning.\n\n\n\n\nCombine multiple traces into one using the calc_avg_trace function. These traces must already have been defined in the first stage.\nUse the “Evaluate” option to operate on your input traces.\n\nExample: the TA_1_1_1 trace you defined in the first stage has some missing data and there is a nearby reliable climate station with a long record. In the first stage you also defined (and filtered etc.) data from this climate station in a variable called TA_OTHER_SOURCE. Then in the second stage, you can use calc_avg_trace within the Evaluate parameter, as follows:\nEvaluate = 'TA_1_1_1 = calc_avg_trace(clean_tv,TA_1_1_1,TA_OTHER_SOURCE,-1)';\nThe function regresses the two variables provided and uses the resulting linear fit to fill the gap(s) in the first variable. Note that fluxes are gap-filled in stage three.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.2 &nbsp; Quick Start: Second Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html#quick-start-create-your-second-stage-ini-file-for-data-cleaning",
    "href": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html#quick-start-create-your-second-stage-ini-file-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to create your second stage INI file, in order to carry out the second stage data cleaning. The following steps assume that you have already created a first stage INI file and run first stage cleaning for your flux site.\n\nOpen your site-specific SITEID1_SecondStage.ini for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better) and your site ID.\nThe TEMPLATE_SecondStage.ini file that you downloaded and copied already includes a set of fundamental climate variables and the bare minimum flux variables needed to run the third stage. We advise testing the file with only these variables first. Note that even if you do not change the content of any traces during the second stage, you must still include any trace that you wish to be available for the third stage (and beyond, e.g., Ameriflux submission), i.e., they must be passed along the pipeline.\nTest the second stage data cleaning in Matlab, as follows:\n fr_automated_cleaning(yearIn,'SITEID1',2)\nNote the third and final input parameter is 2 instead of 1, denoting second stage. You should see a new Clean folder directly under the site ID directory in your Database, for whichever year(s) you ran it for. Within that Clean folder, there will be a SecondStage folder (figure 6.4).\n\nFigure 6.4. Directory tree showing location of data output (in this example, variable1, variable2, …) from second stage cleaning.\nYou can add any other relevant variables into the second stage INI file. Again, use the visualization tools to check your data looks as expected, then you are ready to progress to third stage cleaning.\n\n\n\n\nCombine multiple traces into one using the calc_avg_trace function. These traces must already have been defined in the first stage.\nUse the “Evaluate” option to operate on your input traces.\n\nExample: the TA_1_1_1 trace you defined in the first stage has some missing data and there is a nearby reliable climate station with a long record. In the first stage you also defined (and filtered etc.) data from this climate station in a variable called TA_OTHER_SOURCE. Then in the second stage, you can use calc_avg_trace within the Evaluate parameter, as follows:\nEvaluate = 'TA_1_1_1 = calc_avg_trace(clean_tv,TA_1_1_1,TA_OTHER_SOURCE,-1)';\nThe function regresses the two variables provided and uses the resulting linear fit to fill the gap(s) in the first variable. Note that fluxes are gap-filled in stage three.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.2 &nbsp; Quick Start: Second Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_1_Recommended_Software_Versions.html",
    "href": "PipelineDocumentation/8_1_Recommended_Software_Versions.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "All three stages of data cleaning (plus conversion to Ameriflux output if needed) can be run from Matlab. Our plotApp for data visualization and analysis during data cleaning is run from Matlab too.\nNote that the third stage Matlab script invokes an R-script, which you will also need installed on your local computer. \nAdditionally, our R-Shiny data visualization tool uses R/R-Studio, and we recommend this app for viewing your data in real-time.\nThe scripts and tools that run the data cleaning are kept in a Git repository (called Biomet.net). These can be downloaded without having Git installed.\nOptionally, you can install Git and create your own Github account, in case you wish to contribute code to the Biomet.net library. \nSome users may also need Python installed, for example, if you are processing high-frequency data. \n\n\n\n\nUpdated on: 5 November 2024\n\n\n\n\n\n\n\nSoftware\nRecommended version as of above date\n\n\n\n\nMatlab\n2024a (at least 2023b)\n\n\nR\nv4.3.3\n\n\nRStudio\n2024.09.0-375\n\n\nPython\n3.9.6 for Mac\n\n\nGit\nLatest available \n\n\n\nYour computer/system administrators should be able to help you with all these installations if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ",
      "8.1 &nbsp; Recommended Software Versions"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_1_Recommended_Software_Versions.html#software-current-recommended-versions",
    "href": "PipelineDocumentation/8_1_Recommended_Software_Versions.html#software-current-recommended-versions",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "All three stages of data cleaning (plus conversion to Ameriflux output if needed) can be run from Matlab. Our plotApp for data visualization and analysis during data cleaning is run from Matlab too.\nNote that the third stage Matlab script invokes an R-script, which you will also need installed on your local computer. \nAdditionally, our R-Shiny data visualization tool uses R/R-Studio, and we recommend this app for viewing your data in real-time.\nThe scripts and tools that run the data cleaning are kept in a Git repository (called Biomet.net). These can be downloaded without having Git installed.\nOptionally, you can install Git and create your own Github account, in case you wish to contribute code to the Biomet.net library. \nSome users may also need Python installed, for example, if you are processing high-frequency data. \n\n\n\n\nUpdated on: 5 November 2024\n\n\n\n\n\n\n\nSoftware\nRecommended version as of above date\n\n\n\n\nMatlab\n2024a (at least 2023b)\n\n\nR\nv4.3.3\n\n\nRStudio\n2024.09.0-375\n\n\nPython\n3.9.6 for Mac\n\n\nGit\nLatest available \n\n\n\nYour computer/system administrators should be able to help you with all these installations if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ",
      "8.1 &nbsp; Recommended Software Versions"
    ]
  },
  {
    "objectID": "PipelineDocumentation/3_Data_Cleaning_Principles.html",
    "href": "PipelineDocumentation/3_Data_Cleaning_Principles.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section outlines the data cleaning principles and processes that should be followed closely and applied at each of the three stages, as well as other relevant information.\n\nFigure 3. Chart outlining the data cleaning principles followed in the pipeline.\n\n\n\nPrinciple: during the first stage of data cleaning we want to keep the best measured data values from each sensor, i.e., a user looking for the best measurements from a particular sensor would want to use the first stage data. Missing data points for periods of more than one half-hour will not be gap-filled in any way. \nAt the most basic level, the first stage collects the raw data, applies min/max filtering, assigns consistent variable names in line with Ameriflux guidelines as far as possible, and moves the relevant files to a Clean folder in preparation for stage two. This folder will be created automatically if it does not already exist.\nThe data are stored as binary files in “single-precision floating-point format” (aka float 32), which importantly means they are readable in most common computer languages and softwares.\n\n\n\n\n\nPrinciple: the second stage focuses on creating the best measured data values for each particular property/variable. For example, if there are two (or more) collocated temperature sensors at the same height e.g., 2 metres above ground level, the second stage is intended to create the “best” trace using both (all) sensors, with the highest precision/accuracy and the fewest missing points. By default, it does this by averaging the traces, and gap-filling by using linear regression. Optionally, data can be gap-filled using a nearby climate station, or one of your own nearby stations if you have one. (Other gap-filling methods, such as using reanalysis data, are currently under development.)\nIn practice, the second stage collects the first stage data, generates the “best” observation for each variable and moves the relevant files to a “Clean/SecondStage” folder in preparation for the third stage.\n\n\n\n\n\nPrinciple: USTAR filtering, gap-filling, and CO2 flux partitioning.\nThe third stage collects the second stage data and implements USTAR filtering, gap-filling, and flux partitioning procedures. For this we use the R package REddyProc. Biomet.net functions allow Matlab to interface with R, so all three stages are run directly from Matlab. For more machine learning approaches to gap-filling, we also implement the random forest approach described in the paper by Kim et al. (2020). Additional methane gap-filling processes currently not part of the pipeline are described here, with instructions here.\n\nWe achieve these principles by setting up various configuration files used in each stage. This process is described later (section 6), but there are a few more steps to complete before that. Next, you will set up your project directory structure, then configure it to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "3. Data Cleaning Principles"
    ]
  },
  {
    "objectID": "PipelineDocumentation/3_Data_Cleaning_Principles.html#data-cleaning-principles",
    "href": "PipelineDocumentation/3_Data_Cleaning_Principles.html#data-cleaning-principles",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section outlines the data cleaning principles and processes that should be followed closely and applied at each of the three stages, as well as other relevant information.\n\nFigure 3. Chart outlining the data cleaning principles followed in the pipeline.\n\n\n\nPrinciple: during the first stage of data cleaning we want to keep the best measured data values from each sensor, i.e., a user looking for the best measurements from a particular sensor would want to use the first stage data. Missing data points for periods of more than one half-hour will not be gap-filled in any way. \nAt the most basic level, the first stage collects the raw data, applies min/max filtering, assigns consistent variable names in line with Ameriflux guidelines as far as possible, and moves the relevant files to a Clean folder in preparation for stage two. This folder will be created automatically if it does not already exist.\nThe data are stored as binary files in “single-precision floating-point format” (aka float 32), which importantly means they are readable in most common computer languages and softwares.\n\n\n\n\n\nPrinciple: the second stage focuses on creating the best measured data values for each particular property/variable. For example, if there are two (or more) collocated temperature sensors at the same height e.g., 2 metres above ground level, the second stage is intended to create the “best” trace using both (all) sensors, with the highest precision/accuracy and the fewest missing points. By default, it does this by averaging the traces, and gap-filling by using linear regression. Optionally, data can be gap-filled using a nearby climate station, or one of your own nearby stations if you have one. (Other gap-filling methods, such as using reanalysis data, are currently under development.)\nIn practice, the second stage collects the first stage data, generates the “best” observation for each variable and moves the relevant files to a “Clean/SecondStage” folder in preparation for the third stage.\n\n\n\n\n\nPrinciple: USTAR filtering, gap-filling, and CO2 flux partitioning.\nThe third stage collects the second stage data and implements USTAR filtering, gap-filling, and flux partitioning procedures. For this we use the R package REddyProc. Biomet.net functions allow Matlab to interface with R, so all three stages are run directly from Matlab. For more machine learning approaches to gap-filling, we also implement the random forest approach described in the paper by Kim et al. (2020). Additional methane gap-filling processes currently not part of the pipeline are described here, with instructions here.\n\nWe achieve these principles by setting up various configuration files used in each stage. This process is described later (section 6), but there are a few more steps to complete before that. Next, you will set up your project directory structure, then configure it to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "3. Data Cleaning Principles"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the EcoFlux Lab!",
    "section": "",
    "text": "Contact Information Dr. Sara Knox sara.knox@mcgill.ca\n\n\n\n\n\nOur research group explores the physical, biological and chemical processes that control trace gas, water and energy fluxes between the land surface and the atmosphere. We investigate how land‑atmosphere exchanges of greenhouse gas fluxes respond to a changing climate and disturbances, and how we can modify land management practices for climate change adaptation and mitigation. We combine micrometeorological measurements with remote sensing and modelling to understand soil-plant-atmosphere interactions across a range of spatial and temporal scales. We collaborate with a broad group of researchers and institutions to help inform and advance climate policy.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Russell, S. J., Windham-Myers, L., Stuart-Haëntjens, E. J., Bergamaschi, B. A., Anderson, F., Oikawa, P., & Knox, S. H. (2023). Increased salinity decreases annual gross primary productivity at a Northern California brackish tidal marsh. Environmental Research Letters, 18(3), 034045. https://doi.org/10.1088/1748-9326/acbbdf\n\n\nQuan, N., Lee, S.-C., Chopra, C., Nesic, Z., Porto, P., Pow, P., et al. (2023). Estimating Net Carbon and Greenhouse Gas Balances of Potato and Pea Crops on a Conventional Farm in Western Canada. Journal of Geophysical Research: Biogeosciences, 128(3), e2022JG007113. https://doi.org/10.1029/2022JG007113\n\n\nLee, S.-C., Knox, S. H., McKendry, I., & Black, T. A. (2022). Biogeochemical and biophysical responses to episodes of wildfire smoke from natural ecosystems in southwestern British Columbia, Canada. Atmospheric Chemistry and Physics, 22(4), 2333–2349. https://doi.org/10.5194/acp-22-2333-2022\n\n\nNyberg, M., Black, T. A., Ketler, R., Lee, S.-C., Johnson, M., Merkens, M., et al. (2022). Impacts of Active Versus Passive Re-Wetting on the Carbon Balance of a Previously Drained Bog. Journal of Geophysical Research: Biogeosciences, 127(9), e2022JG006881. https://doi.org/10.1029/2022JG006881\n\n\nZhang, Z., Poulter, B., Knox, S. H., Stavert, A., McNicol, G., Fluet-Chouinard, E., et al. (2021). Anthropogenic emission is the main contributor to the rise of atmospheric methane during 1993–2017. National Science Review, 9(5). https://doi.org/10.1093/nsr/nwab200\n\n\nFisher, J. B., Keenan, T. F., Buechner, C., Shirkey, G., Perez-Quezada, J. F., Knox, S. H., et al. (2021). Once Upon a Time, in AmeriFlux. Journal of Geophysical Research: Biogeosciences, 126(1), e2020JG006148. https://doi.org/10.1029/2020JG006148\n\n\nDelwiche, K. B., Knox, S. H., Malhotra, A., Fluet-Chouinard, E., McNicol, G., Feron, S., et al. (2021). FLUXNET-CH₄: a global, multi-ecosystem dataset and analysis of methane seasonality from freshwater wetlands. Earth System Science Data, 13(7), 3607–3689. https://doi.org/10.5194/essd-13-3607-2021\n\n\nDronova, I., Taddeo, S., Hemes, K. S., Knox, S. H., Valach, A., Oikawa, P. Y., et al. (2021). Remotely sensed phenological heterogeneity of restored wetlands: linking vegetation structure and function. Agricultural and Forest Meteorology, 296, 108215. https://doi.org/10.1016/j.agrformet.2020.108215\n\n\nKnox, S. H., Bansal, S., McNicol, G., Schafer, K., Sturtevant, C., Ueyama, M., et al. (2021). Identifying dominant environmental predictors of freshwater wetland methane fluxes across diurnal to seasonal time scales. Global Change Biology, 27(15), 3582–3604. https://doi.org/10.1111/gcb.15661\n\n\nLee, S.-C., Black, T. A., Nyberg, M., Merkens, M., Nesic, Z., Ng, D., & Knox, S. H. (2021). Biophysical Impacts of Historical Disturbances, Restoration Strategies, and Vegetation Types in a Peatland Ecosystem. Journal of Geophysical Research: Biogeosciences, 126(10), e2021JG006532. https://doi.org/10.1029/2021JG006532\n\n\nMiller, G. J., Dronova, I., Oikawa, P. Y., Knox, S. H., Windham-Myers, L., Shahan, J., & Stuart-Haëntjens, E. (2021). The Potential of Satellite Remote Sensing Time Series to Uncover Wetland Phenology under Unique Challenges of Tidal Setting. Remote Sensing, 13(18), 3589. https://doi.org/10.3390/rs13183589\n\n\nBogard, M. J., Bergamaschi, B. A., Butman, D. E., Anderson, F., Knox, S. H., & Windham-Myers, L. (2020). Hydrologic Export Is a Major Component of Coastal Wetland Carbon Budgets. Global Biogeochemical Cycles, 34(8), e2019GB006430. https://doi.org/10.1029/2019GB006430\n\n\nKim, Y., Johnson, M. S., Knox, S. H., Black, T. A., Dalmagro, H. J., Kang, M., et al. (2019). Gap-filling approaches for eddy covariance methane fluxes: A comparison of three machine learning algorithms and a traditional method with principal component analysis. Global Change Biology, 26(3), 1499–1518. https://doi.org/10.1111/gcb.14845\n\n\nKnox, S. H., Jackson, R. B., Poulter, B., McNicol, G., Fluet-Chouinard, E., Zhang, Z., et al. (2019). FLUXNET-CH4 Synthesis Activity: Objectives, Observations, and Future Directions. Bulletin of the American Meteorological Society, 100(12), 2607–2632. https://doi.org/10.1175/BAMS-D-18-0268.1\n\n\nMcNicol, G., Knox, S. H., Guilderson, T. P., Baldocchi, D. D., & Silver, W. L. (2019). Where old meets new: An ecosystem study of methanogenesis in a reflooded agricultural peatland. Global Change Biology, 26(2), 772–785. https://doi.org/10.1111/gcb.14916",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#peer-reviewed-journal-articles",
    "href": "Publications.html#peer-reviewed-journal-articles",
    "title": "Publications",
    "section": "",
    "text": "Russell, S. J., Windham-Myers, L., Stuart-Haëntjens, E. J., Bergamaschi, B. A., Anderson, F., Oikawa, P., & Knox, S. H. (2023). Increased salinity decreases annual gross primary productivity at a Northern California brackish tidal marsh. Environmental Research Letters, 18(3), 034045. https://doi.org/10.1088/1748-9326/acbbdf\n\n\nQuan, N., Lee, S.-C., Chopra, C., Nesic, Z., Porto, P., Pow, P., et al. (2023). Estimating Net Carbon and Greenhouse Gas Balances of Potato and Pea Crops on a Conventional Farm in Western Canada. Journal of Geophysical Research: Biogeosciences, 128(3), e2022JG007113. https://doi.org/10.1029/2022JG007113\n\n\nLee, S.-C., Knox, S. H., McKendry, I., & Black, T. A. (2022). Biogeochemical and biophysical responses to episodes of wildfire smoke from natural ecosystems in southwestern British Columbia, Canada. Atmospheric Chemistry and Physics, 22(4), 2333–2349. https://doi.org/10.5194/acp-22-2333-2022\n\n\nNyberg, M., Black, T. A., Ketler, R., Lee, S.-C., Johnson, M., Merkens, M., et al. (2022). Impacts of Active Versus Passive Re-Wetting on the Carbon Balance of a Previously Drained Bog. Journal of Geophysical Research: Biogeosciences, 127(9), e2022JG006881. https://doi.org/10.1029/2022JG006881\n\n\nZhang, Z., Poulter, B., Knox, S. H., Stavert, A., McNicol, G., Fluet-Chouinard, E., et al. (2021). Anthropogenic emission is the main contributor to the rise of atmospheric methane during 1993–2017. National Science Review, 9(5). https://doi.org/10.1093/nsr/nwab200\n\n\nFisher, J. B., Keenan, T. F., Buechner, C., Shirkey, G., Perez-Quezada, J. F., Knox, S. H., et al. (2021). Once Upon a Time, in AmeriFlux. Journal of Geophysical Research: Biogeosciences, 126(1), e2020JG006148. https://doi.org/10.1029/2020JG006148\n\n\nDelwiche, K. B., Knox, S. H., Malhotra, A., Fluet-Chouinard, E., McNicol, G., Feron, S., et al. (2021). FLUXNET-CH₄: a global, multi-ecosystem dataset and analysis of methane seasonality from freshwater wetlands. Earth System Science Data, 13(7), 3607–3689. https://doi.org/10.5194/essd-13-3607-2021\n\n\nDronova, I., Taddeo, S., Hemes, K. S., Knox, S. H., Valach, A., Oikawa, P. Y., et al. (2021). Remotely sensed phenological heterogeneity of restored wetlands: linking vegetation structure and function. Agricultural and Forest Meteorology, 296, 108215. https://doi.org/10.1016/j.agrformet.2020.108215\n\n\nKnox, S. H., Bansal, S., McNicol, G., Schafer, K., Sturtevant, C., Ueyama, M., et al. (2021). Identifying dominant environmental predictors of freshwater wetland methane fluxes across diurnal to seasonal time scales. Global Change Biology, 27(15), 3582–3604. https://doi.org/10.1111/gcb.15661\n\n\nLee, S.-C., Black, T. A., Nyberg, M., Merkens, M., Nesic, Z., Ng, D., & Knox, S. H. (2021). Biophysical Impacts of Historical Disturbances, Restoration Strategies, and Vegetation Types in a Peatland Ecosystem. Journal of Geophysical Research: Biogeosciences, 126(10), e2021JG006532. https://doi.org/10.1029/2021JG006532\n\n\nMiller, G. J., Dronova, I., Oikawa, P. Y., Knox, S. H., Windham-Myers, L., Shahan, J., & Stuart-Haëntjens, E. (2021). The Potential of Satellite Remote Sensing Time Series to Uncover Wetland Phenology under Unique Challenges of Tidal Setting. Remote Sensing, 13(18), 3589. https://doi.org/10.3390/rs13183589\n\n\nBogard, M. J., Bergamaschi, B. A., Butman, D. E., Anderson, F., Knox, S. H., & Windham-Myers, L. (2020). Hydrologic Export Is a Major Component of Coastal Wetland Carbon Budgets. Global Biogeochemical Cycles, 34(8), e2019GB006430. https://doi.org/10.1029/2019GB006430\n\n\nKim, Y., Johnson, M. S., Knox, S. H., Black, T. A., Dalmagro, H. J., Kang, M., et al. (2019). Gap-filling approaches for eddy covariance methane fluxes: A comparison of three machine learning algorithms and a traditional method with principal component analysis. Global Change Biology, 26(3), 1499–1518. https://doi.org/10.1111/gcb.14845\n\n\nKnox, S. H., Jackson, R. B., Poulter, B., McNicol, G., Fluet-Chouinard, E., Zhang, Z., et al. (2019). FLUXNET-CH4 Synthesis Activity: Objectives, Observations, and Future Directions. Bulletin of the American Meteorological Society, 100(12), 2607–2632. https://doi.org/10.1175/BAMS-D-18-0268.1\n\n\nMcNicol, G., Knox, S. H., Guilderson, T. P., Baldocchi, D. D., & Silver, W. L. (2019). Where old meets new: An ecosystem study of methanogenesis in a reflooded agricultural peatland. Global Change Biology, 26(2), 772–785. https://doi.org/10.1111/gcb.14916",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#theses",
    "href": "Publications.html#theses",
    "title": "Publications",
    "section": "Theses",
    "text": "Theses\n\n\nSatriawan, T. (2022). Interannual variability of carbon dioxide (CO₂) and methane (CH₄) fluxes in a temperate bog over a 5-year period (Master’s Thesis). University of British Columbia. https://doi.org/10.14288/1.0416306\n\n\nNyberg, M. (2021). Impacts of restoration and climate variability on peatland GHG fluxes (Master’s Thesis). University of British Columbia. https://doi.org/10.14288/1.0401463\n\n\nRussell, S. J. (2021). Increased salinity decreases annual gross primary productivity of a Northern California brackish wetland (Master’s Thesis). University of British Columbia. https://doi.org/10.14288/1.0406272",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#research-talks-poster-presentations",
    "href": "Publications.html#research-talks-poster-presentations",
    "title": "Publications",
    "section": "Research Talks & Poster Presentations",
    "text": "Research Talks & Poster Presentations\n\n\n\n\n\n\n\nSkeeter, J., & Knox, S. H. (2023, April). Ongoing and Proposed Research in the Burns Bog Ecological Conservancy Area.\n\n\nKnox, S. H., & Skeeter, J. (2023, March). UBC Micrometeorology Lab Studies Review.\n\n\nLu, T.-Y., Russell, S. J., Skeeter, J., Lee, S., Oikawa, P., & Knox, S. H. (2022, December). Investigating environmental controls on carbon exchange and predicting gaseous carbon fluxes at a salt marsh in British Columbia.\n\n\nNg, D., & Knox, S. H. (2022, December). Characterizing within-footprint spatial heterogeneity of CH4 emissions in freshwater wetlands through remote sensing and Footprint-weighted Flux Maps.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2022, September). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2022, June). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2021, December). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nNyberg, M., Knox, S. H., Black, T., Johnson, M., Ketler, R., & Nesic, Z. (2020, December). Impacts of restoration and climate variability on peatland greenhouse gas fluxes.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2020, December). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "OutreachNews.html",
    "href": "OutreachNews.html",
    "title": "Outreach and News",
    "section": "",
    "text": "A new short-term flux tower was installed at the Burns Bog Seedling site to monitor CO2 fluxes from a 7-year old lodge-pole pine stand that sprouted following a fire in Burns Bog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJune and I presented at the CGU Annual meeting which was held in Banff this year."
  },
  {
    "objectID": "OutreachNews.html#section",
    "href": "OutreachNews.html#section",
    "title": "Outreach and News",
    "section": "",
    "text": "A new short-term flux tower was installed at the Burns Bog Seedling site to monitor CO2 fluxes from a 7-year old lodge-pole pine stand that sprouted following a fire in Burns Bog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJune and I presented at the CGU Annual meeting which was held in Banff this year."
  },
  {
    "objectID": "OutreachNews.html#section-1",
    "href": "OutreachNews.html#section-1",
    "title": "Outreach and News",
    "section": "2022",
    "text": "2022\n\nDecember\nA recent talk I gave during the UBC IRES Seminar Series on ‘Wetlands in a changing world: processes, feedbacks, and the climate benefits of wetlands’\n\n\nNovember\nLearn more about our recently funded work looking into the role of wetlands as nature-based climate solutions. Excited to work with this great team of researchers! \nHonoured and thrilled to be named CRC in Eco-Meteorology! This is really a group accomplishment that reflects my amazing team (past and present) and all the wonderful and talented colleagues and mentors I’ve had the opportunity to worth with.\n\n\n\nSeptember\nThanks @Let’s talk science for featuring our work in your career profiles! See the accompanying interview here.\n\nMarion’s paper was featured in EOS:"
  },
  {
    "objectID": "OutreachNews.html#section-2",
    "href": "OutreachNews.html#section-2",
    "title": "Outreach and News",
    "section": "2021",
    "text": "2021\n\nJune\nCoverage of our 2021 Global Change Biology Paper:"
  },
  {
    "objectID": "OutreachNews.html#section-3",
    "href": "OutreachNews.html#section-3",
    "title": "Outreach and News",
    "section": "2019",
    "text": "2019\n\nAugust\nThe UBC Geography department highlighting our 2019 BAMS paper:\n\nAn AmeriFlux blog post highlighting our FLUXNET-CH4 work:\n\n\n\nJune\nA blog post I wrote describing life as a new faculty member:"
  },
  {
    "objectID": "OutreachNews.html#and-earlier",
    "href": "OutreachNews.html#and-earlier",
    "title": "Outreach and News",
    "section": "2018 and Earlier",
    "text": "2018 and Earlier\n\nJune (2016)\nCoverage of our work on wetland restoration in the Sacramento-San Joaquin Delta:\n\n\n\nOctober (2016)\nAn interview I did as part of STEM week at Los Altos High School in Los Altos, California:"
  }
]