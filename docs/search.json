[
  {
    "objectID": "Data.html#field-sites",
    "href": "Data.html#field-sites",
    "title": "Field Sites & Data",
    "section": "Field Sites",
    "text": "Field Sites\nThis web-app shows the sites where operate Eddy Covariance towers and Flux Chamber sites in wetlands across the Metro Vancouver.\n\nClick the points to sites to see more information.\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\nView map in separate window\n\nBurns Bog 1 (BB)\n\n\n\nData Preview Graph (CA-DBB) (2014 - *, Delta, BC, Canada)\nDownload Data for CA-DBB from Ameriflux.\nData from the web plots are available here\n\nprefix BB. note that this is raw data ‚Äì no filtering or QCQA. Database codes are available here.\n\nBB1 Site Photos (also see Dr.¬†Andreas Christen‚Äôs previous photos from when he held his position at UBC)\n\n\n\n\n\n\n\n\n\n\n\nBurns Bog 2 (BB2)\n\n\n\nData Preview Graph (CA-DB2) (2019 - *, Delta, BC, Canada)\nDownload Data for CA-DB2 will soon be available from Ameriflux.\nData from the web plots are available here\n\nprefix BB2. note that this is raw data ‚Äì no filtering or QCQA. Database codes are available here.\n\nBB2 Site Photos\n\n\n\n\n\n\n\n\n\n\n\nBurns Bog Seedling (BBS)\n\n\n\nFlux chambers were installed in spring 2023\nA temporary flux tower was installed in summer 2023\n\nmore info to come\n\n\n\n\n\n\n\n\n\n\n\n\nDelta Salt Marsh (DSM)\n\n\n\nData Preview Graph (CA-DSM) (2021 - *, Delta, BC, Canada)\nData from CA-DSM will soon be available on AmeriFlux.\nData from the web plots are available here\n\nprefix DSM. note that this is raw data ‚Äì no filtering or QCQA.\n\nDSM Site Photos\n\n\n\n\n\n\n\n\n\n\n\nRichmond Brackish Marsh (RBM)\n\n\n\nData Preview Graph (CA-RBM) (2021 - *, Richmond, BC, Canada)\nData from CA-RBM will soon be available on AmeriFlux.\nData from the web plots are available here\n\nprefix RBM. note that this is raw data ‚Äì no filtering or QCQA.\n\nRBM Site Photos\n\n\n\n\n\n\n\n\n\n\n\nUBC Climate Station on Totem Field\n\nAccess to custom data download of standard measured meteorological observations (air temperature, humidity, precipitation, soil temperatures, solar irradiance, wind) measured since 1958. Data can be downloaded in .csv format."
  },
  {
    "objectID": "Data.html#code-and-documentation",
    "href": "Data.html#code-and-documentation",
    "title": "Field Sites & Data",
    "section": "Code and Documentation",
    "text": "Code and Documentation\nOur lab github hosts code for flux processing, README docs for our lab, and various other bits of information.\nMore documentation related to maintenance procedures for our sites can be found here."
  },
  {
    "objectID": "People.html",
    "href": "People.html",
    "title": "People",
    "section": "",
    "text": "Name\nDr.¬†Sara Helen Knox\n\n\nPronouns\nShe/Her/Hers\n\n\nEmail\nsara.knox@mcgill.ca\n\n\nGitHub\nhttps://github.com/sknox01\n\n\n\n\n\nI am broadly interested in the impacts of climate variability and land-use change on land-atmosphere exchanges of water, energy, and trace gases. I also seek to understand how ecosystem responses to global change can feedback to slow or accelerate future climate change. My research and training is in micrometeorology, hydrology, and ecosystem ecology. I focus on biosphere-atmosphere interactions in a variety of climates and ecosystems.\n\nEducation\n\nUniversity of California, Berkeley, PhD\n\n\nCarleton University, MSc\n\n\nMcGill University, BSc\n\n\n\n\n\n\n\n\n\n\nName\nRick Ketler - Project Manager (Geography)\n\n\nEmail\nrick.ketler@ubc.ca\n\n\nOffice\nPCMH B1100\n\n\n\n\n\nCurrently Research Projects Manager for UBC Geography Physical Labs. I have designed and installed research sites in terrain ranging from temperate bogs to arctic tundra. I primarily support professors, PDF‚Äôs and grad students with physical scientific measurements in the field and in the lab.\n\nEducation\n\nB.Sc. Physics Co-op University of Victoria, B.C.\n\n\n\n\n\n\n\n\n\n\nName\nZoran Nesic - Senior Research Engineer\n\n\nEmail\nzoran.nesic@ubc.ca\n\n\nOffice\nMCML 137\n\n\n\n\n\nI am responsible for management of numerous research and equipment design projects for various faculties and departments at UBC. I design eddy covariance and soil respiration systems that are used at various North American universities as well as by other research institutions. My main research interests are (1) the design of automated measurement systems for long-term environmental measurements and (2) the standardization of measurement and data processing procedures to ensure high quality and reproducibility of research results.\n\nEducation\n\nB.Sc. in Electrical Engineering, University of Belgrade\n\n\nM.A.Sc. in Electrical Engineering, University of British Columbia",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#faculty-and-staff",
    "href": "People.html#faculty-and-staff",
    "title": "People",
    "section": "",
    "text": "Name\nDr.¬†Sara Helen Knox\n\n\nPronouns\nShe/Her/Hers\n\n\nEmail\nsara.knox@mcgill.ca\n\n\nGitHub\nhttps://github.com/sknox01\n\n\n\n\n\nI am broadly interested in the impacts of climate variability and land-use change on land-atmosphere exchanges of water, energy, and trace gases. I also seek to understand how ecosystem responses to global change can feedback to slow or accelerate future climate change. My research and training is in micrometeorology, hydrology, and ecosystem ecology. I focus on biosphere-atmosphere interactions in a variety of climates and ecosystems.\n\nEducation\n\nUniversity of California, Berkeley, PhD\n\n\nCarleton University, MSc\n\n\nMcGill University, BSc\n\n\n\n\n\n\n\n\n\n\nName\nRick Ketler - Project Manager (Geography)\n\n\nEmail\nrick.ketler@ubc.ca\n\n\nOffice\nPCMH B1100\n\n\n\n\n\nCurrently Research Projects Manager for UBC Geography Physical Labs. I have designed and installed research sites in terrain ranging from temperate bogs to arctic tundra. I primarily support professors, PDF‚Äôs and grad students with physical scientific measurements in the field and in the lab.\n\nEducation\n\nB.Sc. Physics Co-op University of Victoria, B.C.\n\n\n\n\n\n\n\n\n\n\nName\nZoran Nesic - Senior Research Engineer\n\n\nEmail\nzoran.nesic@ubc.ca\n\n\nOffice\nMCML 137\n\n\n\n\n\nI am responsible for management of numerous research and equipment design projects for various faculties and departments at UBC. I design eddy covariance and soil respiration systems that are used at various North American universities as well as by other research institutions. My main research interests are (1) the design of automated measurement systems for long-term environmental measurements and (2) the standardization of measurement and data processing procedures to ensure high quality and reproducibility of research results.\n\nEducation\n\nB.Sc. in Electrical Engineering, University of Belgrade\n\n\nM.A.Sc. in Electrical Engineering, University of British Columbia",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#current-students-and-postdocs",
    "href": "People.html#current-students-and-postdocs",
    "title": "People",
    "section": "Current Students and PostDocs",
    "text": "Current Students and PostDocs\n\n\n\n\n\n\n\nName\nDr.¬†June Skeeter\n\n\nPosition\nPostdoctoral Scholar\n\n\nPronouns\nThey/Them/Theirs\n\n\nEmail\njune.skeeter@ubc.ca\n\n\nOffice\nGeography Room 127\n\n\nGitHub\nhttps://github.com/June-Skeeter\n\n\n\n\n\nMy name is June. I am a geographer, researcher, and educator who has been living as an uninvited guest on unceded Coast Salish Territory since 2015. I received a PhD in geography from UBC in 2022 and am now working as a postdoctoral researcher for the UBC Micrometeorology group. I study greenhouse gas exchange in wetland ecosystems and associated feedback mechanisms that influence the earth‚Äôs climate system. My research employs a variety of methods from different fields, including: micrometeorology, remote sensing, spatial analysis, and machine learning.\n\n\n\n\n\n\n\n\nName\nDr.¬†Joyson Ahongshangbam\n\n\nPosition\nPostdoctoral Scholar\n\n\nEmail\njoyson.ahongshangbam@mcgill.ca\n\n\nOrcid\nhttps://orcid.org/0000-0002-2678-6879\n\n\n\n\n\nI am interested in studying the carbon and water cycle in different ecosystems (forest, urban vegetation and wetlands) using observations such as eddy covariance, sap flow and remote sensing techniques. My research includes understanding the responses of carbon and water dynamics with the land cover change, management activities and climate change (drought and heatwave).\n\nPrevious experience\n\nUniversity of Helsinki, Finland, Postdoctoral researcher\n\nEducation\n\nUniversity of G√∂ttingen, Germany, PhD\n\n\nIndian Institute of Remote sensing, India, Masters\n\n\n\n\n\n\n\n\n\n\nName\nTzu-Yi Lu\n\n\nLevel of Study\nPh.D.¬†Candidate\n\n\n\n\n\nI received my MS in Geography from National Taiwan University in 2017. I am interested in understanding the response of the wetland ecosystem to climate change, especially in quantifying the net exchange of carbon. My previous research investigated the relationship between environmental controls and CO2 flux in low-latitude wetland ecosystems, applying an Artificial Neural Network technique to simulate the variance of carbon exchange by meteorological variables.\n\n\n\n\n\n\n\n\nName\nKatrina Poppe\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nPh.D.¬†Candidate\n\n\nEmail\npoppek@student.ubc.ca\n\n\n\n\n\nI earned an MS in Environmental Science from Western Washington University in 2016 and continued at WWU as a Research Associate for several years. My previous research has focused primarily on blue carbon, studying soil carbon sequestration rates in Pacific Northwest estuaries and in United Arab Emirates mangroves, in addition to monitoring and modeling vegetation and sediment dynamics in relation to estuary restoration and sea level rise. I am currently interested in studying greenhouse gas fluxes in Pacific Northwest tidal wetlands ‚Äì particularly how they respond to ecosystem restoration and climate change ‚Äì to ultimately better understand the value of tidal wetland management actions as natural climate solutions.\n\n\n\n\n\n\n\n\nName\nSarah Russell\n\n\nLevel of Study\nPh.D.¬†Student\n\n\n\n\n\nI received a BS in Biological Sciences from Wellesley College in 2017, then worked as an ecosystem ecology field technician and research assistant before moving to Vancouver. I am interested in land-atmosphere carbon dynamics and am particularly interested in quantifying the terrestrial carbon sink. My research at UBC involves modeling greenhouse gas fluxes from restored tidal wetlands in the Sacramento-San Joaquin River Delta.\n\n\n\n\n\n\n\n\nName\nTed Scott\n\n\nPronouns\nHe/Him/His\n\n\nLevel of Study\nPh.D.¬†Student\n\n\nEmail\ntedjs@student.ubc.ca\n\n\n\n\n\nPrior to UBC, while at the University of Minnesota, I earned a BS in Computer Science (1997), then a MS (2000) and PhD in Geophysics (2006) under David Kohlstedt, investigating the dynamic properties of partially molten peridotites. My work helped constrain the composition and behavior of highly molten planetary interiors such as Jupiter‚Äôs moon Io. I then worked at Microsoft for several years as a data scientist and engineer, and more recently as a high-school science and math teacher. My current research interests include studying greenhouse gas fluxes in wetland environments. I then synthesize the data collected across multiple sites in various environments to better understand how individually and collectively they can contribute as a natural climate solution. As in my prior PhD, I hope my work can be used to better constrain climate models and lead to sound policy as we address climate change. In my free time, I enjoy trail running and rock climbing.\n\n\n\n\n\n\n\n\nName\nHehan (Zoe) Zhang\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nM.Sc. Student\n\n\nEmail\nhehanzha@student.ubc.ca\n\n\n\n\n\nI received a BSc in Environmental Science from the University of British Columbia in 2022. My undergraduate thesis research investigated the impacts of fire on greenhouse gas (GHG) fluxes (CO2, CH4, and N2O) within different burned zones of a bog. The methodology incorporated chamber measurements and gas chromatography for the analysis. Currently, as a second-year MSc student, I am examining the impacts of seedling removal ‚Äî a post-fire management practice ‚Äî on the GHG fluxes (CO2 and CH4) within the 2016 burned zone of the Burns Bog, Delta, BC. This research utilizes a smart chamber with a portable analyzer. Additionally, I am interested in quantifying the net carbon exchange at the ecosystem scale using eddy covariance techniques and understanding carbon dynamics across varying ecosystem types.\n\n\n\n\n\n\n\n\nName\nVanessa Valenti\n\n\nLevel of Study\nM.Sc. Student\n\n\n\n\n\nI earned a Bachelor‚Äôs degree in Geography/Environmental Studies from the University of California, Los Angeles in 2019. Before coming to UBC, I worked as a scientific programmer at the NASA Goddard Space Flight Center, providing visualization and computation support to earth system and atmosphere climate models. I am interested in modelling land-atmosphere exchanges and projecting responses of wetland and forested ecosystems to climate change.\n\n\n\n\n\n\n\n\nName\nKelsey McGuire\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nB.Sc. Student\n\n\nEmail\nkmcgu@student.ubc.ca\n\n\n\n\n\nI am currently doing a B.Sc in Geographical Sciences, with concentrations in Climatology and Geographic Information Systems. Within the lab, I am supporting various graduate students with their research on how wetland and tidal sites can act as natural climate solutions, where I will help to write code for data processing, and assist on field site visits. I hope in later years to conduct more research on land-atmosphere exchanges, specifically around the carbon cycle, and find ways to incorporate different GIS technologies to help in visualizing it.\n\n\n\n\n\n\n\n\nName\nHimari Honda\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nB.Sc. Student\n\n\nEmail\nhhonda02@student.ubc.ca\n\n\n\n\n\nI am an undergraduate student at UBC majoring in Geographical Sciences with a concentration in climatology. My work in the lab consists of facilitating graduate students with their research and field visits to the flux towers. I am currently working with Zoe Zhang to pursue further research on the effects of heatwaves on carbon dioxide and methane fluxes at restored peatlands. In the near future, I am interested in studying how greenhouse gas exchange is influenced by varying biophysical and meteorological factors.",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#lab-alumni",
    "href": "People.html#lab-alumni",
    "title": "People",
    "section": "Lab Alumni",
    "text": "Lab Alumni\n\n\n\nDarian Ng\n\n(M.Sc. Student)\n\n\n\n\n\nAdin Litman\n\n(B.Sc. Student)\n\n\n\n\n\n\n\n\nTin Satriawan\n\n(M.Sc. Student)\n\n\n\n\n\nDr.¬†Sung-Ching (Nick) Lee\n\n(Postdoctoral Scholar)\n\n\n\n\n\n\n\n\nMarion Nyberg\n\n(M.Sc. Student)\n\n\n\n\n\nAylin Barreras-Apodaca\n\n(Visiting International M.Sc. Student)\n\n\n\n\n\n\n\n\nNicole Choi\n\n(B.Sc. Student)\n\n\n\n\n\nCristina Mace\n\n(B.Sc. Student)\n\n\n\n\n\n\n\n\nAzumi Konaka\n\n(B.Sc. Student)\n\n\n\n\n\nWeiwen Fu\n\n(B.Sc. Student)",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "Meetings.html",
    "href": "Meetings.html",
    "title": "Lab Meetings",
    "section": "",
    "text": "Lab meetings are held weekly on Mondays from 12:30-1:30 Pacific time, the schedule is listed below. You can sign up for a day/topic using the google sheet link (contact june for access).\n\n\n\n\nTable¬†1: Schedule for Fall 2023 Lab meetings.\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nLead\nTopic\n4\n\n\n\n\nJan 12\nJune\nDiscussing WTD corrections at BB\nNA\n\n\nJan 19\n‚Äì\nQuick Check-in\nNA\n\n\nJan 26\nJune\nEddy Pro Automation Overview\nNA\n\n\nFeb 2\n‚Äì\nQuick Check-in\nNA\n\n\nFeb 9\nZoe\nHow to recompute chamber fluxes + Growing season analysis\nNA\n\n\nFeb 16\n‚Äì\nQuick Check-in\nNA\n\n\nFeb 23\nNo Lab Meeting\nReading Break\nNA\n\n\nMar 1\nTed\nprelim gap-filling via SSA results ü§û\nNA\n\n\nMar 8\nNo Lab Meeting\nNA\nNA\n\n\nMar 15\nVanessa\nprobably research proposal discussion/feedback\nNA\n\n\nMar 22\nTzu-Yi\nEGU Practice Presentation\nNA\n\n\nMar 29\nNo Lab Meeting\nGood Friday\nNA\n\n\nApr 5\n‚Äì\nQuick Check-in\nNA\n\n\nApr 12\nKatrina\npossibly soil sequestration results\nNA\n\n\nApr 19\nJoyson\nWetland cooling potential - first results\nNA\n\n\nApr 26\n‚Äì\nQuick Check-in\nNA\n\n\nMay 3\n‚Äì\nQuick Check-in\nNA\n\n\nMay 10\nSarah R\nparitioning results\nNA\n\n\nMay 17\nJune\nQuick Check-in\nNA\n\n\nMay 24\nZoe\nCGU Practice Presentation\nNA\n\n\nMay 31\n‚Äì\nQuick Check-in\nVanessa and Tzu-Yi away (field school TA)",
    "crumbs": [
      "Home",
      "Lab Meetings"
    ]
  },
  {
    "objectID": "Join.html",
    "href": "Join.html",
    "title": "Join the Lab!",
    "section": "",
    "text": "Thank you for your interest in joining our lab! I am always happy to hear from students and postdocs interested in our ongoing research activities.\nPlease contact me by email (sara.knox@ubc.ca) for more details. The deadline for applications in geography is mid-December for MSc students and early January for PhD students, however, I encourage you to contact me well before this deadline. Also, please keep in mind the deadlines for external funding opportunities through NSERC. While graduate student support at UBC is available through teaching assistantships and UBC fellowships, I strongly encourage prospective students to apply for external funding.",
    "crumbs": [
      "Home",
      "Join the lab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_Data_Visualization.html",
    "href": "PipelineDocumentation/7_Data_Visualization.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "In progress‚Ä¶\nFunctions for visualizing data:\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons.\n\nguiPlotTraces\n\nplotApp.\n\n\n\nIn progress‚Ä¶",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_Data_Visualization.html#data-visualization-apps",
    "href": "PipelineDocumentation/7_Data_Visualization.html#data-visualization-apps",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "In progress‚Ä¶\nFunctions for visualizing data:\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons.\n\nguiPlotTraces\n\nplotApp.\n\n\n\nIn progress‚Ä¶",
    "crumbs": [
      "Pipeline Documentation",
      "7. Data Visualization"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html",
    "href": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to create your second stage INI file, in order to carry out the second stage data cleaning. The following steps assume that you have already created a first stage INI file and run first stage cleaning for your flux site.\n\nOpen your site-specific SITEID1_SecondStage.ini for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better) and your site ID.\nThe TEMPLATE_SecondStage.ini file that you downloaded and copied already includes a set of fundamental climate variables and the bare minimum flux variables to continue on and run the third stage. We advise testing the file with only these variables first. Note that even if you do not change the content of any traces during the second stage, you must still include any trace that you wish to be available for the third stage (and beyond, e.g., Ameriflux submission), i.e., they must be passed along the pipeline.\nTest the second stage data cleaning in Matlab, as follows:\n fr_automated_cleaning(yearIn,'SITEID',2)\nNote the input parameter 2 instead of 1, denoting second stage. You should see a new Clean folder directly under the site ID directory in your Database, for whichever year(s) you ran it for. Within that Clean folder, there will be a SecondStage folder (figure 6.4).\n\nFigure 6.4. Directory tree showing location of data output (in this example, variable1, variable2, ‚Ä¶) from second stage cleaning.\nYou can add any other relevant variables into the second stage INI file. Again, use the visualization tools to check your data looks as expected, then you are ready to progress to third stage cleaning.\n\n\n\n\n\nCombine multiple traces into one using the calc_avg_trace function. These traces must already have been defined in the first stage.\nUse the ‚ÄúEvaluate‚Äù option to operate on your input traces.\n\nExample: the TA_1_1_1 trace you defined in the first stage has some missing data and there is a nearby reliable climate station with a long record. In the first stage you also defined (and filtered etc.) data from this climate station in a variable called TA_OTHER_SOURCE. Then in the second stage, you can use calc_avg_trace within the Evaluate parameter, as follows:\nEvaluate = 'TA_1_1_1 = calc_avg_trace(clean_tv,TA_1_1_1,TA_OTHER_SOURCE,-1)';\nThe function regresses the two variables provided and uses the resulting linear fit to fill the gap(s) in the first variable. Note that fluxes are gap-filled in stage three.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.2 &nbsp; Quick Start: Second Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html#quick-start-create-your-second-stage-ini-file-for-data-cleaning",
    "href": "PipelineDocumentation/6_2_Quick_Start_Create_Second_Stage_INI.html#quick-start-create-your-second-stage-ini-file-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to create your second stage INI file, in order to carry out the second stage data cleaning. The following steps assume that you have already created a first stage INI file and run first stage cleaning for your flux site.\n\nOpen your site-specific SITEID1_SecondStage.ini for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better) and your site ID.\nThe TEMPLATE_SecondStage.ini file that you downloaded and copied already includes a set of fundamental climate variables and the bare minimum flux variables to continue on and run the third stage. We advise testing the file with only these variables first. Note that even if you do not change the content of any traces during the second stage, you must still include any trace that you wish to be available for the third stage (and beyond, e.g., Ameriflux submission), i.e., they must be passed along the pipeline.\nTest the second stage data cleaning in Matlab, as follows:\n fr_automated_cleaning(yearIn,'SITEID',2)\nNote the input parameter 2 instead of 1, denoting second stage. You should see a new Clean folder directly under the site ID directory in your Database, for whichever year(s) you ran it for. Within that Clean folder, there will be a SecondStage folder (figure 6.4).\n\nFigure 6.4. Directory tree showing location of data output (in this example, variable1, variable2, ‚Ä¶) from second stage cleaning.\nYou can add any other relevant variables into the second stage INI file. Again, use the visualization tools to check your data looks as expected, then you are ready to progress to third stage cleaning.\n\n\n\n\n\nCombine multiple traces into one using the calc_avg_trace function. These traces must already have been defined in the first stage.\nUse the ‚ÄúEvaluate‚Äù option to operate on your input traces.\n\nExample: the TA_1_1_1 trace you defined in the first stage has some missing data and there is a nearby reliable climate station with a long record. In the first stage you also defined (and filtered etc.) data from this climate station in a variable called TA_OTHER_SOURCE. Then in the second stage, you can use calc_avg_trace within the Evaluate parameter, as follows:\nEvaluate = 'TA_1_1_1 = calc_avg_trace(clean_tv,TA_1_1_1,TA_OTHER_SOURCE,-1)';\nThe function regresses the two variables provided and uses the resulting linear fit to fill the gap(s) in the first variable. Note that fluxes are gap-filled in stage three.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.2 &nbsp; Quick Start: Second Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html",
    "href": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides quick-start instructions for create your first and second stage INI files, editing the relevant third stage configuration files, and outputting your clean data in the correct format for submission to Ameriflux.\n\n\nData files will be available soon‚Ä¶\n\nFirst Stage Template INI file\nSecond Stage Template INI file\nThird Stage Configuration file\n\n\n\nDownload the files above; put these TEMPLATE files in the location shown in figure 6.1:\n\nFigure 6.1. Directory tree showing location of template INI files inside relevant SITEID folder.\nMake copies of all three files that begin ‚ÄúTEMPLATE‚Äù and rename the copies, replacing ‚ÄúTEMPLATE‚Äù with your site ID (SITEID1 in this example; this filename format is required).",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html#quick-start-create-ini-and-other-configuration-files-for-data-cleaning",
    "href": "PipelineDocumentation/6_Quick_Start_Create_INI_Files.html#quick-start-create-ini-and-other-configuration-files-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides quick-start instructions for create your first and second stage INI files, editing the relevant third stage configuration files, and outputting your clean data in the correct format for submission to Ameriflux.\n\n\nData files will be available soon‚Ä¶\n\nFirst Stage Template INI file\nSecond Stage Template INI file\nThird Stage Configuration file\n\n\n\nDownload the files above; put these TEMPLATE files in the location shown in figure 6.1:\n\nFigure 6.1. Directory tree showing location of template INI files inside relevant SITEID folder.\nMake copies of all three files that begin ‚ÄúTEMPLATE‚Äù and rename the copies, replacing ‚ÄúTEMPLATE‚Äù with your site ID (SITEID1 in this example; this filename format is required).",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_0_Note_High_Freq_Data_Processing.html",
    "href": "PipelineDocumentation/1_0_Note_High_Freq_Data_Processing.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "For those users processing flux data in EddyPro, here we provide general guidelines as to what we recommend using for input parameters.\nThis EddyPro API helps you automate and parallelize EddyPro processing. Within this git repository, you can find our recommended input parameters for the following (linked here):\n\nLi-Cor closed-path systems;\nLi-Cor open-path systems.\n\nIf you are not using Li-Cor instruments or EddyPro then we suggest following this working group: FLUXNET CH4 and N2O processing committee.",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility",
      "1.0 &nbsp; Note: High Frequency Data Processing"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_0_Note_High_Freq_Data_Processing.html#a-note-on-high-frequency-data-processing",
    "href": "PipelineDocumentation/1_0_Note_High_Freq_Data_Processing.html#a-note-on-high-frequency-data-processing",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "For those users processing flux data in EddyPro, here we provide general guidelines as to what we recommend using for input parameters.\nThis EddyPro API helps you automate and parallelize EddyPro processing. Within this git repository, you can find our recommended input parameters for the following (linked here):\n\nLi-Cor closed-path systems;\nLi-Cor open-path systems.\n\nIf you are not using Li-Cor instruments or EddyPro then we suggest following this working group: FLUXNET CH4 and N2O processing committee.",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility",
      "1.0 &nbsp; Note: High Frequency Data Processing"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_0_Recommended_Software_Versions.html",
    "href": "PipelineDocumentation/8_0_Recommended_Software_Versions.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "In progress‚Ä¶\n\n\n\nAll three stages of data cleaning can be run from Matlab. \nNote that the third stage Matlab script invokes an R-script, which you will also need installed on your local computer. \nThe scripts and tools that run the data cleaning are kept in a Git repository\nOptionally, you can install Git and create your own Github account. \nSome users may also need Python installed. \n\n\n\n\nIn progress‚Ä¶\nUpdated on: October 2024 \n\n\n\n\n\n\n\nSoftware\nRecommended version as of above date\n\n\n\n\nMatlab\n2023b / 2024a?\n\n\nR\nv4.3.3\n\n\nRStudio\n?\n\n\nPython\n3.9.6 for Mac\n\n\nGit\nLatest available \n\n\n\nYour computer/system administrators should be able to help you with all these installations if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ",
      "8.0 &nbsp; Recommended Software Versions"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_0_Recommended_Software_Versions.html#software-current-recommended-versions",
    "href": "PipelineDocumentation/8_0_Recommended_Software_Versions.html#software-current-recommended-versions",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "In progress‚Ä¶\n\n\n\nAll three stages of data cleaning can be run from Matlab. \nNote that the third stage Matlab script invokes an R-script, which you will also need installed on your local computer. \nThe scripts and tools that run the data cleaning are kept in a Git repository\nOptionally, you can install Git and create your own Github account. \nSome users may also need Python installed. \n\n\n\n\nIn progress‚Ä¶\nUpdated on: October 2024 \n\n\n\n\n\n\n\nSoftware\nRecommended version as of above date\n\n\n\n\nMatlab\n2023b / 2024a?\n\n\nR\nv4.3.3\n\n\nRStudio\n?\n\n\nPython\n3.9.6 for Mac\n\n\nGit\nLatest available \n\n\n\nYour computer/system administrators should be able to help you with all these installations if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ",
      "8.0 &nbsp; Recommended Software Versions"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html",
    "href": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The third stage cleaning generally requires the least amount of work by the user, but is usually the most computationally intensive stage, as it includes running models for gap-filling fluxes. The following example assumes you have already completed first and second stage cleaning for one site.\n\nOpen your site-specific SITEID1_config.yml for editing (figure 6.5):\n\nFigure 6.5. Directory tree showing location of third stage custom YAML file that must be copied (green highlighted text) and edited (yellow highlighted text).\nAt the top of your site-specific file, input the site ID, the year that measurements at the site began, and the metadata for your site (figure 6.6; yellow highlighted text):\n\nFigure 6.6. Third stage site-specific custom YAML file showing which fields to edit in yellow highlighted text.\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the TraceAnalysis_ini directory, and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs and if they already exist in the global_config.yml they will be overwritten by the site-specific settings.\nNote on gap-filling FCH4: currently the predictors for all random forest models used to fill gaps are set to: Predictors: SW_IN_1_1_1,TA_1_1_1,VPD_1_1_1. However, for FCH4, these inputs should be changed to prioritize soil variables such as soil temperature, soil moisture, and water table depth. You can change these settings under ‚ÄúOptional parameters‚Äù (figure 6.7; peach highlighting). \n\nFigure 6.7. Third stage site-specific custom YAML file showing where to change inputs for FCH4 random forest gap-filling.\nNext, test the third stage data cleaning in Matlab; remember that it can take a lot longer to run than first and second stages. Note that the cleaning stage argument for third stage cleaning is 7 (not 3; this is a legacy artifact), as follows:\n fr_automated_cleaning(yearIn,siteID,7)  % third stage\nThe output will appear in two directories: ThirdStage and ThirdStage_Default_Ustar within the Clean directory, where the second stage output is; again, we recommend that you inspect this data using the visualization tools. \n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux:\nfr_automated_cleaning(yearIn,SITEID,8)  % Ameriflux CSV output\nThe output will appear in an Ameriflux directory within the Clean directory, where the second and third stage output is.\nNote that you can run all stages at once (or a subset, provided the previous stages to the subset have already been run, i.e., the data exists):\nfr_automated_cleaning(yearIn,SITEID,[1 2 7 8])  % all three stages plus Ameriflux output",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.3 &nbsp; Quick Start: Third Stage and Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html#quick-start-third-stage-cleaning-and-converting-to-ameriflux-output",
    "href": "PipelineDocumentation/6_3_Quick_Start_Third_Stage_Ameriflux.html#quick-start-third-stage-cleaning-and-converting-to-ameriflux-output",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The third stage cleaning generally requires the least amount of work by the user, but is usually the most computationally intensive stage, as it includes running models for gap-filling fluxes. The following example assumes you have already completed first and second stage cleaning for one site.\n\nOpen your site-specific SITEID1_config.yml for editing (figure 6.5):\n\nFigure 6.5. Directory tree showing location of third stage custom YAML file that must be copied (green highlighted text) and edited (yellow highlighted text).\nAt the top of your site-specific file, input the site ID, the year that measurements at the site began, and the metadata for your site (figure 6.6; yellow highlighted text):\n\nFigure 6.6. Third stage site-specific custom YAML file showing which fields to edit in yellow highlighted text.\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the TraceAnalysis_ini directory, and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs and if they already exist in the global_config.yml they will be overwritten by the site-specific settings.\nNote on gap-filling FCH4: currently the predictors for all random forest models used to fill gaps are set to: Predictors: SW_IN_1_1_1,TA_1_1_1,VPD_1_1_1. However, for FCH4, these inputs should be changed to prioritize soil variables such as soil temperature, soil moisture, and water table depth. You can change these settings under ‚ÄúOptional parameters‚Äù (figure 6.7; peach highlighting). \n\nFigure 6.7. Third stage site-specific custom YAML file showing where to change inputs for FCH4 random forest gap-filling.\nNext, test the third stage data cleaning in Matlab; remember that it can take a lot longer to run than first and second stages. Note that the cleaning stage argument for third stage cleaning is 7 (not 3; this is a legacy artifact), as follows:\n fr_automated_cleaning(yearIn,siteID,7)  % third stage\nThe output will appear in two directories: ThirdStage and ThirdStage_Default_Ustar within the Clean directory, where the second stage output is; again, we recommend that you inspect this data using the visualization tools. \n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux:\nfr_automated_cleaning(yearIn,SITEID,8)  % Ameriflux CSV output\nThe output will appear in an Ameriflux directory within the Clean directory, where the second and third stage output is.\nNote that you can run all stages at once (or a subset, provided the previous stages to the subset have already been run, i.e., the data exists):\nfr_automated_cleaning(yearIn,SITEID,[1 2 7 8])  % all three stages plus Ameriflux output",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.3 &nbsp; Quick Start: Third Stage and Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_0_Quick_Start_Template_FilesOLD.html",
    "href": "PipelineDocumentation/6_0_Quick_Start_Template_FilesOLD.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Data files will be available soon..\n\n\n\nFirst Stage Template INI file\nSecond Stage Template INI file\nThird Stage Configuration file\n\n\n\nDownload the files above; put these TEMPLATE files in the location shown in figure 6.1:\n\nFigure 6.1. Directory tree showing location of template INI files inside relevant SITEID folder.\nMake copies of all three files that begin ‚ÄúTEMPLATE‚Äù and rename the copies, replacing ‚ÄúTEMPLATE‚Äù with your site ID (SITEID1 in this example; this filename format is required)."
  },
  {
    "objectID": "PipelineDocumentation/6_0_Quick_Start_Template_FilesOLD.html#quick-start-download-template-files-and-sample-data",
    "href": "PipelineDocumentation/6_0_Quick_Start_Template_FilesOLD.html#quick-start-download-template-files-and-sample-data",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Data files will be available soon..\n\n\n\nFirst Stage Template INI file\nSecond Stage Template INI file\nThird Stage Configuration file\n\n\n\nDownload the files above; put these TEMPLATE files in the location shown in figure 6.1:\n\nFigure 6.1. Directory tree showing location of template INI files inside relevant SITEID folder.\nMake copies of all three files that begin ‚ÄúTEMPLATE‚Äù and rename the copies, replacing ‚ÄúTEMPLATE‚Äù with your site ID (SITEID1 in this example; this filename format is required)."
  },
  {
    "objectID": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html",
    "href": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to start cleaning your data for stage one. Note that the example assumes you have already created Flux and Met databases for the same site.\n\n\n\n\nOpen your site-specific SITEID1_FirstStage.ini file for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better), your site ID, and the timezone information. Difference_GMT_to_local_time is the timezone that your site is located in, and Timezone is the timezone that your data is reported in (UTC, standard time, or daylight saving time). Both these parameters are currently necessary in order to output the correct timezone for Ameriflux requirements (local standard time).\nNext, scroll down (skipping ‚ÄúGlobal Variables‚Äù sections for now) to where you see:\n %----------------------------------------------------\n %--&gt; Insert met variables here\n %----------------------------------------------------\nBeginning with air temperature, define the input data to each trace given in the INI file. For example, if you have an output variable that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 6.2; yellow highlighted text). During this first cleaning stage this trace will be renamed as TA_1_1_1 using the variableName parameter, following the Ameriflux naming convention.\n\nFigure 6.2. Air temperature trace as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab‚Äôs datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs). \n\nOnce you have done this for a few variables, test the data cleaning in Matlab, using the fr_automated_cleaning.m function as follows:\n fr_automated_cleaning(yearIn,'SITEID',1)\nwhere yearIn (YYYY) is the year of data you want to clean (it is possible to clean multiple years at once but we will keep things simple with one year for now), SITEID would be SITEID1 from the earlier example, and ‚Äò1‚Äô represents the first stage of data cleaning.\nProvided everything worked with no errors, you should now see output data matching the variables you defined in a new Clean folder within your Met folder..\n\n\n\nIf you do NOT have a four-component net radiometer such as a CNR4 at your site, you can skip all of step 5 and go to step 6. If you do have a CNR4 or similar, we have provided an ‚Äúinclude‚Äù INI file that loads relevant radiation variables for you. We strongly advise that you do not edit this or any other include file. Instead, scroll to the bottom of your site-specific INI file where the include files are called.\n\nUncomment* (e.g., delete % - NOT #) the line containing RAD_FirstStage_include.ini, so that the block of code looks like this:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n%#include EddyPro_Common_FirstStage_include.ini\n%#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n%#include EddyPro_LI7700_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\n*Recall that this file is read by Matlab, so comments are created using %; the # sign must be present to call the include file.\nNext, scroll almost to the top of the file where you see:\n%----------------------------------------------------------\n% Global variable specification (trace-specific)\n%----------------------------------------------------------\n%--&gt; Radiation sensor information: \n%--&gt; If using CNR4 with RAD_FirstStage_include file, add inputFileNames etc., for example:\n%globalVars.Trace.SW_IN_1_1_1.inputFileName     = {'MET_CNR4_SWi_Avg'}\n\nglobalVars.Trace.SW_IN_1_1_1.inputFileName              = {''}\nglobalVars.Trace.SW_IN_1_1_1.inputFileName_dates        = [datenum(1900,1,1) datenum(2999,12,31)]\nglobalVars.Trace.SW_IN_1_1_1.instrument                 = 'CNR4'\nglobalVars.Trace.SW_IN_1_1_1.instrumentSN               = ''\nFor SW_IN_1_1_1 (incoming shortwave radiation), add:\n\nThe inputFileName from your database;\nEdit the start date for your data record in inputFileName_dates;\nEdit instrument (if necessary); and\nAdd the serial number (instrumentSN).\n\nAll these parameters are used for the other radiation variables defined below this one, apart from inputFileName, which you will need to input for each variable. Also, if you have a PAR radiometer you can use the PPFD global variables in this radiation section in the same way.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this change. This step will add the radiation variables into the same Met folder as in step 4.\n\nNext, we will clean your Flux variables. As previously mentioned if you followed step 5, we have provided ‚Äúinclude‚Äù files that load most information on common traces for you.\n\nTo use these files scroll to the bottom of your first stage INI file. For anyone with EddyPro output, you need to use EddyPro_Common_FirstStage_include.ini, which contains all variables common to all IRGAs; then, also uncomment the other line(s) for your specific IRGA(s).\nFor example, let‚Äôs assume you are using a LiCor LI-7200 and LI-7700 at your site. Then you would uncomment the relevant include files, as follows:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n#include EddyPro_LI7700_FirstStage_include.ini\n%#include RAD_FirstStage_include.ini\nNow scroll up to:\n%---------------------------------------\n% Global variables (instrument-specific)\n%---------------------------------------\nEnter/edit the relevant parameter details for your IRGA(s) (instrument, serial number, start date of measurements). Do the same for your anemometer and full eddy-covariance (EC) system instruments (e.g., CSAT3 plus IRGA). ‚ÄòotherTraces‚Äô includes all instrument types not previously defined within global variables, and must be enabled, with measurement record dates defined.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this addition. You should see your first-stage cleaned data appear in a new Clean folder within your Flux folder.\n\nAt this stage, you can start to add any Met or Flux variables that may not already be included in the template files. Also, for example, if you have more than one air temperature measurement, you would create more traces to assign these, and use the Ameriflux naming convention to distinguish and define their relative positions (figure 6.3).\n\nFigure 6.3. Second air temperature trace defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\nTIPS\n\n\nPay attention to the output display, it is informative.\n\n\nWe recommend using the ‚Äúquick-look‚Äù visualization tools at any stage of cleaning to check your data looks as expected (e.g., the filenames are correct and the values conform to your minMax bounds).",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.1 &nbsp; Quick Start: First Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html#quick-start-create-your-first-stage-ini-file-for-data-cleaning",
    "href": "PipelineDocumentation/6_1_Quick_Start_Create_First_Stage_INI.html#quick-start-create-your-first-stage-ini-file-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to start cleaning your data for stage one. Note that the example assumes you have already created Flux and Met databases for the same site.\n\n\n\n\nOpen your site-specific SITEID1_FirstStage.ini file for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better), your site ID, and the timezone information. Difference_GMT_to_local_time is the timezone that your site is located in, and Timezone is the timezone that your data is reported in (UTC, standard time, or daylight saving time). Both these parameters are currently necessary in order to output the correct timezone for Ameriflux requirements (local standard time).\nNext, scroll down (skipping ‚ÄúGlobal Variables‚Äù sections for now) to where you see:\n %----------------------------------------------------\n %--&gt; Insert met variables here\n %----------------------------------------------------\nBeginning with air temperature, define the input data to each trace given in the INI file. For example, if you have an output variable that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 6.2; yellow highlighted text). During this first cleaning stage this trace will be renamed as TA_1_1_1 using the variableName parameter, following the Ameriflux naming convention.\n\nFigure 6.2. Air temperature trace as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab‚Äôs datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs). \n\nOnce you have done this for a few variables, test the data cleaning in Matlab, using the fr_automated_cleaning.m function as follows:\n fr_automated_cleaning(yearIn,'SITEID',1)\nwhere yearIn (YYYY) is the year of data you want to clean (it is possible to clean multiple years at once but we will keep things simple with one year for now), SITEID would be SITEID1 from the earlier example, and ‚Äò1‚Äô represents the first stage of data cleaning.\nProvided everything worked with no errors, you should now see output data matching the variables you defined in a new Clean folder within your Met folder..\n\n\n\nIf you do NOT have a four-component net radiometer such as a CNR4 at your site, you can skip all of step 5 and go to step 6. If you do have a CNR4 or similar, we have provided an ‚Äúinclude‚Äù INI file that loads relevant radiation variables for you. We strongly advise that you do not edit this or any other include file. Instead, scroll to the bottom of your site-specific INI file where the include files are called.\n\nUncomment* (e.g., delete % - NOT #) the line containing RAD_FirstStage_include.ini, so that the block of code looks like this:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n%#include EddyPro_Common_FirstStage_include.ini\n%#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n%#include EddyPro_LI7700_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\n*Recall that this file is read by Matlab, so comments are created using %; the # sign must be present to call the include file.\nNext, scroll almost to the top of the file where you see:\n%----------------------------------------------------------\n% Global variable specification (trace-specific)\n%----------------------------------------------------------\n%--&gt; Radiation sensor information: \n%--&gt; If using CNR4 with RAD_FirstStage_include file, add inputFileNames etc., for example:\n%globalVars.Trace.SW_IN_1_1_1.inputFileName     = {'MET_CNR4_SWi_Avg'}\n\nglobalVars.Trace.SW_IN_1_1_1.inputFileName              = {''}\nglobalVars.Trace.SW_IN_1_1_1.inputFileName_dates        = [datenum(1900,1,1) datenum(2999,12,31)]\nglobalVars.Trace.SW_IN_1_1_1.instrument                 = 'CNR4'\nglobalVars.Trace.SW_IN_1_1_1.instrumentSN               = ''\nFor SW_IN_1_1_1 (incoming shortwave radiation), add:\n\nThe inputFileName from your database;\nEdit the start date for your data record in inputFileName_dates;\nEdit instrument (if necessary); and\nAdd the serial number (instrumentSN).\n\nAll these parameters are used for the other radiation variables defined below this one, apart from inputFileName, which you will need to input for each variable. Also, if you have a PAR radiometer you can use the PPFD global variables in this radiation section in the same way.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this change. This step will add the radiation variables into the same Met folder as in step 4.\n\nNext, we will clean your Flux variables. As previously mentioned if you followed step 5, we have provided ‚Äúinclude‚Äù files that load most information on common traces for you.\n\nTo use these files scroll to the bottom of your first stage INI file. For anyone with EddyPro output, you need to use EddyPro_Common_FirstStage_include.ini, which contains all variables common to all IRGAs; then, also uncomment the other line(s) for your specific IRGA(s).\nFor example, let‚Äôs assume you are using a LiCor LI-7200 and LI-7700 at your site. Then you would uncomment the relevant include files, as follows:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n#include EddyPro_LI7700_FirstStage_include.ini\n%#include RAD_FirstStage_include.ini\nNow scroll up to:\n%---------------------------------------\n% Global variables (instrument-specific)\n%---------------------------------------\nEnter/edit the relevant parameter details for your IRGA(s) (instrument, serial number, start date of measurements). Do the same for your anemometer and full eddy-covariance (EC) system instruments (e.g., CSAT3 plus IRGA). ‚ÄòotherTraces‚Äô includes all instrument types not previously defined within global variables, and must be enabled, with measurement record dates defined.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this addition. You should see your first-stage cleaned data appear in a new Clean folder within your Flux folder.\n\nAt this stage, you can start to add any Met or Flux variables that may not already be included in the template files. Also, for example, if you have more than one air temperature measurement, you would create more traces to assign these, and use the Ameriflux naming convention to distinguish and define their relative positions (figure 6.3).\n\nFigure 6.3. Second air temperature trace defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\nTIPS\n\n\nPay attention to the output display, it is informative.\n\n\nWe recommend using the ‚Äúquick-look‚Äù visualization tools at any stage of cleaning to check your data looks as expected (e.g., the filenames are correct and the values conform to your minMax bounds).",
    "crumbs": [
      "Pipeline Documentation",
      "6. Quick Start: Create INI Files for Data Cleaning",
      "6.1 &nbsp; Quick Start: First Stage INI File"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_5_Install_R_Studio.html",
    "href": "PipelineDocumentation/2_5_Install_R_Studio.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "As with all the other software installations for use in data cleaning, first check which versions are recommended at present.\n\n\nInstallation instructions for R on Windows and Mac can be found here: cran.rstudio.com.\nInstallation instructions for RStudio on Windows and Mac: posit.co/download/rstudio-desktop.\nNext, install the following libraries in RStudio by running the install.packages() command, as follows:\ninstall.packages(c('REddyProc','fs','yaml','rlist','data.table','tidyverse','caret','ranger','zoo','randomForest')) ADD MORE HERE!",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.5 &nbsp; Install Software: R/RStudio"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_5_Install_R_Studio.html#install-r-and-rstudio-on-your-local-computer",
    "href": "PipelineDocumentation/2_5_Install_R_Studio.html#install-r-and-rstudio-on-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "As with all the other software installations for use in data cleaning, first check which versions are recommended at present.\n\n\nInstallation instructions for R on Windows and Mac can be found here: cran.rstudio.com.\nInstallation instructions for RStudio on Windows and Mac: posit.co/download/rstudio-desktop.\nNext, install the following libraries in RStudio by running the install.packages() command, as follows:\ninstall.packages(c('REddyProc','fs','yaml','rlist','data.table','tidyverse','caret','ranger','zoo','randomForest')) ADD MORE HERE!",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.5 &nbsp; Install Software: R/RStudio"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_2_Clone_Biomet_Library.html",
    "href": "PipelineDocumentation/2_2_Clone_Biomet_Library.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The Biomet.net repository contains libraries of computer code (in Matlab, R, Python, etc.) related to running the data cleaning pipeline. It contains (a) the scripts to run the first, second, and third cleaning stages, as well as conversion to AmeriFlux format, and also (b) many functions to help analyse and visualise data throughout all the stages of data processing and cleaning.\nIn principle, users should not need to edit this code library. Instead, as far as possible, you will interface with it using various configuration files unique to your own project(s). This process is described later. First, you need to clone the repository.\n\n\n\nMake sure you are in the directory (folder) where you wish the repository to live, for example, your computer C: drive for PCs, or /Users/&lt;username&gt;/ for Macs, or something similar depending on your preferences.\nNext, follow the instructions on this website to clone the repository.\nIf you have any difficulties or are unsure of git procedures in general, see this online tutorial.\nAlways keep your local copy of the Biomet.net code up to date by periodically using the git pull command every few days, or when you know there has been a change to the code. You must be in the repository directory for this to work.\n\nNOTE: It is not good practice to save any of your code into the Biomet.net folder! If you wish to contribute your own code to the Biomet.net library, see section 2.1.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.2 &nbsp; Clone Biomet Library"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_2_Clone_Biomet_Library.html#clone-the-biomet.net-library-git-repository-to-your-local-computer",
    "href": "PipelineDocumentation/2_2_Clone_Biomet_Library.html#clone-the-biomet.net-library-git-repository-to-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The Biomet.net repository contains libraries of computer code (in Matlab, R, Python, etc.) related to running the data cleaning pipeline. It contains (a) the scripts to run the first, second, and third cleaning stages, as well as conversion to AmeriFlux format, and also (b) many functions to help analyse and visualise data throughout all the stages of data processing and cleaning.\nIn principle, users should not need to edit this code library. Instead, as far as possible, you will interface with it using various configuration files unique to your own project(s). This process is described later. First, you need to clone the repository.\n\n\n\nMake sure you are in the directory (folder) where you wish the repository to live, for example, your computer C: drive for PCs, or /Users/&lt;username&gt;/ for Macs, or something similar depending on your preferences.\nNext, follow the instructions on this website to clone the repository.\nIf you have any difficulties or are unsure of git procedures in general, see this online tutorial.\nAlways keep your local copy of the Biomet.net code up to date by periodically using the git pull command every few days, or when you know there has been a change to the code. You must be in the repository directory for this to work.\n\nNOTE: It is not good practice to save any of your code into the Biomet.net folder! If you wish to contribute your own code to the Biomet.net library, see section 2.1.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.2 &nbsp; Clone Biomet Library"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_SoftwareInstallation.html",
    "href": "PipelineDocumentation/2_SoftwareInstallation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides instructions on installing the software and configuring the libraries needed for running the data-cleaning pipeline.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_SoftwareInstallation.html#software-installation",
    "href": "PipelineDocumentation/2_SoftwareInstallation.html#software-installation",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides instructions on installing the software and configuring the libraries needed for running the data-cleaning pipeline.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation"
    ]
  },
  {
    "objectID": "Pubs_list.html#theses",
    "href": "Pubs_list.html#theses",
    "title": "Publications",
    "section": "Theses",
    "text": "Theses"
  },
  {
    "objectID": "Pubs_list.html#research-talks-poster-presentations",
    "href": "Pubs_list.html#research-talks-poster-presentations",
    "title": "Publications",
    "section": "Research Talks & Poster Presentations",
    "text": "Research Talks & Poster Presentations"
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html",
    "href": "Documentation/Fieldwork/Cleaning.html",
    "title": "Cleaning Procedures",
    "section": "",
    "text": "This page details the steps for cleaning the 7700 and 7200. Always do the 7700 first, followed by the 7200. You must clean the sensors before calibration.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#getting-started",
    "href": "Documentation/Fieldwork/Cleaning.html#getting-started",
    "title": "Cleaning Procedures",
    "section": "Getting Started",
    "text": "Getting Started\n\nConnect the laptop to the site system using the ethernet cable and dongle to the top box on the outside of the scaffolding.\n\nNOTE: remote connection via vinimet & mobile hotspot can also work in a pinch.\n\n\nTO DO: Add pics of logger box at each each site for reference.\n\nRetract the boom to access the flux sensors. Loosen the two bolts securing the boom &gt; pull quick release tab&gt; slide boom in &gt; Be mindful of cables\n\nFor BB1 & DSM: place stepladder on plywood on the ground. Make sure it‚Äôs solid.\nFor BB2 & RBM: loosen the wing nut &gt; remove the fastening boot &gt; lift up sensors to reduce force needed to pull out the bolt &gt; rotate the arm to bring the sensors within reach from platform. Be careful this can be quite difficult.\n\n\n\n\nBB1 & DSM Cleaning\n\n\n\n\n\n\nBB2 & RBM Cleaning\n\n\n\n\n\n\n\n\nDo a visual inspection of the sensors and note/address any obvious issues.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7700",
    "href": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7700",
    "title": "Cleaning Procedures",
    "section": "Cleaning the LI 7700",
    "text": "Cleaning the LI 7700\n\nOn the laptop, open the program ‚ÄòLI-7700‚Äô. Click ‚ÄúConnect‚Äù &gt;&gt; Ethernet &gt;&gt; Select Instrument &gt;&gt; Connect. Open 2nd data page and look at the single graph with signal strength\n\nTO DO: Add Screenshot\n\nWipe top and bottom windows with windshield cleaner (wet then dry cloth) until signal strength is sufficient:\n\n\n\n\n\n\nTO DO: Replace w/ another (different) image on cleaning mirrors?\n\nDesired LI 7700 Signal Strength by Site\n\n\n\nSite\nValue\n\n\n\n\nBB1\nHigh 50‚Äôs\n\n\nBB2\nAround 80\n\n\nDMS\nAround 70\n\n\nRBM\n‚Ä¶\\\n\n\n\n\nDisconnect from the 7700 and close the LI-7700 program.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7200",
    "href": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7200",
    "title": "Cleaning Procedures",
    "section": "Cleaning the LI 7200",
    "text": "Cleaning the LI 7200\n\nPower down the flux system. Inside the power box, flip the 3 switches as shown in the table below. The power is off when the red light is on.\n\n\nEC Sensor Power Channels\n\n\n\nSite\nLI-7200 flow module (pump)\nLI-7700\nLI-7550\n\n\n\n\nBB1\n12\n7\n2\n\n\nBB2\n3\n4\n5\n\n\nDMS\n3\n4\n5\n\n\nRBM\nTBD\nTBD\nTBD\n\n\n\n\nTake the intake tube off by loosening the nut with a wrench. The tube can then just hang loosely from the tower, disconnected from the 7200. Then loosen the knobs on top of 7200 to release the top and take side part off to open the 7200 sensor head. This can can hang next to the instrument while cleaning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWipe top and bottom windows with windshield cleaner (wet then dry cloth). Then clean intake filter with a wipe.\n\nYou may need to replace with a spare clean cap intake. If so: remove the foil wrap, loosen the metal ring with pliers and push it all the way to the tube, remove and replace the cap, use pliers to put the metal ring back on the cap\n\nPower system back up using switches listed in the table above.\n\nNOTE: The start up sequence is: 1) LI-7200 pump, 2) LI-7700, 3) LI-7550. Allow 90 seconds between each sensor when powering up. This order is important for proper start up.\nInside the Li-7550 box, the USB logging light should be flashing\n\nOn the laptop open the LI 7x00 program and connect the the 7500 to check that the signal strength is sufficient:\n\n\n\nDesired LI 7200 Signal Strength by Site\n\n\n\nSite\nValue\n\n\n\n\nBB1\nAbout 100\n\n\nBB2\nAbout 103\n\n\nDMS\nAbout 102\n\n\nRBM\n‚Ä¶",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DataEntry.html",
    "href": "Documentation/Fieldwork/DataEntry.html",
    "title": "Data Entry",
    "section": "",
    "text": "This page features links to various spots to input the hand-collected data when out in the field (e.g., Water Table Depths). The sheets are all hosted on the Lab‚Äôs Google Drive under the Project‚Äôs tab.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Data entry"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DataEntry.html#sites",
    "href": "Documentation/Fieldwork/DataEntry.html#sites",
    "title": "Data Entry",
    "section": "Sites",
    "text": "Sites\n\nBurns Bog 1\n\n\n\n\n\n\n\nWater Table Height\n\nAnother Link?\n\n\n\n\nBurns Bog 2\n\n\n\n\n\n\n\nWater Table Height\n\nAnother Link?",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Data entry"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html",
    "href": "Documentation/Fieldwork/DailyMonitoring.html",
    "title": "Daily Site Monitoring",
    "section": "",
    "text": "Below are instructions for visual checks of the data that should be done every day. Web plots for each site can be found by going to:",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html#check-critical-variables",
    "href": "Documentation/Fieldwork/DailyMonitoring.html#check-critical-variables",
    "title": "Daily Site Monitoring",
    "section": "Check Critical Variables",
    "text": "Check Critical Variables\n\nMotor Power (%) & Flow Fate for LI-7200 pump\n\nKeep checking if this one changes over time (over weeks).\n\nFlow drive % should be 50 - 90%\nHigher power means more air-flow resistance in the tubing indicating either clogged up filter (if we use those), dirty rain cap wire-mesh or damaged tubing\n\nWe aim for 30-min averages flow rate of slpm = 15 L/min;\n\nBelow 13 L/min we begin to worry, (data quality drops);\nBelow 10 L/min we need to fix it ASAP (data quality questionable or data missing) by cleaning up the intake tube cap\n\n\n\n\n\nGood Flow at BB1\n\n\n\n\n\nBad Flow at BB2\n\n\n\n\nThermocouples\n\nLI-7200: these two values should be very close to each other (&lt;1 C difference)\n\nIn (t_in_LI_7200)\nOut (t_out_LI_7200)\n\nLI-7700\n\nAir temperature\n\n\n\n\nClimate Data\n\nCheck if there are new data & if the values are reasonable (for all sites)\n\nClick on ‚Äúlast day‚Äù / ‚Äúlast week‚Äù to filter the time series\nIf you see BB1 and BB2 precip data don‚Äôt align with each other, there‚Äôs a possibility that the tipping bucket is clogged.\n\n\n\n\nFlux Data\nFor Fluxes and Gas Concentrations check if there are new data & if the values are reasonable. These will only be calculated if smart flux is running properly.\nGas Concentrations:\n\nCO2 mixing ratio (dry) - during daytime should be around 380-420 ppm (umol/mol), during night time it can be quite high depending on the site (sometimes &gt; 600 ppm)\nCH4 mixing ratio (dry) - should be around 2 ppm\n\nFluxes:\n\nSensible & Latent heat - Typically between -100 and + 450 w m-2\n\nShould generally sum ~ 70%-80% of net radiation\n\nFCO2 & FCH4 - site dependent, but look for gaps, extreme spikes, systematic jumps\n\nSignal strength\n\nView over a longer period to check for any trends. Schedule a site visit if it‚Äôs decreasing rapidly (&lt;10%). It‚Äôs normal for 7700‚Äôs signal strength to drop during rainy periods, but it‚Äôs not normal for 7200‚Äôs to drop that frequently (if it does, check the 7200 head‚Äôs o-ring).\n\nVoltage\n\nLook at data over the past month and tell June or Rick if the battery voltage goes below 24 V at midnight.\n\nThis usually happens in the winter when we have a few days of clouds & rain.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html#remote-login-procedures",
    "href": "Documentation/Fieldwork/DailyMonitoring.html#remote-login-procedures",
    "title": "Daily Site Monitoring",
    "section": "Remote Login Procedures",
    "text": "Remote Login Procedures\nConnect to VPN, go to Remote Desktop & connect to vinimet.geog.ubc.ca (or it‚Äôs better to do this from your personal computer so you won‚Äôt kick off or get kicked off by other people using Vinimet). Note that doing this on your personal computer requires a PC.\n\nLI 7200\nOpen the LI-7x00 software on desktop and connect using the right IP Address\n\n\n\nConnecting to the BB1 7200\n\n\n\nOnce on the main page check:\n\nThe logger status ‚Üí should be logging\nUSB Free Space ‚Üí make sure there‚Äôs still enough space in the USB (usually the USB stick can handle 6-weeks data)\nThe SmartFlux module has to be connected to the system, make sure it‚Äôs not ‚Äúnone‚Äù\nHead pressure - should be between -0.8 to -3.8 kPa\nFlow drive - should be around 50% to 90%\nFlow rate - should be around 13-15 l/m\n\n\n\n\n\nChecking the BB1 7200\n\n\nClick on the ‚ÄúDiagnostics‚Äù tab - check if all parameters are ‚ÄúOK‚Äù\n\n\n\nChecking the BB1 7200 Diagnostics\n\n\n\n\nLI 7700\nOpen the LI-7700 software on desktop and use the same IP address as for the 7200\n\n\n\nConnecting to the BB1 7200\n\n\nCheck 7700 Optics RH. If it‚Äôs &gt;15%, we should replace the internal chemicals.\n\n\n\nChecking the BB1 7700\n\n\n\nCheck SmartFlux internal memory every 2-3 months\n\nInstructions to check memory are under Note_SmartFlux internal memory_ssh connection.docx\nInstructions to clear memory are under Note_SmartFlux internal memory_updater.docx\n\n\nInfo from here",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html",
    "href": "Documentation/LabInfo.html",
    "title": "Information for Current Lab Members",
    "section": "",
    "text": "This page provides information and links to documentation that are relevant to current members of the lab",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#information-for-new-lab-members",
    "href": "Documentation/LabInfo.html#information-for-new-lab-members",
    "title": "Information for Current Lab Members",
    "section": "Information for new lab members",
    "text": "Information for new lab members\n\nAdd your info to the ‚ÄòLab contact information‚Äô doc in Google Drive\nReview the Grad Expectations document \nCoordinate a day/time for your bi-weekly lab meeting with Sara\nMake sure you‚Äôve been added to the Google Drive account and Slack Workspace\nSend bio and picture to Sara so that she can add you to the lab website\nReview program requirements (see below). More information on potential courses can be found here.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#program-requirements",
    "href": "Documentation/LabInfo.html#program-requirements",
    "title": "Information for Current Lab Members",
    "section": "Program requirements",
    "text": "Program requirements\n\nPhD (typical timeline)\n\nFall term Year 1\n\nComplete memo of expectations  in September.\nCourses\nGEOS 500 - develop literature Review and preliminary proposal\n\nSpring term Year 1\n\nCourses\nWork with supervisor to refine preliminary proposal/research gap. Identify committee members\nComplete spring review - present preliminary proposal, assess progress, discuss timeline for comprehensive exams, set tentative topics, confirm committee, confirm any further coursework\n\nSummer Year 1\n\nFieldwork/research\nFormalise comps reading lists and topics. More details on the comps policy can be found here.\n\nFall term Year 2\n\nComplete memo of expectations  in September.\nFieldwork/research\nPrepare and distribute proposal\n\nSpring term Year 2\n\nFieldwork/research\nComprehensive Exam (written exam followed by oral examination two weeks later)\n\nComplete annual review by end of spring/early summer\n\n\nSummer Year 2\n\nFieldwork/research\nFormal meeting to approve Proposal (or previous spring)\n\nFall term Year 3\n\nComplete memo of expectations  in September.\nFieldwork/research\n\nSpring term Year 3\n\nComplete memo of expectations by end of May\nComplete annual review by end of spring/early summer\nFieldwork/research\n\nSummer term Year 3\n\nFieldwork/research\n\nFall term Year 4\n\nComplete memo of expectations  in September.\nResearch\nCheck doctoral deadlines to plan out timing of defense\n\nBe mindful of the time required to complete the examination process\nThe External Examiner nomination form needs to be submitted ~5 months before the expected defense date. Nomination is the supervisor‚Äôs responsibility. Candidates should keep tabs on the process to ensure timely submission, but are not allowed to know who the examiner is.\n\n\nSpring term Year 4\n\nComplete annual review by end of spring/early summer\nResearch\nShare thesis with Supervisory Committee\n\nCommittee may take a while read/approve document\nIncorporate changes/edits\n\nNominate University Examiners - Supervisor‚Äôs responsibility, but candidate should keep tabs to ensure the\nSubmit thesis to GPS for external examination\n\nPlan for a defense 6-8 weeks after submitting\n\nShould anticipate that the examiner will take 4-6 weeks to review the thesis.\nThe earliest possible defense date is 1 week after the external report is submitted.\n\nSummer term Year 4\n\nOral exam\n\nNote: Exam must be scheduled at least 4 weeks in advance. *Make necessary revisions/edits\nDue within 1 month of defense\n\n\n\n\n\nMSc (typical timeline)\n\nFall term Year 1\n\nComplete memo of expectations  in September.\nCourses\nGEOS 500 - develop literature Review and preliminary proposal\n\nSpring term Year 1\n\nCourses\nWork with supervisor to refine preliminary proposal/research gap. Identify committee members\nComplete spring review - present preliminary proposal, assess progress\n\nSummer Year 1\n\nFieldwork/research\n\nFall term Year 2\n\nComplete memo of expectations  in September.\nFieldwork/research\n\nSpring term Year 2\n\nFieldwork/research\nComplete annual review by end of spring/early summer\nPresent thesis at Spring Symposium\n\nSummer Year 2\n\nThesis defense & submit final thesis (guidelines for the thesis can be found here). And see ‚Äòmsc-thesis-defence-revised.pdf‚Äô doc for details.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#leaving-the-lab-checklist",
    "href": "Documentation/LabInfo.html#leaving-the-lab-checklist",
    "title": "Information for Current Lab Members",
    "section": "Leaving the lab checklist",
    "text": "Leaving the lab checklist\nWe‚Äôre sad to see you go, but we‚Äôre excited for your next adventure! However, before you leave, make sure to:\n\nUpload your code to github (or share it with Sara directly). Make sure to follow the guidelines for reproducible data flow so that we can run all your code after you leave!\nMake sure to share all relevant data with Sara. Ideally this would be in the data folder in each of your project/chapter folders.\nReturn all keys, equipment, field gear that belong to be lab/university.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/DataWorkflows.html",
    "href": "Documentation/DataWorkflows.html",
    "title": "Data Pipeline",
    "section": "",
    "text": "Data flow schematic\nMicromet Data Flow (currently for BB1, BB2, DSM, and RBM)  Note: Manitoba sites have SmartFlux stream only\n\n\n\n\n\n\n\nDrive/Folder structure on vinimet\n\nP:\\Sites\\Site (data as received from the sites)\n              \\Met\n                  \\5min (for BB only)\n              \\Flux\n              \\HighFrequencyData\nP:\\Database (everything in read_bor format)\n            \\yyyy\\Sites\n                       \\Met (straight from CRBasic -&gt; read_bor)\n                           \\Clean (First stage clean)\n                       \\Flux (from EddyPro summary, site files -&gt; read_bor & then overwrite with EddyPro recalculated output) \n                           \\Clean (First stage clean)\n                       \\Clean\\SecondStage \n                             \\ThirdStage\n                             \\Ameriflux\n                             \\ThirdStage_REddyProc_RF_Fast (simple uncertainty analysis)\n                             \\ThirdStage_REddyProc_RF_Full (full uncertainty analysis)\nP:\\Micromet_web (note that this mirrors what‚Äôs on host remote.geog.ubc.ca)\n                \\www \n                    \\data (html & javascript for site level web plotting)\n                    \\webdata\\resources\\csv (csv files for web plotting)\nGoogle drive contains:\n\nPictures (to remove and host on google photos)\nDocuments\n\n\n\n\nData stream notes\nThe data processing is run from LoggerNet Task manager through batch files stored under c:\\UBC_Flux\\BiometFTPsite. The batch files are named: Site_automatic_processing.bat. The generic Matlab scripts that‚Äôs called from these batch files is: run_BB_db_update([],{‚ÄòSite‚Äô},0)\nMore Details\n\nMET Files\n\nEvery 4 hrs (e.g., BB1 4:20, BB2 4:10) Loggernet Task Master downloads MET data to P:\\Sites\\Site\\Met\\Site_MET.dat (or Site_biomet.dat)\n\nNote Site is used generically to refer to each site (e.g., BB, BB2, DSM, RBM, Hogg, Young)\nOnce per day 1:10 am\n\n‚ÄòRename files for all sites‚Äô - Called from Loggernet Task Master\n\nRenames Site_MET.dat to Site_MET.yyyyddd\n-nosplash /r ‚Äúrenam_csi_dat_files‚Äù('P:Sites\\Site\\Met');‚Ä¶for each site; exit;‚Äù\n\nSite_Process_CSI_to_database - Called from Loggernet Task Master\n\n-nosplash -minimize /r ‚Äúrun_BB_db_update([],{‚ÄòSite‚Äô});‚Äù\n  function run_BB_db_update(yearln)\n    dv = datevec(now);\n    arg_defualt('yearln', dv(1));\n    sites = {'Site'};\n    db_update_BB_site(yearln, sites);\n    exit\n\ndb_update_BB_site\n\nUpdate Site matlab database with new logger files to P:\\Database\\yyyy\\Site\\Met\nCall C:\\UBC_PC_Setup\\PC_specific\\BB_webupdate to create .csv files used by webplots -&gt; `P:_web\nCall C:\\Ubc_flux\\BiometFTPsite\\BB_Web_Update.bat to move csv files to webserver\n\n\nLocation of data\n\nP: - local drive on vinimet\nP:\\Micromet_web\\www\\webdata\\resources\\csv - location of CSV files\nP:\\Micromet_web\\www\\data - local versions of js, html files\nP:\\Sites\\Sites - raw data\n\n\n\n\nWeb-plotting pipeline\nFirst, go from the bottom of the chart in the first page upwards, find out which step stopped working then use the followiwng instructions to troubleshoot.\nEC Data\n\nSmartFlux files (*_EP-Summary.txt) arrive in the folders as below: \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB\\Flux \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB2\\Flux\n\ninsert image \n\nTroubleshooting for Step 1:\n\nIf there is NO recent data files (.zip or .txt) at all, then:\n\nCheck if you can still connect to LI-7200 to determine if it is an Internet issue or not.\nCheck if the LoggerNet task for data downloads failed. When all tasks are executed properly, the status of the tasks should be shown like this:\n\ninsert image \n\nCheck if the SmartFlux system at site skips processing or not by using WinSCP connecting to it (e.g., BB_site or BB2_site). The second figure below shows the BB2 SmartFlux have skipped processing of Feb.¬†1 (there is no summary file for that day):\n\ninsert images \n\n\n\nIf SmartFlux files arrive, the task ‚ÄúXX_automatic_processing‚Äù (as shown in the figure above) that runs Matlab program will process all data (CSI and SmartFlux) and update the web files too.\n\nTroubleshooting for Step 2:\n\nIf you suspect that this task failed, then:\n\nCheck the dates of the last updated web csv files (\\\\VINIMET.GEOG.UBC.CA\\Micromet_web\\www\\webdata\\resources\\csv).\nCheck a few flux-related csv files to see if the last datestamps are less than 6 hours old.\nIf the files are NOT up-to-date, manually rerun the Matlab program by using run_BB_db_update([]{'BB2},0)\n\nIf you confirm the task is working fine or you‚Äôve re-run the Matlab program, then:\n\nCheck if those files were uploaded to the web server by starting WinSCP to connect to ‚Äòremote.geog.ubc.ca‚Äô and see if the files are there.\n\n\n\n\nMet Data\nData from dataloggers goes to \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB\\Met\\ or \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB2\\Met\\.\n\nTroubleshooting:\n\nCheck if the files have arrived or not.\nCheck the LoggerNet task for data downloads.\nCheck connection to the logger\nCheck the data of the last updates Biomet database files (\\\\VINIMET.GEOG.UBC.CA\\Database)\nCheck a few MET-related csv files to see if the last datestamps are less than 6 hours old.\nManually re-run the task in Matlab by using run_BB_db_update([]{'BB2},0).\nCheck if those csv files were uploaded to the web server.\n\n\nContinue adding info from here\n\n\nOther Documentation & Tasks\nThe following documents in the General procedures, settings and protocols folder in Google Drive may also be of use:\n\nUpdating webplots ‚Üí Micromet web plotting.docx\nDocuments for dealing with full internal memory for SmartFlux ‚Üí Note_SmartFlux internal memory_ssh connection.docx & Note_SmartFlux internal memory_updater.docx\n7200 lab calibration ‚Üí Note_LI-7200 Lab calibration procedure.docx\nGas tank calibration ‚Üí Report_Gas tank calibration.docx\n\nNOTE: Note that the troubleshooting reports folder contains documentation on troubleshooting that is relevant to all sites.\n\n\nCreating a local copy of the database\nTo create a local copy of the Micromet Lab database or create your own database following the Micromet Lab database structure, following the instructions provided here",
    "crumbs": [
      "Documentation",
      "Data pipeline"
    ]
  },
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "Research",
    "section": "",
    "text": "Our research focuses on measuring and modelling greenhouse gas, water, and energy fluxes across a range of spatial and temporal scales. We combine field-based measurements, remote sensing, and modelling to investigate land-atmosphere interactions in our rapidly changing world, with a current emphasis on wetland ecosystems.",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Research.html#the-integrated-ghg-research-and-observations-in-wetlands-igrow-research-program",
    "href": "Research.html#the-integrated-ghg-research-and-observations-in-wetlands-igrow-research-program",
    "title": "Research",
    "section": "The integrated GHG Research and Observations in Wetlands (iGROW) Research Program",
    "text": "The integrated GHG Research and Observations in Wetlands (iGROW) Research Program\nAmong the numerous ecosystem services provided by wetlands climate regulation is identified as one of their most important benefits to society. Wetland ecosystems play an important role in the global carbon cycle; they provide the ideal environment for long-term storage of atmospheric carbon dioxide, yet they are also the largest single source of methane. Climate change has the potential to increase greenhouse gas (GHG) emissions from wetlands, however, the consequences of rising temperatures on wetland GHG exchange remains uncertain. Furthermore, preventing further wetland loss and restoring wetland ecosystems has been identified by researchers and governments as important in limiting future emissions to help meet climate goals. The integrated GHG Research and Observations in Wetlands (iGROW) research program‚Äô, takes an interdisciplinary approach to provide a better understanding of how wetland responses to climate variability and restoration can feedback to slow or accelerate future climate change. iGROW combines state-of-the-art field-based measurements, remote sensing, and modelling to provide new insights into the controls of wetland GHG fluxes across a range of spatial and temporal scales and quantify the potential climate benefits of wetland restoration and conservation. This research is key to better predicting current and future contributions of wetlands to climate change, which is highly relevant for policies aiming to limit the level of global temperature rise. Current iGROW research projects include:\n\nCarbon Fluxes in Restored Wetlands\n\nIn collaboration with several other research groups at UBC, my lab‚Äôs research is focused on measuring GHG, water, and energy fluxes over Burns Bog, a restored peatland in Metro Vancouver. This site is a raised domed peat bog that is undergoing re-wetting as a restoration management strategy following peat harvesting and associated drainage. While restoration can help recover important ecosystems services provided by wetlands, it can also affect the exchange of greenhouse gases, water and energy between the surface and the atmosphere.\n\nBy conducting year-round eddy covariance measurements, we are assessing the biogeochemical and biophysical impacts of peatland restoration, and the impacts of forest fire smoke on carbon cycling and energy fluxes in restored peatlands. This research is also being done in collaboration with researchers and staff at Metro Vancouver who are interested in quantifying the climate mitigation potential of wetland restoration and conservation. \nCollaborators:  UBC Ecohydrology Research Group  UBC Biometeorology and Soil Physics Group  Dan Moore‚Äôs Research Group, UBC Geography \nNews on Burns Bog:  April 20, 2023 - Up in smoke: Human activities are fuelling wildfires that burn essential carbon-sequestering peatlands  June 16, 2021 - David Suzuki: For climate‚Äôs sake, save the peat!  June 29, 2020 - Carbon Neutral Achievement Sets Stage for Bold Climate Action \n\n\nBlue Carbon Research\n\nBlue carbon is the carbon sequestered and stored in coastal ecosystems including tidal marshes, mangroves, and seagrasses. Coastal ecosystems are among the strongest carbon sinks in the biosphere. This coupled with their potential for low methane emissions, has generated widespread interest in these ecosystems for climate change mitigation and adaptation. However, measuring and modelling carbon exchanges in tidal wetlands presents unique challenges due to highly dynamic atmospheric and hydrological fluxes, as well as sensitivities to both terrestrial and marine influences. Through iGROW, my research group recently installed two eddy covariance flux towers (CA-DSM and CA-RBM) in tidal wetlands along the Pacific Coast of Canada to quantify and model the net carbon and GHG balance of these marshes, which represent an important data gap in ecosystem-scale measurements of GHG exchange from tidal marshes. In partnership West Coast Environmental Law, we are also working to translate this blue carbon science into policy-relevant information for municipalities and provincial governments. \n\nOur research group is also involved in the NSF funded Coastal Carbon Research Coordination Network. Specifically, we are involved in the Methane Working Group, which aims to compile all methane flux data from coastal habitats (excluding mangroves) in the CONUS to parameterize and validate a set of nested process-based methane models. \nAdditionally, we are part of a U.S. Department of Energy grant ‚ÄúHigh-frequency Data Integration for Landscape Model Calibration of Carbon Fluxes Across Diverse Tidal Marshes‚Äù focused on leveraging eddy covariance observations to improve landscape-scale models of carbon fluxes across tidal marshes. This grant brings together university and government researchers from across the U.S. \nI am also a co-investigator on a collaborative research effort to model the current and future mitigation capacity of Canada‚Äôs blue carbon ecosystems, which was recently funded by an NSERC Alliance grant. The project is led by Julia Baum (University of Victoria), and includes four co-principal investigators and seventeen collaborators from across the country which are contributing to this national natural ocean climate solutions research effort. The project brings together a consortium of four universities, three government agencies, and four eNGOs. \nPrimary Collaborators:  Oikawa Lab, California State University, East Bay  The U.S. Geological Survey  The Baum Lab, University of Victoria  O‚ÄôConnor Lab, The University of British Columbia \n\n\nCarbon Cycling in the Prairie Pothole Region of Canada\n\nWith growing interest in wetland management and restoration as a Natural Climate Solution, improved estimates of wetland carbon sequestration and GHG fluxes across Canadian wetland types are strongly needed. In partnership with Ducks Unlimited Canada, we focus on wetlands in the Prairie Pothole Region of western Canada since these ecosystems are understudied relative to other wetland types in Canada, yet they play important roles in carbon cycling and climate regulation. Specifically, this study leverages existing funding and infrastructure to investigate carbon and GHG dynamics across two wetland sites, with the aims of measuring and modeling GHG fluxes in prairie wetlands in Manitoba.\nPrimary Collaborators:  Ducks Unlimited Canada - Pascal Badiou \nPrairie Wetlands in the News:  May 19, 2022 - Project intended to demonstrate wetlands‚Äô greenhouse gas impact  April 20, 2022 - DUC analyzing wetlands on farms and ranches for carbon capture  April, 2022 - DUC Carbon Tower Project",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Research.html#past-igrow-research-projects",
    "href": "Research.html#past-igrow-research-projects",
    "title": "Research",
    "section": "Past iGROW Research Projects",
    "text": "Past iGROW Research Projects\n\nFLUXNET-CH4 ‚Äì a global database of eddy covariance CH4 flux measurements and wetland synthesis for CH4\nNatural wetlands emit approximately 30% of global CH4 emissions, as their waterlogged soils create ideal conditions for CH4 production. They are also the largest, and most uncertain, natural source of CH4 to the atmosphere. Direct observations of local CH4 emissions with high measurement frequency are important for constraining CH4 budgets, for understanding the responses of CH4 fluxes to environmental factors and climate, and for creating validation datasets for the land-surface models used to infer global CH4 budgets. However, unlike well-coordinated efforts for synthesizing CO2 flux-tower observations (e.g., FLUXNET), no such network and data synthesis effort existed previously for CH4. \n\nFLUXNET-CH4 is an initiative coordinated through the Global Carbon Project in close partnership with AmeriFlux and EuroFlux, to compile a global database of eddy covariance CH4 flux measurements to answer regional and global questions related to the global CH4 cycle. Through this activity, we coordinated the collection, aggregation, standardization, and post-processing of global CH4 data from the flux tower community. FLUXNET-CH4 Version 1.0 includes data from 81 sites, representing freshwater, coastal, upland, natural, and managed ecosystems. \nAdditionally, through a USGS Powell Center Working Group, we leveraged the FLUXNET-CH4 to provide novel insights into the controls and timing of wetland CH4 emissions for North America and globally, inform and validate biogeochemical models, and upscale wetland CH4 flux measurements globally. \nCollaborators:  Jackson Lab, Stanford University  NASA Goddard  Global Carbon Project  Stanford Machine Learning Group \n\n\nQuantifying carbon benefits of tidal wetland restoration in the Delta: Decision support using a robust, integrated and data-driven model\n\nThis research was funded through the California Department of Fish and Wildlife and focused on advancing remote sensing data in tidally flooded marshes and using the data to improve modeling of greenhouse gases in these systems. The modeling tool will be made publicly available through Google Earth Engine. This research was a collaboration between UBC, California State University, East Bay, UC Berkeley, and the U.S. Geological Survey.\nCollaborators:  Oikawa Lab, California State University, East Bay  The U.S. Geological Survey  The Landscape Research Group, UC Berkeley",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html",
    "href": "Documentation/UsingGit.html",
    "title": "Using Git",
    "section": "",
    "text": "We use git/github to manage our codebase. Version control is an essential to successfully managing of any project with multiple contributors! There can be a steep learning curve when you are first getting started with Git, but the time and hassle you will save your future self by learning will be enormous!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#what-is-a-repository",
    "href": "Documentation/UsingGit.html#what-is-a-repository",
    "title": "Using Git",
    "section": "What is a Repository?",
    "text": "What is a Repository?\nA repository (or repo) is just a collection of code, files, data, etc. that are being tracked by Git. Generally, all components of a repo should work towards a common end, whether that is an expansive goal such as managing all code used in our processing pipeline, maintaining the versions of code running on our data loggers, or the backend of our organizations website",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#what-is-a-branch",
    "href": "Documentation/UsingGit.html#what-is-a-branch",
    "title": "Using Git",
    "section": "What is a Branch?",
    "text": "What is a Branch?\n\n\nBranches can be thought of like limbs on a tree, or perhaps a more accurate analogy can be taken from hydrology, by comparing them to an anbranching river channel.\n\nBranches are separate ‚Äústreams‚Äù of code that break off of the main ‚Äúchannel‚Äù. Some branches may be dead ends that don‚Äôt end up going anywhere. While some branches may lead lead to improvements that can be incorporated back into the main branch of the codebase. Others may break off from the main project entirely.\n\n\n\n\n\nAn anabranching section of the Mackenzie River Delta.\n\n\n\n\n\nBranch Protection Rules\nWe use branch protection rules to protect the main codebase. All changes to the main branch of a protected repository, must be submitted via a ‚Äúpull request‚Äù using a separate branch. This allows for review of the changes by other users before changes are incorporated. It adds an extra level of security by protecting the main branch of code from the erroneous keystrokes of novice and expert users alike! These rules are act as a ‚Äúgate‚Äù that helps ensure any changes to operational code are intentional and agreed upon by relevant parties. If you try to push (force) a change to the main branch of a protected repository on our GitHub, you‚Äôll be given an error message and told to submit a pull request instead! More info can be found here",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#common-git-tasks",
    "href": "Documentation/UsingGit.html#common-git-tasks",
    "title": "Using Git",
    "section": "Common Git Tasks",
    "text": "Common Git Tasks\n\nCreating a Repo From Scratch\nSay you have a collection of code that you want to add to a local repository. The syntax would be as follows:\ngit init -b main\ngit add some_program.py\ngit commit -am \"First Commit: Adding my initial code to repo\"\nWhat is happening here?\ninit: Creates an empty repository with a ‚Äúbranch‚Äù (-b) named ‚Äúmain‚Äù\nadd: indicates that ‚Äúsome_program.py‚Äù will be tracked within the repo. Other files in the repo will not be tracked unless you explicitly add them. Alternatively bash git add . would all all files within the repo. There are more complex patterns that can be setup to explicitly exclude/include some files using ‚Äú.gitignore‚Äù files, but that‚Äôs a topic for another day.\ncommit: Tells git to ‚Äúsave‚Äù a permanent copy of all (-a) tracked files in the repo in there current state and leave a commit message (-m) to leave important notes about the commit. Commit messages are extremely helpful, both for reminding yourself what happened, and indicating to collaborators what‚Äôs included in a commit.\n\nYou should avoid leaving short/nondescript commit messages such as ‚Äúupdated code‚Äù, because that doesn‚Äôt help anyone\nIt is good practice to make a commit any time you make an important change to your project!\n\n\nAdding Your New Repository to GitHub\nGo to github.com and follow the steps outlined in the images bellow:\n\n\n\n\n\n1. Create a new repository\n\n\n\n\n\n\n2. Give it the same name as your local repo. Accept all defaults then click create.\n\n\n\n\n\n\n\n3. The next window will open an empty repo. Copy the url go back to your local git repo and execute the commands listed bellow.\n\n\ngit remote add origin https://github.com/June-Skeeter/SomeTest.git\ngit push --set-upstream origin main\nThese commands will tell your local git where the remote (on the cloud) version of your repository is located and then ‚Äúpush‚Äù your local copy to the cloud. If you go back to the github repo and refresh the webpage, you‚Äôll see that whatever files were in your local copy are now visible remotely!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#working-with-an-existing-repository",
    "href": "Documentation/UsingGit.html#working-with-an-existing-repository",
    "title": "Using Git",
    "section": "Working With an Existing Repository",
    "text": "Working With an Existing Repository\nMore often than not, you‚Äôll end up working with an existing repository. You may need to make some changes, which can be added back to the main branch (following the branch protection rules!) or you may just need to run some fully developed code locally on your computer. For example, maybe you need to do some processing for the HOGG site in Manitoba. You can ‚Äúdownload‚Äù a local copy of the repo and create a branch for your local changes as follows:\ngit clone https://github.com/ubc-micromet/Calculation_Procedures.git\ngit checkout -b MyHoggUpdates\nWhat is happening here?\nclone:: Tells git to download a local copy a repository from a remote location (i.e.¬†a github URL) to your computer.\ncheckout: Tells git to create an new branch (-b) with the name ‚ÄúMyHoggUpdates‚Äù. This branch will contain all of your own adjustments, but the main branch on your local machine will not be altered! If you break something (e.g., delete an important code block), its easy to go back to the main branch. Just type bash git checkout main to go back in to the main branch. Note only need to add bash -b after bash checkout when creating a new branch. Git will automatically revert all files in the repo the the main version.\n\nContributing your Changes\nProvided you have permission to write to the repository (this is granted on a per-user basis by repo ‚Äúowners‚Äù), you can contribute any changes you make by pushing a copy of your branch to github. Say you added a new file ‚Äúnew_HOGG_file.ini‚Äù, you could add it, commit it, then push to git hub as follows:\ngit add new_HOGG_file.ini\ngit commit -am \"A relevant description of your changes\"\ngit push --set-upstream origin MyHoggUpdates\nNote you only need to run the --set-upstream origin Test once per branch. The second, third, hundredth push you can simply type git push\n\n\nPulling Remote Changes\nSay there were some important updates to the main remote branch that you need to incorporate to code on your local branch. Starting from within MyHoggUpdates You could:\ngit checkout main\ngit pull\ngit checkout MyHoggUpdates\ngit merge main\npull downloads all the changes from the main branch on github to your local machine.\nmerge will automatically incorporate any changes for files that don‚Äôt conflict with changes you‚Äôve made to files locally. i.e., if you‚Äôre working on ‚Äúnew_HOGG_file.ini‚Äù, but someone else changed ‚Äúexisting_BB_file.ini‚Äù then the updates to ‚Äúexisting_BB_file.ini‚Äù will be brought in without causing any ‚Äúmerge conflicts‚Äù. If you also made an edit to ‚Äúexisting_BB_file.ini‚Äù that does not match the incoming change, it will create a merge conflict, that needs to be resolved manually.",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#other-useful-commands-and-features",
    "href": "Documentation/UsingGit.html#other-useful-commands-and-features",
    "title": "Using Git",
    "section": "Other Useful Commands and Features",
    "text": "Other Useful Commands and Features\ngit status\nstatus will list which if any files have been changed since the last commit and any files that have been added but are not tracked.\ngit restore existing_BB_file.ini\nrestore: will revert ‚Äúexisting_BB_file.ini‚Äù back to it‚Äôs state in the most recent commit. This is useful if you broke something and want to ‚Äúgo back‚Äù to the way things were. You can give a specific file name after\ngit revert HEAD~n\nrevert allows you go back any ‚Äún‚Äù number of commits, bash Head~2 would go back to the state you had two commits ago.\nIssues: Opening issues on GitHub allows us to delegate work, discuss task and keep notes on various features.\n\nThey can be labeled as bugs, upgrade requests, etc.\n\nIssues can be assigned to one or more members of the organization, and you can leave comments to discuss issues.\nAn issue remains open until they are ‚Äúclosed‚Äù.\n\nGitHub Pages: GitHub can be used to render static sites (like this one). Its a super useful skill/resource. But that‚Äôs a whole separate tutorial for another time!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#i-broke-something",
    "href": "Documentation/UsingGit.html#i-broke-something",
    "title": "Using Git",
    "section": "I broke something",
    "text": "I broke something\nSometimes you break something and need to know how to fix it. Pardon the profanity ‚Ä¶ but Oh Shit, Git is a really helpful resource.",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/index.html",
    "href": "Documentation/index.html",
    "title": "Overview",
    "section": "",
    "text": "This page is intended to serve as a catchall with general information for the lab.\n\nResources for current members of the lab.\nTips and tricks for coding and using github.\nLinks to relevant information.\nOverview of the data pipeline and database.",
    "crumbs": [
      "Documentation",
      "Overview"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/index.html",
    "href": "Documentation/Fieldwork/index.html",
    "title": "Fieldwork maintenance and procedures",
    "section": "",
    "text": "This page covers general procedures for daily monitoring and field site maintenance/calibration. If you need to sechedule a field visit - the availability of lab members by day is shown below in Table¬†1.\n\nUnder normal operating conditions, maintenance trips should occur every 4-6 weeks for each site. Further trips can be scheduled as needed. Ensure you are review the key information before commencing field work. You can checklists below to help prepare for field visits and ensure you are following standard procedures while in the field.\n\n\n\n\n\nTable¬†1: Availability of Lab Members for Fieldwork.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\n9\n\n\n\n\nJune\nNot Available\nAvailable\nNot Available\nAvailable\nIf Necessary\nAvailable\nAvailable\nNA\n\n\nTed\nBefore noon\nBefore 2pm*\nNot Available\nBefore 2pm*\nAvailable\nAvailable\nNot Available\n*after Feb 26, before that ‚ÄúNot Available‚Äù\n\n\nVanessa\nNot Available\nBefore 1pm\nNot Available\nNot Available\nAvailable\nAvailable\nAvailble\nNA\n\n\nSarah R\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nNA\n\n\nTzu-Yi\nAvailable\nAvailable\nAvailable every other week\nNot Available\nIf Necessary\nAvailable\nAvailable\nNA\n\n\nHimari\nAvailable\nNot Available\nNot Available\nNot Available\nNot Available\nAvailable\nAvailable\nNA",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html",
    "href": "Documentation/Fieldwork/Calibration.html",
    "title": "Calibration Procedures",
    "section": "",
    "text": "This page details the steps for calibrating the 7200 and 7700. Always do the 7200 first, followed by the 7700. You must clean the sensors before calibration.\nTO DO: Should we ‚Äòformally‚Äô require calibration on each site visit?",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#getting-started",
    "href": "Documentation/Fieldwork/Calibration.html#getting-started",
    "title": "Calibration Procedures",
    "section": "Getting Started",
    "text": "Getting Started\n\nMake sure the flow module, tubing, 7700 calibration shroud, and necessary wrenches are ready.\n\nNOTE: At BB1 & BB2, it is in the equipment box, for DSM & RBM you must bring to the field.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#calibrating-the-7200",
    "href": "Documentation/Fieldwork/Calibration.html#calibrating-the-7200",
    "title": "Calibration Procedures",
    "section": "Calibrating the 7200",
    "text": "Calibrating the 7200\n\nOn the laptop open the program ‚ÄòLi 7X00‚Äô and connect to instrument. Click the LI-7200 icon and choose ‚Äòcalibration‚Äô.\nSave the old calibration coefficients. Click ‚ÄúConfig Files‚Äù tab &gt;&gt; ‚ÄúSave Configuration‚Äù &gt;&gt; check all and ‚Äúcontinue‚Äù.\n\nPlace the file in this directory ‚ÄúC:‚Äù. The LI 7200 serial numbers are listed in the table below.\n\nThe naming template is:  yyyymmdd(site visit date)_Configuration_Before_calibrations.l7x\n\nScreenshots should also be taken before and after calibrating and saved in the same directory.\n\n\n\nLI 7200 Serial Numbers\n\n\n\nSite\nValue\n\n\n\n\nBB1\n0816\n\n\nBB2\n0815\n\n\nDMS\n1029\n\n\nRBM\n‚Ä¶\n\n\n\n\nCheck the head serial number information in ‚ÄúLI-7200‚Äù &gt; ‚ÄúCalibration‚Äù &gt; ‚ÄúCoefficients‚Äù. Make sure the head serial number on the sensor matches that in the software (see table above).\n\n\n\n\n\n\n\nConnect the calibration flow tube to the 7200 head. At BB2, DSM & RMB - there is a special intake novel that is always connected to the intake tube? At BB1 you need to do manually disconnect the intake tube first?\n\nTO DO: Add pics for each site?\n\n\nSet CO2 zero\n\nConnect the flow meter to the N2 gas tank and open main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow.\n\nNOTE: REVERSED directions (on is right, off is left)\n\nWatch plot on the program: CO2 should drop to 0. On the calibration screen ‚Äì green flags show that CO2 concentrations are steady. You may need to change the scale on the chart.\n\nClick ‚ÄúZero CO2‚Äù &gt; ‚ÄúOK‚Äù\nClick ‚ÄúZero H2O &gt; ‚ÄúOK‚Äù\nCheck graph values continuously throughout the process to make sure values are correct\n\nClose regulator then close the main N2 tank value and disconnect the flow meter.\n\n\n\nSet CO2 span\n\nMove flow meter to CO2 tank and open the main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank) ‚Äî To Do: Confirm value?\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow.\n\nNOTE: REVERSED directions (on is right, off is left)\n\nEnter CO2 concentration (ppm), Confirm the exact value on the tank. Then Watch plot on the program: CO2 should increase until it stabilizes. On the calibration screen ‚Äì green flags show that CO2 concentrations are steady. You may need to change the scale on the chart again.\n\nClick Span CO2‚Äù &gt; ‚ÄúOK‚Äù\n\nDO NOT Click Span H2O\n\n\nClose regulator then close the main CO2 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\n\n\n\nCheck the Coefficients\n\nYou can find the calibration results in ‚ÄúLI-7200‚Äù &gt; ‚ÄúCalibration‚Äù &gt; ‚ÄúManual‚Äù tab. Reference the table below for acceptable values. Take a screenshot (alt + print screen) of the manual tab and save it to the Micromet Google drive:\nMicromet Lab/Projects/(**Flux Site**)/Flux-tower/Calibrations/LI-7200/SNXXXX (reference serial number table above)\n\n\nOptimal Calibration Constant Values\n\n\n\n\n\n\n\n\nConstant\nValue\nNotes\n\n\n\n\nCO2 Zero\n0.85 ~ 1.1\nZero is primarily affected by temperature, and the state of the internal chemicals\n\n\nCO2 Span\n0.97 ~ 1.03\nA value outside this range indicates is a warning sign for me that something is not correct with either the instrument (wrong head, bad sensor) or with the tank (not accurately calibrated).\n\n\nH2O Zero\n0.9 ~ 1.2\nSet in lab (dobule check frequency?)\n\n\nH2O Span\n0.9 ~ 1.1\nSet in lab (dobule check frequency?)\n\n\n\n\n\n\n\n\n\nSave the after calibration coefficients. Click ‚ÄúConfig Files‚Äù tab &gt;&gt; ‚ÄúSave Configuration‚Äù &gt;&gt; check all and ‚Äúcontinue‚Äù. Use same naming/saving convention as above.\nDisconnect calibration tubing from 7200 intake. All the connections on the tubing that stays connected to the ‚ÄòT‚Äô on the 7200 head should be wrench tight when finished with calibrations.\nExit the program",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#calibrating-the-7700",
    "href": "Documentation/Fieldwork/Calibration.html#calibrating-the-7700",
    "title": "Calibration Procedures",
    "section": "Calibrating the 7700",
    "text": "Calibrating the 7700\nTO DO: Ask - should we save screenshots for 7700 calibrations?\n\nOpen the program ‚ÄòLI-7700‚Äô and go to the data page 1 &gt;&gt; 1 chart CH4 (umol/mol).\nUse calibration cylinder to cover the 7700. Make sure to remove 7700 head cap and washer tube first then orient the tube with the straps on top and slide tube over the instrument.\nAttach the small black tube into the LI-7700 cover and attach other end to the black tube with the flow meter (same one as used for 7200). Make sure to check union connections.\n\n\nSet CH4 zero\n\nConnect the flow meter to the N2 gas tank and open main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow. NOTE: REVERSED directions (on is right, off is left)\nWatch plot on the program: CH4 should drop to 0 (or ~.13). On the calibration screen ‚Äì green flags show that it‚Äôs steady Change scale if needed to check that the trace is flat/steady\n\nClick ‚ÄúZero CH4‚Äù &gt; ‚ÄúApply‚Äù\n\nClose regulator then close the main N2 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\n\n\n\nSet CH4 span\n\nMove flow meter to CH4 tank and open the main valve (on is left, off is right).\n\nNote: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\n\nTO DO: Confirm value?\n\nEnter CH4 concentration (ppm), Confirm the exact value on the tank. Then Watch plot on the program: CH4 should increase until it stabilizes. On the calibration screen ‚Äì green flags show that CH4 concentrations are steady. You may need to change the scale on the chart again.\n\nClick ‚ÄúSpan CH4‚Äù &gt; ‚ÄúApply‚Äù\n\nClose regulator then close the main CH4 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\nRemove the calibration shield, make sure to disconnect the tubing first. Then make sure to replace the 7700 head cap and washer tube first.\n\nTO DO: Check the Coefficients",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html",
    "title": "Fieldwork Protocol & Procedures",
    "section": "",
    "text": "This page covers general procedures for doing maintenance and calibration at our flux sites. Under normal operating conditions, maintenance trips should occur every 4-6 weeks for each site. Further trips can be scheduled as needed. Ensure you are review the key information before commencing field work. You can checklists below to help prepare for field visits and ensure you are following standard procedures while in the field.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#planning-a-trip",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#planning-a-trip",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Planning a Trip",
    "text": "Planning a Trip\nMake a clear list of tasks & goals. If you are going to one of the one of the flux stations you can check the field logs. See what was done on the last visit and check for notes to see if there is anything that needs to be done.\n\nBB1&2\nDSM\nRBM\nUse the checklists below to make sure you have all the supplies you need and complete all crucial tasks",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#scheduling-communication",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#scheduling-communication",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Scheduling & Communication",
    "text": "Scheduling & Communication\nLet the lab know when you will be going into the field and make sure you have at least one person joining you.\n\nYou can use the fieldwork slack channel to discuss scheduling\nBook the Micromet Truck using the google calendar\n\nContact Rick for calendar access or to request alternative vehicle options.\n\nIf you are going into Burns Bog:\n\nBoth participants must be permit holders\nYou must also call Metro Vancouver to check in and check out: 604-520-6442",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#safety",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#safety",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Safety",
    "text": "Safety\nEnsure that you are familiar with the lab‚Äôs Safety Plan\n\nIn Summer: Be cautions of fire safety:\n\nDon‚Äôt park on gras, bring water sprayer spray and fire extinguisher.\n\nIn Winter: Be mindful of the weather forecasts:\n\nDress appropriately, bring extra socks, a head lamp, gloves, etc.\nCheck the tides when going to RBM or DSM",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/ReproducibleDataFlow.html",
    "href": "Documentation/ReproducibleDataFlow.html",
    "title": "Tips for Reproducible Data Flow",
    "section": "",
    "text": "R resources\nHere is some helpful info on setting up a project in R. I‚Äôve adapted the figure from that link to better suit the needs of our research group (see below).\n\n\n\n\n\nOther helpful resources can be found in the ‚ÄòR_dynamic_document_overview_html.html‚Äô and ‚Äò0_set_up_workspace.html‚Äô doc in the ‚ÄòCoding resources/R resources‚Äô in the Google Drive. These were written by Dan Moore, but are not meant to be distributed publicly.",
    "crumbs": [
      "Documentation",
      "Tips for Reproducible Data Flow"
    ]
  },
  {
    "objectID": "acknowledgement.html",
    "href": "acknowledgement.html",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "",
    "text": "The University of British Columbia and the city of Vancouver are on the traditional, ancestral, and unceded territory of the Coast Salish Peoples. Specifically the UBC Vancouver campus is on x ∑m…ôŒ∏k ∑…ôyÃì…ôm (Musqueam) land.",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "acknowledgement.html#field-sites",
    "href": "acknowledgement.html#field-sites",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "Field Sites",
    "text": "Field Sites\nOur field sites are located in the StatlÃï…ô·∫É (Sto:lo) River Delta (aka the Fraser River Delta). This map below shows the overlapping territories and conveys the multiplicity of occupancy of First Nations wh in and around the StatlÃï…ô·∫É delta. This area encompass the traditional territories of a number of First Nations, including the:\n\n\n\nx ∑m…ôŒ∏k ∑…ôyÃì…ôm\nSt√≥:loÃÑ\nscÃì…ôwaŒ∏ena…Å…¨ t…ôm…ôx ∑ (Tsawwassen)\nkwantlen\nSemiahmoo\nStz‚Äôuminus\n√Å,LE·πàENE»ª »ΩTE (WÃ±S√ÅNEƒÜ)\nsqÃì…ôcÃìiyÃìa…Å…¨ t…ôm…ôx ∑ (Katzie)\nAnd others",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "acknowledgement.html#what-does-it-mean",
    "href": "acknowledgement.html#what-does-it-mean",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "What does it mean?",
    "text": "What does it mean?\n\n\nTraditional and Ancestral Recognize by whom the lands were traditionally used and/or occupied and the cultures have been handed down from generation to generation. The area around UBC was used by many different people, including the x ∑m…ôŒ∏k ∑…ôyÃì…ôm, …ôlÃìilw…ôta…Å…¨, and Skwxw√∫7mesh-ulh Nations. The map to the left shows the overlapping territories of these nations, along with others in the region.\nThese people are are part of a broader linguistic / cultural group of Coast Salish speaking people. The x ∑m…ôŒ∏k ∑…ôyÃì…ôm and …ôlÃìilw…ôta…Å…¨ speak dialects of Hul‚Äôq‚Äôumi‚Äônum‚Äô / Halq‚Äôem√©ylem / h…ônÃìqÃì…ôminÃì…ômÃì and the Skwxw√∫7mesh-ulh speak S·∏µwxÃ±w√∫7mesh sn√≠chim.\n\n\n\n\n\n\n\nUnceded: Refers to land that was not turned over to the government by a treaty or other agreement. Over 95% of the land in BC, and many lands elsewhere in the world were never ceded by treaty. Without treaties, these lands remain the sovereign territory of the First Nations that call them home. Yet at the same time, the lands have been claimed by Canada and these First Nations living on these lands lack a framework to express their sovereignty. This by no means absolves the Canadian government of their crimes where lands were ‚Äúceded‚Äù by treaty. Treaties were more frequently reached by coercion than negotiation. The RCMP was created specifically to force indigenous people off their lands by any means necessary.",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_3_Install_Matlab.html",
    "href": "PipelineDocumentation/2_3_Install_Matlab.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Currently, it is necessary to use Matlab to run the data cleaning scripts.\n\nFirst, check which Matlab version is currently recommended for the pipeline code, in section 8.2. This is important so that all the pipeline code runs correctly.\nVisit this website for details on how to install Matlab, or contact your systems administrator for help.\nMake sure you install Matlab with the following toolboxes only:\n\nCurve fitting\nGlobal Optimization\nOptimization\nSignal Processing\nStatistics and Machine Learning\nIf you see something else that you may want to try, go for it (e.g., Neural Networks,‚Ä¶) but having every available toolbox installed is a waste of your disk space.\n\nIf you already have the correct version of Matlab on your local computer, you can install the necessary toolboxes using the ‚ÄúAdd-Ons‚Äù button in the ‚ÄúHome‚Äù tab within Matlab.\nWhen you have Matlab along with the relevant toolboxes installed, you are ready to configure it to use with the Biomet.net library.\n\nNOTE: you can have more than one version of Matlab on your computer in case you prefer to use another version of Matlab for other projects.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.3 &nbsp; Install Software: Matlab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_3_Install_Matlab.html#install-matlab-on-your-local-computer",
    "href": "PipelineDocumentation/2_3_Install_Matlab.html#install-matlab-on-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Currently, it is necessary to use Matlab to run the data cleaning scripts.\n\nFirst, check which Matlab version is currently recommended for the pipeline code, in section 8.2. This is important so that all the pipeline code runs correctly.\nVisit this website for details on how to install Matlab, or contact your systems administrator for help.\nMake sure you install Matlab with the following toolboxes only:\n\nCurve fitting\nGlobal Optimization\nOptimization\nSignal Processing\nStatistics and Machine Learning\nIf you see something else that you may want to try, go for it (e.g., Neural Networks,‚Ä¶) but having every available toolbox installed is a waste of your disk space.\n\nIf you already have the correct version of Matlab on your local computer, you can install the necessary toolboxes using the ‚ÄúAdd-Ons‚Äù button in the ‚ÄúHome‚Äù tab within Matlab.\nWhen you have Matlab along with the relevant toolboxes installed, you are ready to configure it to use with the Biomet.net library.\n\nNOTE: you can have more than one version of Matlab on your computer in case you prefer to use another version of Matlab for other projects.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.3 &nbsp; Install Software: Matlab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/PipelineDocumentation.html",
    "href": "PipelineDocumentation/PipelineDocumentation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Compiled by: Rosie Howard, 2024\nEdits and contributions: Zoran Nesic, Sara Knox, June Skeeter, Paul Moore, and other EcoFlux Lab members and affiliates.\nRevisions: October 2024\n- These are the ‚Äúquick-start‚Äù instructions for new user of the data-cleaning pipeline. The full documentation will be made available at a later date.\n\n\n\nMotivation: The Importance of Flux Data Standardization and Reproducibility\nSoftware Installation\n‚ÄÉ2.1 ¬†Install Software: Git, and Create Github account (optional)\n‚ÄÉ2.2 ¬†Clone Biomet.net Library\n‚ÄÉ2.3 ¬†Install Software: Matlab\n‚ÄÉ2.4 ¬†Configure Matlab for Biomet.net\n‚ÄÉ2.5 ¬†Install Software: R/RStudio\n‚ÄÉ2.6 ¬†Install Software: Python (optional)\nData Cleaning Principles\n\n\n‚ÄÉ4 ¬†Quick Start: Project Directory Structure\n\n\n\n\nQuick Start: Create Database\n‚ÄÉ5.0 ¬†Quick Start: EddyPro Processing\n‚ÄÉ5.1 ¬†Quick Start: Create Database and Visualize\n\n\n\n\nQuick Start: Create INI Files for Data Cleaning\n‚ÄÉ6.1 ¬†Quick Start: First Stage INI File\n‚ÄÉ6.2 ¬†Quick Start: Second Stage INI File\n‚ÄÉ6.3 ¬†Quick Start: Third Stage and Ameriflux Output\n\nData Visualization\nTroubleshooting, FAQ, and Recommended Software Versions\n‚ÄÉ8.1 ¬†Troubleshooting and FAQ\n‚ÄÉ8.2 ¬†Software: Current Recommended Versions",
    "crumbs": [
      "Pipeline Documentation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/PipelineDocumentation.html#data-cleaning-pipeline-documentation-and-instructions",
    "href": "PipelineDocumentation/PipelineDocumentation.html#data-cleaning-pipeline-documentation-and-instructions",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Compiled by: Rosie Howard, 2024\nEdits and contributions: Zoran Nesic, Sara Knox, June Skeeter, Paul Moore, and other EcoFlux Lab members and affiliates.\nRevisions: October 2024\n- These are the ‚Äúquick-start‚Äù instructions for new user of the data-cleaning pipeline. The full documentation will be made available at a later date.\n\n\n\nMotivation: The Importance of Flux Data Standardization and Reproducibility\nSoftware Installation\n‚ÄÉ2.1 ¬†Install Software: Git, and Create Github account (optional)\n‚ÄÉ2.2 ¬†Clone Biomet.net Library\n‚ÄÉ2.3 ¬†Install Software: Matlab\n‚ÄÉ2.4 ¬†Configure Matlab for Biomet.net\n‚ÄÉ2.5 ¬†Install Software: R/RStudio\n‚ÄÉ2.6 ¬†Install Software: Python (optional)\nData Cleaning Principles\n\n\n‚ÄÉ4 ¬†Quick Start: Project Directory Structure\n\n\n\n\nQuick Start: Create Database\n‚ÄÉ5.0 ¬†Quick Start: EddyPro Processing\n‚ÄÉ5.1 ¬†Quick Start: Create Database and Visualize\n\n\n\n\nQuick Start: Create INI Files for Data Cleaning\n‚ÄÉ6.1 ¬†Quick Start: First Stage INI File\n‚ÄÉ6.2 ¬†Quick Start: Second Stage INI File\n‚ÄÉ6.3 ¬†Quick Start: Third Stage and Ameriflux Output\n\nData Visualization\nTroubleshooting, FAQ, and Recommended Software Versions\n‚ÄÉ8.1 ¬†Troubleshooting and FAQ\n‚ÄÉ8.2 ¬†Software: Current Recommended Versions",
    "crumbs": [
      "Pipeline Documentation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html",
    "href": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a generic, quick-start example that converts (1) EddyPro raw data output, and (2) Campbell Scientific TOA5 output, to a formatted database ready to be processed using the standardised libraries (i.e., Biomet.net). We also present some examples of how to quickly check the contents of your new database.\n\n\nAs previously mentioned, we will follow an example that focuses on flux-related EddyPro and met-related Campbell Scientific output data:\n\nFirst, in your newly created Sites directory, within the relevant SITEID, create a new Flux folder (figure 5.1).\n\nFigure 5.1. Directory tree showing file path to the location where raw flux data for site with SITEID1 should be stored.\nCopy your EddyPro raw output data to this Flux folder. This data will remain untouched so that you always have a local copy of the original data.\nNow, create a Met folder within the same SITEID directory (SITEID1 in this example), and copy your Campbell Scientific TOA5 data for this site to this Met folder.\nIn your Matlab folder, create one new ‚Äúmain‚Äù Matlab file that will act as a ‚Äúdo-it-all‚Äù script. The example given here (figure 5.2) ‚Äî named MyMicrometSitesCleaning_Main.m (you can make your filename less generic but we advise including ‚ÄúMain‚Äù) ‚Äî will create the database, and later we will add the code for data cleaning. This script can be copied from the code block at the bottom of this page. \n\nFigure 5.2. Matlab code to create database from raw EddyPro output. Yellow highlighted text should be edited.\nNext, run your ‚ÄúMain‚Äù Matlab program; you should see some data output in your Database directory. Data is grouped by year, then by site, then by data type, e.g., Flux or Met (figure 5.3; Met data not shown here).\n\nFigure 5.3. Directory tree showing file path to output in Database directory following database conversion.\n\nYour data is now in a format ready for cleaning using the pipeline.\n\n\n\nHere are some quick tips to inspect the Flux data from SITEID1 in your newly created database:\n\ngui_Browse_folder function:\n pth = biomet_path(yearIn, 'SITEID1', 'Flux')  % define path to folder you wish to browse\n gui_Browse_Folder(pth)\n \nThis function opens a Matlab app that looks in the Flux folder for SITEID1 for a specific year (as defined in the biomet_path function input parameters) and plots each variable in turn. You can scroll through or use the dropdown in order to check that your data looks reasonable and as expected.\nLoad one trace, e.g., co2_mixing_ratio and plot it.\n %% Load one trace and plot it\n pth = biomet_path(yearIn,'SITEID1','Flux');   \n tv = read_bor(fullfile(pth,'clean_tv'),8);      % load the time vector (Matlab's datenum format)\n tv_dt = datetime(tv,'convertfrom','datenum');   % convert to Matlab's datetime object\n x = read_bor(fullfile(pth,'co2_mixing_ratio')); % load co2_mixing_ratio from SITEID1/Flux folder\n plot(tv_dt,x)                                   % plot data\n grid on;zoom on\n \nplotApp function:\nSimply type plotApp on the command line, and it will open an app that can compare traces from the same and different cleaning stages (once you have completed those), databases, and also produce statistical plots and outputs. More details of this and other visualization tools are described at length in section 7.\n\n\n\n\nMyMicrometSitesCleaning_Main.m template script for copying (see Figure 5.2 for necessary edits):\n%% Main function for MyMicrometSites data processing\n% Created by &lt;author&gt; on &lt;date&gt;\n% \n% ============================\n% Setup the project and siteID\nprojectPath = '/Users/&lt;username&gt;/Project/My_MicrometSites/';\nstructProject=set_TAB_project(projectPath);\nsiteID = 'SITEID1';\n\n% Create database from raw data\n%% Flux data from EddyPro output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Flux','MY_EDDYPRO_OUTPUT.csv');\n\n% Read the file \noptionsFileRead.flagFileType = 'fulloutput';    % select fulloutput, biomet, or summary\n[~, ~,tv,outStruct] = fr_read_EddyPro_file(fileName,[],[],optionsFileRead);\n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Flux'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1); \n\n%% Met data from Campbell Scientific TOA5 output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Met','MY_CS_TOA5_OUTPUT.csv');\n\n% Read the file \n[~,~,~,outStruct] = fr_read_TOA5_file(fileName); \n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Met'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1);",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database",
      "5.1 &nbsp; Quick Start: Create Database and Visualize"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "href": "PipelineDocumentation/5_1_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a generic, quick-start example that converts (1) EddyPro raw data output, and (2) Campbell Scientific TOA5 output, to a formatted database ready to be processed using the standardised libraries (i.e., Biomet.net). We also present some examples of how to quickly check the contents of your new database.\n\n\nAs previously mentioned, we will follow an example that focuses on flux-related EddyPro and met-related Campbell Scientific output data:\n\nFirst, in your newly created Sites directory, within the relevant SITEID, create a new Flux folder (figure 5.1).\n\nFigure 5.1. Directory tree showing file path to the location where raw flux data for site with SITEID1 should be stored.\nCopy your EddyPro raw output data to this Flux folder. This data will remain untouched so that you always have a local copy of the original data.\nNow, create a Met folder within the same SITEID directory (SITEID1 in this example), and copy your Campbell Scientific TOA5 data for this site to this Met folder.\nIn your Matlab folder, create one new ‚Äúmain‚Äù Matlab file that will act as a ‚Äúdo-it-all‚Äù script. The example given here (figure 5.2) ‚Äî named MyMicrometSitesCleaning_Main.m (you can make your filename less generic but we advise including ‚ÄúMain‚Äù) ‚Äî will create the database, and later we will add the code for data cleaning. This script can be copied from the code block at the bottom of this page. \n\nFigure 5.2. Matlab code to create database from raw EddyPro output. Yellow highlighted text should be edited.\nNext, run your ‚ÄúMain‚Äù Matlab program; you should see some data output in your Database directory. Data is grouped by year, then by site, then by data type, e.g., Flux or Met (figure 5.3; Met data not shown here).\n\nFigure 5.3. Directory tree showing file path to output in Database directory following database conversion.\n\nYour data is now in a format ready for cleaning using the pipeline.\n\n\n\nHere are some quick tips to inspect the Flux data from SITEID1 in your newly created database:\n\ngui_Browse_folder function:\n pth = biomet_path(yearIn, 'SITEID1', 'Flux')  % define path to folder you wish to browse\n gui_Browse_Folder(pth)\n \nThis function opens a Matlab app that looks in the Flux folder for SITEID1 for a specific year (as defined in the biomet_path function input parameters) and plots each variable in turn. You can scroll through or use the dropdown in order to check that your data looks reasonable and as expected.\nLoad one trace, e.g., co2_mixing_ratio and plot it.\n %% Load one trace and plot it\n pth = biomet_path(yearIn,'SITEID1','Flux');   \n tv = read_bor(fullfile(pth,'clean_tv'),8);      % load the time vector (Matlab's datenum format)\n tv_dt = datetime(tv,'convertfrom','datenum');   % convert to Matlab's datetime object\n x = read_bor(fullfile(pth,'co2_mixing_ratio')); % load co2_mixing_ratio from SITEID1/Flux folder\n plot(tv_dt,x)                                   % plot data\n grid on;zoom on\n \nplotApp function:\nSimply type plotApp on the command line, and it will open an app that can compare traces from the same and different cleaning stages (once you have completed those), databases, and also produce statistical plots and outputs. More details of this and other visualization tools are described at length in section 7.\n\n\n\n\nMyMicrometSitesCleaning_Main.m template script for copying (see Figure 5.2 for necessary edits):\n%% Main function for MyMicrometSites data processing\n% Created by &lt;author&gt; on &lt;date&gt;\n% \n% ============================\n% Setup the project and siteID\nprojectPath = '/Users/&lt;username&gt;/Project/My_MicrometSites/';\nstructProject=set_TAB_project(projectPath);\nsiteID = 'SITEID1';\n\n% Create database from raw data\n%% Flux data from EddyPro output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Flux','MY_EDDYPRO_OUTPUT.csv');\n\n% Read the file \noptionsFileRead.flagFileType = 'fulloutput';    % select fulloutput, biomet, or summary\n[~, ~,tv,outStruct] = fr_read_EddyPro_file(fileName,[],[],optionsFileRead);\n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Flux'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1); \n\n%% Met data from Campbell Scientific TOA5 output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Met','MY_CS_TOA5_OUTPUT.csv');\n\n% Read the file \n[~,~,~,outStruct] = fr_read_TOA5_file(fileName); \n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Met'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1);",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database",
      "5.1 &nbsp; Quick Start: Create Database and Visualize"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_1_Install_Git_Optional.html",
    "href": "PipelineDocumentation/2_1_Install_Git_Optional.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. It is useful if you think you may contribute to the Biomet.net library, which is a Git repository.\n\nDownload the latest version of Git for Windows or Mac here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\nNext, if you don‚Äôt already have a personal github account, go to Github and set one up.\nOnce these tasks are complete, if you wish to contribute your own code to the Biomet.net library, start by creating a separate git branch to work on. Once you are happy with your code, create a pull request, then someone will review and approve the new files or contact you if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.1 &nbsp; Install Software: Git (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_1_Install_Git_Optional.html#install-git-and-create-personal-github-account-optional",
    "href": "PipelineDocumentation/2_1_Install_Git_Optional.html#install-git-and-create-personal-github-account-optional",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. It is useful if you think you may contribute to the Biomet.net library, which is a Git repository.\n\nDownload the latest version of Git for Windows or Mac here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\nNext, if you don‚Äôt already have a personal github account, go to Github and set one up.\nOnce these tasks are complete, if you wish to contribute your own code to the Biomet.net library, start by creating a separate git branch to work on. Once you are happy with your code, create a pull request, then someone will review and approve the new files or contact you if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.1 &nbsp; Install Software: Git (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_6_Install_Python_Optional.html",
    "href": "PipelineDocumentation/2_6_Install_Python_Optional.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. Python can be used in a number of eddy covariance data packages (e.g., some of these flux data post-processing and QA/QC resources.\nPython is also necessary for those interested in running high-frequency data reprocessing described in this EddyPro API or gap-filling fluxes using the approach described in the paper ‚ÄúGap-filling eddy covariance methane fluxes: Comparison of machine learning model predictions and uncertainties at FLUXNET-CH4 wetlands‚Äù.\nHowever, you can skip this step if you do no plan to use any of the resources above.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.6 &nbsp; Install Software: Python (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_6_Install_Python_Optional.html#python-installation-on-your-local-computer-optional",
    "href": "PipelineDocumentation/2_6_Install_Python_Optional.html#python-installation-on-your-local-computer-optional",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. Python can be used in a number of eddy covariance data packages (e.g., some of these flux data post-processing and QA/QC resources.\nPython is also necessary for those interested in running high-frequency data reprocessing described in this EddyPro API or gap-filling fluxes using the approach described in the paper ‚ÄúGap-filling eddy covariance methane fluxes: Comparison of machine learning model predictions and uncertainties at FLUXNET-CH4 wetlands‚Äù.\nHowever, you can skip this step if you do no plan to use any of the resources above.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.6 &nbsp; Install Software: Python (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html",
    "href": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Before completing this step, you must have the Biomet.net repository cloned to your local computer (section 2.2). This step is important for ensuring the pipeline scripts and tools run successfully on your local computer.\n\n\n\nOnce Biomet.net is successfully cloned, open Matlab.\nIn ‚ÄúHome‚Äù tab, click ‚ÄúSet Path‚Äù menu option. Figure 5.1 shows what your screen should be displaying when you do this, for both Windows and Mac users:\n\nFigure 2.1. View of the Set Path menu option, for (a) Windows and (b) Mac.\n\nSelect ‚ÄúAdd Folder‚Ä¶‚Äù and navigate to Biomet.net/matlab/Startup. Click ‚ÄúSelect Folder‚Äù (PC) or ‚ÄúOpen‚Äù (Mac):\n\nFigure 2.2. View of Add Folder to Path menu option, adding the Startup directory, for (a) Windows and (b) Mac.\n\nAt this point the Matlab path should look very similar to Figure 5.3: only one folder in the path does not belong to Matlab nor the /user/Documents/Matlab folder, and that is the Startup folder you just added.\nNext, click ‚ÄúSave‚Äù to save these settings for the future. We recommend that no other folders are saved in the default path to avoid unintentional consequences when running the data cleaning. If the saving fails, if possible you should restart Matlab in admin mode and repeat the process (it is often a permissions issue at this point; see Troubleshooting section 8.1 for more information).\n\nFigure 2.3. View of Matlab path after adding the Startup directory but before restarting Matlab, for (a) Windows and (b) Mac.\n\nNow, you can restart Matlab. Following the restart, check the Matlab path (click ‚ÄúSet Path‚Äù) and it should have the Biomet.net library included, as shown in Figure 5.4. In some instances you might also see ‚ÄúUBC_PC_Setup‚Äù but this may not appear in all installations - this will be automatically added only when required. Once confirmed, you can close this window, taking care not to change anything further.\n\nFigure 2.4. View of Matlab path after adding the Startup directory and after restarting Matlab, for (a) Windows and (b) Mac.\n\n\nMatlab is now ready to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.4 &nbsp; Configure Matlab for Biomet"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html#configure-matlab-for-biomet.net-library",
    "href": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html#configure-matlab-for-biomet.net-library",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Before completing this step, you must have the Biomet.net repository cloned to your local computer (section 2.2). This step is important for ensuring the pipeline scripts and tools run successfully on your local computer.\n\n\n\nOnce Biomet.net is successfully cloned, open Matlab.\nIn ‚ÄúHome‚Äù tab, click ‚ÄúSet Path‚Äù menu option. Figure 5.1 shows what your screen should be displaying when you do this, for both Windows and Mac users:\n\nFigure 2.1. View of the Set Path menu option, for (a) Windows and (b) Mac.\n\nSelect ‚ÄúAdd Folder‚Ä¶‚Äù and navigate to Biomet.net/matlab/Startup. Click ‚ÄúSelect Folder‚Äù (PC) or ‚ÄúOpen‚Äù (Mac):\n\nFigure 2.2. View of Add Folder to Path menu option, adding the Startup directory, for (a) Windows and (b) Mac.\n\nAt this point the Matlab path should look very similar to Figure 5.3: only one folder in the path does not belong to Matlab nor the /user/Documents/Matlab folder, and that is the Startup folder you just added.\nNext, click ‚ÄúSave‚Äù to save these settings for the future. We recommend that no other folders are saved in the default path to avoid unintentional consequences when running the data cleaning. If the saving fails, if possible you should restart Matlab in admin mode and repeat the process (it is often a permissions issue at this point; see Troubleshooting section 8.1 for more information).\n\nFigure 2.3. View of Matlab path after adding the Startup directory but before restarting Matlab, for (a) Windows and (b) Mac.\n\nNow, you can restart Matlab. Following the restart, check the Matlab path (click ‚ÄúSet Path‚Äù) and it should have the Biomet.net library included, as shown in Figure 5.4. In some instances you might also see ‚ÄúUBC_PC_Setup‚Äù but this may not appear in all installations - this will be automatically added only when required. Once confirmed, you can close this window, taking care not to change anything further.\n\nFigure 2.4. View of Matlab path after adding the Startup directory and after restarting Matlab, for (a) Windows and (b) Mac.\n\n\nMatlab is now ready to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.4 &nbsp; Configure Matlab for Biomet"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html",
    "href": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a quick-start summary for setting up your project directory structure and configuring Matlab to work with this structure. \nYou need the file path to your project root folder for these next steps. For example, if the name of your project is ‚ÄúMy_MicrometSites‚Äù, the project path might look like:\nprojectPath = '/Users/&lt;username&gt;/Projects/My_MicrometSites/';\n\n\n\nDefine your projectPath in Matlab. Then pick one flux site to work with; note the site ID, which must be all upper case.\nIn Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned), as follows:\n create_TAB_ProjectFolders(projectPath,'SITEID')\nNote that both input arguments are of type ‚Äústring‚Äù. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info.¬†\nAs the function name suggests, this will create some folders on your computer, and also transfer some necessary small files. In your project root directory (e.g., My_MicrometSites in figure 4.1), you should now see three new directories with the following names: (1) Database, (2) Matlab, (3) Sites.\n\nFigure 4.1. Directory tree showing contents of project folder after running the create_TAB_ProjectFolders Matlab function.\nNext, run set_TAB_project(projectPath). This process sets up the Biomet.net toolbox to work with your project.\n\n\n\n\nFor now we will describe only the new directories and files necessary for you to begin data cleaning. \n\n\n\nThe new Database directory contains a Calculation_Procedures directory, and within Calculation_Procedures, there is one called TraceAnalysis_ini.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your SITEID (SITEID1 in figure 4.2), as follows:\n\nFigure 4.2. Directory tree showing contents of TraceAnalysis_ini folder after running the create_TAB_ProjectFolders Matlab function.\nEventually (but not yet!), this Database directory will also contain the following:\n\nInitial database created using your raw data;\nYour site-specific INI files that configure how the pipeline scripts will clean the data;\nThe cleaned data once the INI files have been created and the pipeline has been run.\n\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory, under the appropriate SITEID (covered shortly in section 5.1). The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 4.3. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created ‚Äúbehind the scenes‚Äù in step 2 above, then used during step 3;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes, and may also be useful to you later when you want to visualize your data.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start: Project Directory Structure"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html#quick-start-project-directory-structure-and-matlab-configuration",
    "href": "PipelineDocumentation/4_Quick_Start_Set_Up_Structure_Config.html#quick-start-project-directory-structure-and-matlab-configuration",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a quick-start summary for setting up your project directory structure and configuring Matlab to work with this structure. \nYou need the file path to your project root folder for these next steps. For example, if the name of your project is ‚ÄúMy_MicrometSites‚Äù, the project path might look like:\nprojectPath = '/Users/&lt;username&gt;/Projects/My_MicrometSites/';\n\n\n\nDefine your projectPath in Matlab. Then pick one flux site to work with; note the site ID, which must be all upper case.\nIn Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned), as follows:\n create_TAB_ProjectFolders(projectPath,'SITEID')\nNote that both input arguments are of type ‚Äústring‚Äù. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info.¬†\nAs the function name suggests, this will create some folders on your computer, and also transfer some necessary small files. In your project root directory (e.g., My_MicrometSites in figure 4.1), you should now see three new directories with the following names: (1) Database, (2) Matlab, (3) Sites.\n\nFigure 4.1. Directory tree showing contents of project folder after running the create_TAB_ProjectFolders Matlab function.\nNext, run set_TAB_project(projectPath). This process sets up the Biomet.net toolbox to work with your project.\n\n\n\n\nFor now we will describe only the new directories and files necessary for you to begin data cleaning. \n\n\n\nThe new Database directory contains a Calculation_Procedures directory, and within Calculation_Procedures, there is one called TraceAnalysis_ini.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your SITEID (SITEID1 in figure 4.2), as follows:\n\nFigure 4.2. Directory tree showing contents of TraceAnalysis_ini folder after running the create_TAB_ProjectFolders Matlab function.\nEventually (but not yet!), this Database directory will also contain the following:\n\nInitial database created using your raw data;\nYour site-specific INI files that configure how the pipeline scripts will clean the data;\nThe cleaned data once the INI files have been created and the pipeline has been run.\n\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory, under the appropriate SITEID (covered shortly in section 5.1). The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 4.3. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created ‚Äúbehind the scenes‚Äù in step 2 above, then used during step 3;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes, and may also be useful to you later when you want to visualize your data.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start: Project Directory Structure"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_Troubleshooting_FAQ.html",
    "href": "PipelineDocumentation/8_Troubleshooting_FAQ.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "In progress‚Ä¶\n\n\n\nWhen running data cleaning (any stage), pay very close attention to the output on your screen. It tells you when everything runs smoothly, and if something goes wrong, in most cases it will be informative as to why and whereabouts things went wrong.\nOne potential recurring error comes from having extra white space in your INI file between [TRACE] and [END]. Each parameter must be defined on a new line, with no wraparound. We recommend using a text editor that has line numbers (such as VS Code) to help you avoid and/or diagnose this issue.\n\n\n\n\nThis section outlines some recurring special cases and how to deal with them:\n\nThere is a variable defined in an include INI file that you have no raw input data for.\nIn this case, you can add the following code to the global variables section of your first stage INI file .\n%--&gt;Avoiding errors due to missing input files \ndateRangeNoData = [datenum(1900,1,1) datenum(1900,12,31)]\nglobalVars.Trace.dummyVariable.inputFileName_dates = dateRangeNoData\nE.g., if you are running data cleaning for the year 2023, this code essentially tells the pipeline that for this ‚ÄúdummyVariable‚Äù, no data exists for 2023 (and only exists for the year 1900), and the program will continue smoothly with no errors.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ"
    ]
  },
  {
    "objectID": "PipelineDocumentation/8_Troubleshooting_FAQ.html#troubleshooting-special-cases-and-faq",
    "href": "PipelineDocumentation/8_Troubleshooting_FAQ.html#troubleshooting-special-cases-and-faq",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "In progress‚Ä¶\n\n\n\nWhen running data cleaning (any stage), pay very close attention to the output on your screen. It tells you when everything runs smoothly, and if something goes wrong, in most cases it will be informative as to why and whereabouts things went wrong.\nOne potential recurring error comes from having extra white space in your INI file between [TRACE] and [END]. Each parameter must be defined on a new line, with no wraparound. We recommend using a text editor that has line numbers (such as VS Code) to help you avoid and/or diagnose this issue.\n\n\n\n\nThis section outlines some recurring special cases and how to deal with them:\n\nThere is a variable defined in an include INI file that you have no raw input data for.\nIn this case, you can add the following code to the global variables section of your first stage INI file .\n%--&gt;Avoiding errors due to missing input files \ndateRangeNoData = [datenum(1900,1,1) datenum(1900,12,31)]\nglobalVars.Trace.dummyVariable.inputFileName_dates = dateRangeNoData\nE.g., if you are running data cleaning for the year 2023, this code essentially tells the pipeline that for this ‚ÄúdummyVariable‚Äù, no data exists for 2023 (and only exists for the year 1900), and the program will continue smoothly with no errors.",
    "crumbs": [
      "Pipeline Documentation",
      "8. Troubleshooting and FAQ"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_Motivation.html",
    "href": "PipelineDocumentation/1_Motivation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Collecting and handling eddy covariance data presents challenges on many front, including site and instrument selection, high frequency data processing, and data post-processing and QCQA. While there are many resources online resources that can help with all of these different challenges of eddy covariance measurements (see examples below), here we focus on the approach that the EcoFlux Lab uses for flux data post-processing and QCQA. Here we provide a detailed outline of our procedures for data post-processing QCQA, gap-filling, and CO2 flux partitioning. We also provide data visualization tools. We draw on expertise from regional networks and FLUXNET, and leverage widely used tools in the field such as REddyProc and other recently developed machine learning-based gap-filling resources . By following these procedures, you will be able to develop a robust data pipeline to ensure high quality data that you can submit to your regional flux networks. Data standardization and reproducibility helps minimize errors, enhances data reliability, and enables the integration of data sets from diverse ecosystems into global networks like FLUXNET.\nNote that in this pipeline, we are focused on data post-processing, rather than the processing of high frequency (e.g., 20Hz data). While we do provide a link to resources on high frequency flux processing [LINK TO JUNE‚ÄôS code], this is not within the main scope of this data pipeline. For more information on site set-up, instrument selection, and processing of high frequency data, please contact Xiangmin Sun (xsun130@asu.edu) from the FLUXNET CH4 and N2O processing committee\nWhile the data pipeline described here requires some coding knowledge, the whole pipeline can be run without having to modify any existing code. You only need to edit some initialization files, and then execute the code.\nAdditional resources:\n\nFlux Data Post-Processing and QA/QC\nFluxcourse Educational Materials\nVideos on the FLUXNET website",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_Motivation.html#motivation-the-importance-of-flux-data-standardization-and-reproducibility",
    "href": "PipelineDocumentation/1_Motivation.html#motivation-the-importance-of-flux-data-standardization-and-reproducibility",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Collecting and handling eddy covariance data presents challenges on many front, including site and instrument selection, high frequency data processing, and data post-processing and QCQA. While there are many resources online resources that can help with all of these different challenges of eddy covariance measurements (see examples below), here we focus on the approach that the EcoFlux Lab uses for flux data post-processing and QCQA. Here we provide a detailed outline of our procedures for data post-processing QCQA, gap-filling, and CO2 flux partitioning. We also provide data visualization tools. We draw on expertise from regional networks and FLUXNET, and leverage widely used tools in the field such as REddyProc and other recently developed machine learning-based gap-filling resources . By following these procedures, you will be able to develop a robust data pipeline to ensure high quality data that you can submit to your regional flux networks. Data standardization and reproducibility helps minimize errors, enhances data reliability, and enables the integration of data sets from diverse ecosystems into global networks like FLUXNET.\nNote that in this pipeline, we are focused on data post-processing, rather than the processing of high frequency (e.g., 20Hz data). While we do provide a link to resources on high frequency flux processing [LINK TO JUNE‚ÄôS code], this is not within the main scope of this data pipeline. For more information on site set-up, instrument selection, and processing of high frequency data, please contact Xiangmin Sun (xsun130@asu.edu) from the FLUXNET CH4 and N2O processing committee\nWhile the data pipeline described here requires some coding knowledge, the whole pipeline can be run without having to modify any existing code. You only need to edit some initialization files, and then execute the code.\nAdditional resources:\n\nFlux Data Post-Processing and QA/QC\nFluxcourse Educational Materials\nVideos on the FLUXNET website",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_Quick_Start_Create_Database.html",
    "href": "PipelineDocumentation/5_Quick_Start_Create_Database.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section contains quick-start instructions on how to create an initial database from your raw data, which can subsequently be used in the pipeline.\nFirst, you can download some sample data:\n\n\n\nComing soon‚Ä¶ \n\nDownload the sample data and config file above, and keep it handy.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "href": "PipelineDocumentation/5_Quick_Start_Create_Database.html#quick-start-create-database-from-raw-data-and-visualize-contents",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section contains quick-start instructions on how to create an initial database from your raw data, which can subsequently be used in the pipeline.\nFirst, you can download some sample data:\n\n\n\nComing soon‚Ä¶ \n\nDownload the sample data and config file above, and keep it handy.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Quick Start: Create Database"
    ]
  },
  {
    "objectID": "PipelineDocumentation/3_Data_Cleaning_Principles.html",
    "href": "PipelineDocumentation/3_Data_Cleaning_Principles.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section outlines the data cleaning principles and processes that should be followed closely and applied at each of the three stages, as well as other relevant information.\n\nFigure 3. Chart outlining the data cleaning principles followed in the pipeline.\n\n\n\n\nPrinciple: during the first stage of data cleaning we want to keep the best measured data values from each sensor, i.e., a user looking for the best measurements from a particular sensor would want to use the first stage data. Missing data points for periods of more than one half-hour will not be gap-filled in any way. \nIn summary, the first stage collects the raw data, applies min/max filtering, assigns consistent variable names in line with Ameriflux guidelines as far as possible, and moves the relevant files to a Clean folder in preparation for stage two. This folder will be created automatically if it does not already exist.\nThe data are stored as binary files in ‚Äúsingle-precision floating-point format‚Äù (aka float 32), which importantly means they are readable in most common computer languages and softwares.\n\n\n\n\n\nPrinciple: the second stage focuses on creating the best measured data values for each particular property/variable. For example, if there are two (or more) collocated temperature sensors at the same height e.g., 2 metres above ground level, the second stage is intended to create the ‚Äúbest‚Äù trace using both (all) sensors, with the highest precision/accuracy and the fewest missing points. It does this by averaging the traces, and also gap-filling either by using linear regression, or using data from a nearby climate station or reanalysis data (other gap-filling methods are currently under evaluation).\nIn practice, the second stage collects the stage one data, generates the ‚Äúbest‚Äù observation for each variable and moves the relevant files to a ‚ÄúClean/SecondStage‚Äù folder in preparation for stage three.\n\n\n\n\n\nPrinciple: USTAR filtering, gap-filling, and CO2 flux partitioning.\nThe third stage collects the stage two data and implements USTAR filtering, gap-filling, and flux partitioning procedures. For this we use the R REddyProc package which has been adapted to interface with Matlab, so that all three stages can be run together. For more machine learning approaches to gap-filling, we also implement the random forest approach described in the paper by Kim et al.¬†(2020).\n\nWe achieve these principles by setting up various configuration files used in each stage. This process is described later (section 6), but there are a few more steps to complete before that. Next, you will set up your project directory structure, then configure it to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "3. Data Cleaning Principles"
    ]
  },
  {
    "objectID": "PipelineDocumentation/3_Data_Cleaning_Principles.html#data-cleaning-principles",
    "href": "PipelineDocumentation/3_Data_Cleaning_Principles.html#data-cleaning-principles",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section outlines the data cleaning principles and processes that should be followed closely and applied at each of the three stages, as well as other relevant information.\n\nFigure 3. Chart outlining the data cleaning principles followed in the pipeline.\n\n\n\n\nPrinciple: during the first stage of data cleaning we want to keep the best measured data values from each sensor, i.e., a user looking for the best measurements from a particular sensor would want to use the first stage data. Missing data points for periods of more than one half-hour will not be gap-filled in any way. \nIn summary, the first stage collects the raw data, applies min/max filtering, assigns consistent variable names in line with Ameriflux guidelines as far as possible, and moves the relevant files to a Clean folder in preparation for stage two. This folder will be created automatically if it does not already exist.\nThe data are stored as binary files in ‚Äúsingle-precision floating-point format‚Äù (aka float 32), which importantly means they are readable in most common computer languages and softwares.\n\n\n\n\n\nPrinciple: the second stage focuses on creating the best measured data values for each particular property/variable. For example, if there are two (or more) collocated temperature sensors at the same height e.g., 2 metres above ground level, the second stage is intended to create the ‚Äúbest‚Äù trace using both (all) sensors, with the highest precision/accuracy and the fewest missing points. It does this by averaging the traces, and also gap-filling either by using linear regression, or using data from a nearby climate station or reanalysis data (other gap-filling methods are currently under evaluation).\nIn practice, the second stage collects the stage one data, generates the ‚Äúbest‚Äù observation for each variable and moves the relevant files to a ‚ÄúClean/SecondStage‚Äù folder in preparation for stage three.\n\n\n\n\n\nPrinciple: USTAR filtering, gap-filling, and CO2 flux partitioning.\nThe third stage collects the stage two data and implements USTAR filtering, gap-filling, and flux partitioning procedures. For this we use the R REddyProc package which has been adapted to interface with Matlab, so that all three stages can be run together. For more machine learning approaches to gap-filling, we also implement the random forest approach described in the paper by Kim et al.¬†(2020).\n\nWe achieve these principles by setting up various configuration files used in each stage. This process is described later (section 6), but there are a few more steps to complete before that. Next, you will set up your project directory structure, then configure it to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "3. Data Cleaning Principles"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the EcoFlux Lab!",
    "section": "",
    "text": "Contact Information Dr.¬†Sara Knox sara.knox@mcgill.ca\n\n\n\n\n\nOur research group explores the physical, biological and chemical processes that control trace gas, water and energy fluxes between the land surface and the atmosphere. We investigate how land‚Äëatmosphere exchanges of greenhouse gas fluxes respond to a changing climate and disturbances, and how we can modify land management practices for climate change adaptation and mitigation. We combine micrometeorological measurements with remote sensing and modelling to understand soil-plant-atmosphere interactions across a range of spatial and temporal scales. We collaborate with a broad group of researchers and institutions to help inform and advance climate policy.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Russell, S. J., Windham-Myers, L., Stuart-Ha√´ntjens, E. J., Bergamaschi, B. A., Anderson, F., Oikawa, P., & Knox, S. H. (2023). Increased salinity decreases annual gross primary productivity at a Northern California brackish tidal marsh. Environmental Research Letters, 18(3), 034045. https://doi.org/10.1088/1748-9326/acbbdf\n\n\nQuan, N., Lee, S.-C., Chopra, C., Nesic, Z., Porto, P., Pow, P., et al. (2023). Estimating Net Carbon and Greenhouse Gas Balances of Potato and Pea Crops on a Conventional Farm in Western Canada. Journal of Geophysical Research: Biogeosciences, 128(3), e2022JG007113. https://doi.org/10.1029/2022JG007113\n\n\nLee, S.-C., Knox, S. H., McKendry, I., & Black, T. A. (2022). Biogeochemical and biophysical responses to episodes of wildfire smoke from natural ecosystems in southwestern British Columbia, Canada. Atmospheric Chemistry and Physics, 22(4), 2333‚Äì2349. https://doi.org/10.5194/acp-22-2333-2022\n\n\nNyberg, M., Black, T. A., Ketler, R., Lee, S.-C., Johnson, M., Merkens, M., et al. (2022). Impacts of Active Versus Passive Re-Wetting on the Carbon Balance of a Previously Drained Bog. Journal of Geophysical Research: Biogeosciences, 127(9), e2022JG006881. https://doi.org/10.1029/2022JG006881\n\n\nZhang, Z., Poulter, B., Knox, S. H., Stavert, A., McNicol, G., Fluet-Chouinard, E., et al. (2021). Anthropogenic emission is the main contributor to the rise of atmospheric methane during 1993‚Äì2017. National Science Review, 9(5). https://doi.org/10.1093/nsr/nwab200\n\n\nFisher, J. B., Keenan, T. F., Buechner, C., Shirkey, G., Perez-Quezada, J. F., Knox, S. H., et al. (2021). Once Upon a Time, in AmeriFlux. Journal of Geophysical Research: Biogeosciences, 126(1), e2020JG006148. https://doi.org/10.1029/2020JG006148\n\n\nDelwiche, K. B., Knox, S. H., Malhotra, A., Fluet-Chouinard, E., McNicol, G., Feron, S., et al. (2021). FLUXNET-CH‚ÇÑ: a global, multi-ecosystem dataset and analysis of methane seasonality from freshwater wetlands. Earth System Science Data, 13(7), 3607‚Äì3689. https://doi.org/10.5194/essd-13-3607-2021\n\n\nDronova, I., Taddeo, S., Hemes, K. S., Knox, S. H., Valach, A., Oikawa, P. Y., et al. (2021). Remotely sensed phenological heterogeneity of restored wetlands: linking vegetation structure and function. Agricultural and Forest Meteorology, 296, 108215. https://doi.org/10.1016/j.agrformet.2020.108215\n\n\nKnox, S. H., Bansal, S., McNicol, G., Schafer, K., Sturtevant, C., Ueyama, M., et al. (2021). Identifying dominant environmental predictors of freshwater wetland methane fluxes across diurnal to seasonal time scales. Global Change Biology, 27(15), 3582‚Äì3604. https://doi.org/10.1111/gcb.15661\n\n\nLee, S.-C., Black, T. A., Nyberg, M., Merkens, M., Nesic, Z., Ng, D., & Knox, S. H. (2021). Biophysical Impacts of Historical Disturbances, Restoration Strategies, and Vegetation Types in a Peatland Ecosystem. Journal of Geophysical Research: Biogeosciences, 126(10), e2021JG006532. https://doi.org/10.1029/2021JG006532\n\n\nMiller, G. J., Dronova, I., Oikawa, P. Y., Knox, S. H., Windham-Myers, L., Shahan, J., & Stuart-Ha√´ntjens, E. (2021). The Potential of Satellite Remote Sensing Time Series to Uncover Wetland Phenology under Unique Challenges of Tidal Setting. Remote Sensing, 13(18), 3589. https://doi.org/10.3390/rs13183589\n\n\nBogard, M. J., Bergamaschi, B. A., Butman, D. E., Anderson, F., Knox, S. H., & Windham-Myers, L. (2020). Hydrologic Export Is a Major Component of Coastal Wetland Carbon Budgets. Global Biogeochemical Cycles, 34(8), e2019GB006430. https://doi.org/10.1029/2019GB006430\n\n\nKim, Y., Johnson, M. S., Knox, S. H., Black, T. A., Dalmagro, H. J., Kang, M., et al. (2019). Gap-filling approaches for eddy covariance methane fluxes: A comparison of three machine learning algorithms and a traditional method with principal component analysis. Global Change Biology, 26(3), 1499‚Äì1518. https://doi.org/10.1111/gcb.14845\n\n\nKnox, S. H., Jackson, R. B., Poulter, B., McNicol, G., Fluet-Chouinard, E., Zhang, Z., et al. (2019). FLUXNET-CH4 Synthesis Activity: Objectives, Observations, and Future Directions. Bulletin of the American Meteorological Society, 100(12), 2607‚Äì2632. https://doi.org/10.1175/BAMS-D-18-0268.1\n\n\nMcNicol, G., Knox, S. H., Guilderson, T. P., Baldocchi, D. D., & Silver, W. L. (2019). Where old meets new: An ecosystem study of methanogenesis in a reflooded agricultural peatland. Global Change Biology, 26(2), 772‚Äì785. https://doi.org/10.1111/gcb.14916",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#peer-reviewed-journal-articles",
    "href": "Publications.html#peer-reviewed-journal-articles",
    "title": "Publications",
    "section": "",
    "text": "Russell, S. J., Windham-Myers, L., Stuart-Ha√´ntjens, E. J., Bergamaschi, B. A., Anderson, F., Oikawa, P., & Knox, S. H. (2023). Increased salinity decreases annual gross primary productivity at a Northern California brackish tidal marsh. Environmental Research Letters, 18(3), 034045. https://doi.org/10.1088/1748-9326/acbbdf\n\n\nQuan, N., Lee, S.-C., Chopra, C., Nesic, Z., Porto, P., Pow, P., et al. (2023). Estimating Net Carbon and Greenhouse Gas Balances of Potato and Pea Crops on a Conventional Farm in Western Canada. Journal of Geophysical Research: Biogeosciences, 128(3), e2022JG007113. https://doi.org/10.1029/2022JG007113\n\n\nLee, S.-C., Knox, S. H., McKendry, I., & Black, T. A. (2022). Biogeochemical and biophysical responses to episodes of wildfire smoke from natural ecosystems in southwestern British Columbia, Canada. Atmospheric Chemistry and Physics, 22(4), 2333‚Äì2349. https://doi.org/10.5194/acp-22-2333-2022\n\n\nNyberg, M., Black, T. A., Ketler, R., Lee, S.-C., Johnson, M., Merkens, M., et al. (2022). Impacts of Active Versus Passive Re-Wetting on the Carbon Balance of a Previously Drained Bog. Journal of Geophysical Research: Biogeosciences, 127(9), e2022JG006881. https://doi.org/10.1029/2022JG006881\n\n\nZhang, Z., Poulter, B., Knox, S. H., Stavert, A., McNicol, G., Fluet-Chouinard, E., et al. (2021). Anthropogenic emission is the main contributor to the rise of atmospheric methane during 1993‚Äì2017. National Science Review, 9(5). https://doi.org/10.1093/nsr/nwab200\n\n\nFisher, J. B., Keenan, T. F., Buechner, C., Shirkey, G., Perez-Quezada, J. F., Knox, S. H., et al. (2021). Once Upon a Time, in AmeriFlux. Journal of Geophysical Research: Biogeosciences, 126(1), e2020JG006148. https://doi.org/10.1029/2020JG006148\n\n\nDelwiche, K. B., Knox, S. H., Malhotra, A., Fluet-Chouinard, E., McNicol, G., Feron, S., et al. (2021). FLUXNET-CH‚ÇÑ: a global, multi-ecosystem dataset and analysis of methane seasonality from freshwater wetlands. Earth System Science Data, 13(7), 3607‚Äì3689. https://doi.org/10.5194/essd-13-3607-2021\n\n\nDronova, I., Taddeo, S., Hemes, K. S., Knox, S. H., Valach, A., Oikawa, P. Y., et al. (2021). Remotely sensed phenological heterogeneity of restored wetlands: linking vegetation structure and function. Agricultural and Forest Meteorology, 296, 108215. https://doi.org/10.1016/j.agrformet.2020.108215\n\n\nKnox, S. H., Bansal, S., McNicol, G., Schafer, K., Sturtevant, C., Ueyama, M., et al. (2021). Identifying dominant environmental predictors of freshwater wetland methane fluxes across diurnal to seasonal time scales. Global Change Biology, 27(15), 3582‚Äì3604. https://doi.org/10.1111/gcb.15661\n\n\nLee, S.-C., Black, T. A., Nyberg, M., Merkens, M., Nesic, Z., Ng, D., & Knox, S. H. (2021). Biophysical Impacts of Historical Disturbances, Restoration Strategies, and Vegetation Types in a Peatland Ecosystem. Journal of Geophysical Research: Biogeosciences, 126(10), e2021JG006532. https://doi.org/10.1029/2021JG006532\n\n\nMiller, G. J., Dronova, I., Oikawa, P. Y., Knox, S. H., Windham-Myers, L., Shahan, J., & Stuart-Ha√´ntjens, E. (2021). The Potential of Satellite Remote Sensing Time Series to Uncover Wetland Phenology under Unique Challenges of Tidal Setting. Remote Sensing, 13(18), 3589. https://doi.org/10.3390/rs13183589\n\n\nBogard, M. J., Bergamaschi, B. A., Butman, D. E., Anderson, F., Knox, S. H., & Windham-Myers, L. (2020). Hydrologic Export Is a Major Component of Coastal Wetland Carbon Budgets. Global Biogeochemical Cycles, 34(8), e2019GB006430. https://doi.org/10.1029/2019GB006430\n\n\nKim, Y., Johnson, M. S., Knox, S. H., Black, T. A., Dalmagro, H. J., Kang, M., et al. (2019). Gap-filling approaches for eddy covariance methane fluxes: A comparison of three machine learning algorithms and a traditional method with principal component analysis. Global Change Biology, 26(3), 1499‚Äì1518. https://doi.org/10.1111/gcb.14845\n\n\nKnox, S. H., Jackson, R. B., Poulter, B., McNicol, G., Fluet-Chouinard, E., Zhang, Z., et al. (2019). FLUXNET-CH4 Synthesis Activity: Objectives, Observations, and Future Directions. Bulletin of the American Meteorological Society, 100(12), 2607‚Äì2632. https://doi.org/10.1175/BAMS-D-18-0268.1\n\n\nMcNicol, G., Knox, S. H., Guilderson, T. P., Baldocchi, D. D., & Silver, W. L. (2019). Where old meets new: An ecosystem study of methanogenesis in a reflooded agricultural peatland. Global Change Biology, 26(2), 772‚Äì785. https://doi.org/10.1111/gcb.14916",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#theses",
    "href": "Publications.html#theses",
    "title": "Publications",
    "section": "Theses",
    "text": "Theses\n\n\nSatriawan, T. (2022). Interannual variability of carbon dioxide (CO‚ÇÇ) and methane (CH‚ÇÑ) fluxes in a temperate bog over a 5-year period (Master‚Äôs Thesis). University of British Columbia. https://doi.org/10.14288/1.0416306\n\n\nNyberg, M. (2021). Impacts of restoration and climate variability on peatland GHG fluxes (Master‚Äôs Thesis). University of British Columbia. https://doi.org/10.14288/1.0401463\n\n\nRussell, S. J. (2021). Increased salinity decreases annual gross primary productivity of a Northern California brackish wetland (Master‚Äôs Thesis). University of British Columbia. https://doi.org/10.14288/1.0406272",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#research-talks-poster-presentations",
    "href": "Publications.html#research-talks-poster-presentations",
    "title": "Publications",
    "section": "Research Talks & Poster Presentations",
    "text": "Research Talks & Poster Presentations\n\n\n\n\n\n\n\nSkeeter, J., & Knox, S. H. (2023, April). Ongoing and Proposed Research in the Burns Bog Ecological Conservancy Area.\n\n\nKnox, S. H., & Skeeter, J. (2023, March). UBC Micrometeorology Lab Studies Review.\n\n\nLu, T.-Y., Russell, S. J., Skeeter, J., Lee, S., Oikawa, P., & Knox, S. H. (2022, December). Investigating environmental controls on carbon exchange and predicting gaseous carbon fluxes at a salt marsh in British Columbia.\n\n\nNg, D., & Knox, S. H. (2022, December). Characterizing within-footprint spatial heterogeneity of CH4 emissions in freshwater wetlands through remote sensing and Footprint-weighted Flux Maps.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2022, September). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2022, June). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2021, December). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nNyberg, M., Knox, S. H., Black, T., Johnson, M., Ketler, R., & Nesic, Z. (2020, December). Impacts of restoration and climate variability on peatland greenhouse gas fluxes.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2020, December). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "OutreachNews.html",
    "href": "OutreachNews.html",
    "title": "Outreach and News",
    "section": "",
    "text": "A new short-term flux tower was installed at the Burns Bog Seedling site to monitor CO2 fluxes from a 7-year old lodge-pole pine stand that sprouted following a fire in Burns Bog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJune and I presented at the CGU Annual meeting which was held in Banff this year."
  },
  {
    "objectID": "OutreachNews.html#section",
    "href": "OutreachNews.html#section",
    "title": "Outreach and News",
    "section": "",
    "text": "A new short-term flux tower was installed at the Burns Bog Seedling site to monitor CO2 fluxes from a 7-year old lodge-pole pine stand that sprouted following a fire in Burns Bog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJune and I presented at the CGU Annual meeting which was held in Banff this year."
  },
  {
    "objectID": "OutreachNews.html#section-1",
    "href": "OutreachNews.html#section-1",
    "title": "Outreach and News",
    "section": "2022",
    "text": "2022\n\nDecember\nA recent talk I gave during the UBC IRES Seminar Series on ‚ÄòWetlands in a changing world: processes, feedbacks, and the climate benefits of wetlands‚Äô\n\n\nNovember\nLearn more about our recently funded work looking into the role of wetlands as nature-based climate solutions. Excited to work with this great team of researchers! \nHonoured and thrilled to be named CRC in Eco-Meteorology! This is really a group accomplishment that reflects my amazing team (past and present) and all the wonderful and talented colleagues and mentors I‚Äôve had the opportunity to worth with.\n\n\n\nSeptember\nThanks @Let‚Äôs talk science for featuring our work in your career profiles! See the accompanying interview here.\n\nMarion‚Äôs paper was featured in EOS:"
  },
  {
    "objectID": "OutreachNews.html#section-2",
    "href": "OutreachNews.html#section-2",
    "title": "Outreach and News",
    "section": "2021",
    "text": "2021\n\nJune\nCoverage of our 2021 Global Change Biology Paper:"
  },
  {
    "objectID": "OutreachNews.html#section-3",
    "href": "OutreachNews.html#section-3",
    "title": "Outreach and News",
    "section": "2019",
    "text": "2019\n\nAugust\nThe UBC Geography department highlighting our 2019 BAMS paper:\n\nAn AmeriFlux blog post highlighting our FLUXNET-CH4 work:\n\n\n\nJune\nA blog post I wrote describing life as a new faculty member:"
  },
  {
    "objectID": "OutreachNews.html#and-earlier",
    "href": "OutreachNews.html#and-earlier",
    "title": "Outreach and News",
    "section": "2018 and Earlier",
    "text": "2018 and Earlier\n\nJune (2016)\nCoverage of our work on wetland restoration in the Sacramento-San Joaquin Delta:\n\n\n\nOctober (2016)\nAn interview I did as part of STEM week at Los Altos High School in Los Altos, California:"
  }
]