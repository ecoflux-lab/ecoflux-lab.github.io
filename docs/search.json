[
  {
    "objectID": "Data.html#field-sites",
    "href": "Data.html#field-sites",
    "title": "Field Sites & Data",
    "section": "Field Sites",
    "text": "Field Sites\nThis web-app shows the sites where operate Eddy Covariance towers and Flux Chamber sites in wetlands across the Metro Vancouver.\n\nClick the points to sites to see more information.\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n&lt;p&gt;\n\nView map in separate window\n\nBurns Bog 1 (BB)\n\n\n\nData Preview Graph (CA-DBB) (2014 - *, Delta, BC, Canada)\nDownload Data for CA-DBB from Ameriflux.\nData from the web plots are available here\n\nprefix BB. note that this is raw data – no filtering or QCQA. Database codes are available here.\n\nBB1 Site Photos (also see Dr. Andreas Christen’s previous photos from when he held his position at UBC)\n\n\n\n\n\n\n\n\n\n\n\nBurns Bog 2 (BB2)\n\n\n\nData Preview Graph (CA-DB2) (2019 - *, Delta, BC, Canada)\nDownload Data for CA-DB2 will soon be available from Ameriflux.\nData from the web plots are available here\n\nprefix BB2. note that this is raw data – no filtering or QCQA. Database codes are available here.\n\nBB2 Site Photos\n\n\n\n\n\n\n\n\n\n\n\nBurns Bog Seedling (BBS)\n\n\n\nFlux chambers were installed in spring 2023\nA temporary flux tower was installed in summer 2023\n\nmore info to come\n\n\n\n\n\n\n\n\n\n\n\n\nDelta Salt Marsh (DSM)\n\n\n\nData Preview Graph (CA-DSM) (2021 - *, Delta, BC, Canada)\nData from CA-DSM will soon be available on AmeriFlux.\nData from the web plots are available here\n\nprefix DSM. note that this is raw data – no filtering or QCQA.\n\nDSM Site Photos\n\n\n\n\n\n\n\n\n\n\n\nRichmond Brackish Marsh (RBM)\n\n\n\nData Preview Graph (CA-RBM) (2021 - *, Richmond, BC, Canada)\nData from CA-RBM will soon be available on AmeriFlux.\nData from the web plots are available here\n\nprefix RBM. note that this is raw data – no filtering or QCQA.\n\nRBM Site Photos\n\n\n\n\n\n\n\n\n\n\n\nUBC Climate Station on Totem Field\n\nAccess to custom data download of standard measured meteorological observations (air temperature, humidity, precipitation, soil temperatures, solar irradiance, wind) measured since 1958. Data can be downloaded in .csv format."
  },
  {
    "objectID": "Data.html#code-and-documentation",
    "href": "Data.html#code-and-documentation",
    "title": "Field Sites & Data",
    "section": "Code and Documentation",
    "text": "Code and Documentation\nOur lab github hosts code for flux processing, README docs for our lab, and various other bits of information.\nMore documentation related to maintenance procedures for our sites can be found here."
  },
  {
    "objectID": "People.html",
    "href": "People.html",
    "title": "People",
    "section": "",
    "text": "Name\nDr. Sara Helen Knox\n\n\nPronouns\nShe/Her/Hers\n\n\nEmail\nsara.knox@mcgill.ca\n\n\nGitHub\nhttps://github.com/sknox01\n\n\n\n\n\nI am broadly interested in the impacts of climate variability and land-use change on land-atmosphere exchanges of water, energy, and trace gases. I also seek to understand how ecosystem responses to global change can feedback to slow or accelerate future climate change. My research and training is in micrometeorology, hydrology, and ecosystem ecology. I focus on biosphere-atmosphere interactions in a variety of climates and ecosystems.\n\nEducation\n\nUniversity of California, Berkeley, PhD\n\n\nCarleton University, MSc\n\n\nMcGill University, BSc\n\n\n\n\n\n\n\n\n\n\nName\nRick Ketler - Project Manager (Geography)\n\n\nEmail\nrick.ketler@ubc.ca\n\n\nOffice\nPCMH B1100\n\n\n\n\n\nCurrently Research Projects Manager for UBC Geography Physical Labs. I have designed and installed research sites in terrain ranging from temperate bogs to arctic tundra. I primarily support professors, PDF’s and grad students with physical scientific measurements in the field and in the lab.\n\nEducation\n\nB.Sc. Physics Co-op University of Victoria, B.C.\n\n\n\n\n\n\n\n\n\n\nName\nZoran Nesic - Senior Research Engineer\n\n\nEmail\nzoran.nesic@ubc.ca\n\n\nOffice\nMCML 137\n\n\n\n\n\nI am responsible for management of numerous research and equipment design projects for various faculties and departments at UBC. I design eddy covariance and soil respiration systems that are used at various North American universities as well as by other research institutions. My main research interests are (1) the design of automated measurement systems for long-term environmental measurements and (2) the standardization of measurement and data processing procedures to ensure high quality and reproducibility of research results.\n\nEducation\n\nB.Sc. in Electrical Engineering, University of Belgrade\n\n\nM.A.Sc. in Electrical Engineering, University of British Columbia\n\n\n\n\n\n\n\n\n\n\nName\nDr. Rosemary Howard\n\n\nPosition\nResearch Associate\n\n\nEmail\n\n\n\n\n\n\nPrevious experience\nEducation\n\n\n\n\n\n\n\n\nName\nDr. Paul Moore\n\n\nPosition\nData Scientist\n\n\nEmail\n\n\n\n\n\n\nPrevious experience\nEducation\n\n\n\n\n\n\n\n\nName\nDr. Joyson Ahongshangbam\n\n\nPosition\nPostdoctoral Scholar\n\n\nEmail\njoyson.ahongshangbam@mcgill.ca\n\n\nOrcid\nhttps://orcid.org/0000-0002-2678-6879\n\n\n\n\n\nI am interested in studying the carbon and water cycle in different ecosystems (forest, urban vegetation and wetlands) using observations such as eddy covariance, sap flow and remote sensing techniques. My research includes understanding the responses of carbon and water dynamics with the land cover change, management activities and climate change (drought and heatwave).\n\nPrevious experience\n\nUniversity of Helsinki, Finland, Postdoctoral researcher\n\nEducation\n\nUniversity of Göttingen, Germany, PhD\n\n\nIndian Institute of Remote sensing, India, Masters\n\n\n\n\n\n\n\n\n\n\nName\nDr. Xuicheng Yang\n\n\nPosition\nPostdoctoral Scholar\n\n\nEmail\nxiuchengyang@uvic.ca\n\n\n\n\n\nI am conducting remote sensing analyses to characterize Canada’s tidal marsh ecosystems. I am also synthesizing spatial datasets on the distributions and threats to Canada’s coastal ecosystems, including land use changes and sea level rise, to develop, analyze, and visualize future ecosystem scenarios, with a particular focus on carbon storage.\n\nPrevious experience\n\nUniversity of Connecticut, USA, Postdoctoral Fellow\n\nEducation\n\nUniversity of Strasbourg, France, PhD\n\n\nPeking University, China, Masters\n\n\n\n\n\n\n\n\n\n\nName\nTzu-Yi Lu\n\n\nLevel of Study\nPh.D. Candidate\n\n\n\n\n\nI received my MS in Geography from National Taiwan University in 2017. I am interested in understanding the response of the wetland ecosystem to climate change, especially in quantifying the net exchange of carbon. My previous research investigated the relationship between environmental controls and CO2 flux in low-latitude wetland ecosystems, applying an Artificial Neural Network technique to simulate the variance of carbon exchange by meteorological variables.\n\n\n\n\n\n\n\n\nName\nKatrina Poppe\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nPh.D. Candidate\n\n\nEmail\npoppek@student.ubc.ca\n\n\n\n\n\nI earned an MS in Environmental Science from Western Washington University in 2016 and continued at WWU as a Research Associate for several years. My previous research has focused primarily on blue carbon, studying soil carbon sequestration rates in Pacific Northwest estuaries and in United Arab Emirates mangroves, in addition to monitoring and modeling vegetation and sediment dynamics in relation to estuary restoration and sea level rise. I am currently interested in studying greenhouse gas fluxes in Pacific Northwest tidal wetlands – particularly how they respond to ecosystem restoration and climate change – to ultimately better understand the value of tidal wetland management actions as natural climate solutions.\n\n\n\n\n\n\n\n\nName\nSarah Russell\n\n\nLevel of Study\nPh.D. Student\n\n\n\n\n\nI received a BS in Biological Sciences from Wellesley College in 2017, then worked as an ecosystem ecology field technician and research assistant before moving to Vancouver. I am interested in land-atmosphere carbon dynamics and am particularly interested in quantifying the terrestrial carbon sink. My research at UBC involves modeling greenhouse gas fluxes from restored tidal wetlands in the Sacramento-San Joaquin River Delta.\n\n\n\n\n\n\n\n\nName\nJinshuai Li\n\n\nPosition\nVisiting PhD Candidate\n\n\nEmail\njinshuai.li@mail.mcgill.ca\n\n\n\n\n\nI am a PhD candidate at the Institute of Geographic Sciences and Natural Resources, Chinese Academy of Sciences. Currently, I am doing research at McGill University as a visiting student. I am interested in the basic characteristics of the wetland carbon cycle and the main environmental response features. At the present stage, my research focuses on quantifying the environmental response characteristics of methane emissions from wetlands, and based on this, I am trying to construct models to quantify methane emissions and further reduce the simulation errors.\n\n\n\n\n\n\n\n\nName\nVanessa Valenti\n\n\nLevel of Study\nM.Sc. Student\n\n\n\n\n\nI earned a Bachelor’s degree in Geography/Environmental Studies from the University of California, Los Angeles in 2019. Before coming to UBC, I worked as a scientific programmer at the NASA Goddard Space Flight Center, providing visualization and computation support to earth system and atmosphere climate models. I am interested in modelling land-atmosphere exchanges and projecting responses of wetland and forested ecosystems to climate change.\n\n\n\n\n\n\n\n\nName\nDylan Gwilliam\n\n\nLevel of Study\nM.Sc. Student\n\n\n\n\n\nI received my B.Sc. in Environment from McGill in 2024, specialising in Land Surface Processes and Environmental Change. Before coming back to McGill for my Master’s, I worked 9 months in Science and Conservation Planning at the Nature Conservancy of Canada. I am interested in quantifying land-atmosphere greenhouse gas fluxes from wetlands and how they are altered after degradation and restoration. I believe understanding this missing piece of the puzzle can help to better value and protect wetland ecosystems and their role as nature-based climate solutions.",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#faculty-and-staff",
    "href": "People.html#faculty-and-staff",
    "title": "People",
    "section": "",
    "text": "Name\nDr. Sara Helen Knox\n\n\nPronouns\nShe/Her/Hers\n\n\nEmail\nsara.knox@mcgill.ca\n\n\nGitHub\nhttps://github.com/sknox01\n\n\n\n\n\nI am broadly interested in the impacts of climate variability and land-use change on land-atmosphere exchanges of water, energy, and trace gases. I also seek to understand how ecosystem responses to global change can feedback to slow or accelerate future climate change. My research and training is in micrometeorology, hydrology, and ecosystem ecology. I focus on biosphere-atmosphere interactions in a variety of climates and ecosystems.\n\nEducation\n\nUniversity of California, Berkeley, PhD\n\n\nCarleton University, MSc\n\n\nMcGill University, BSc\n\n\n\n\n\n\n\n\n\n\nName\nRick Ketler - Project Manager (Geography)\n\n\nEmail\nrick.ketler@ubc.ca\n\n\nOffice\nPCMH B1100\n\n\n\n\n\nCurrently Research Projects Manager for UBC Geography Physical Labs. I have designed and installed research sites in terrain ranging from temperate bogs to arctic tundra. I primarily support professors, PDF’s and grad students with physical scientific measurements in the field and in the lab.\n\nEducation\n\nB.Sc. Physics Co-op University of Victoria, B.C.\n\n\n\n\n\n\n\n\n\n\nName\nZoran Nesic - Senior Research Engineer\n\n\nEmail\nzoran.nesic@ubc.ca\n\n\nOffice\nMCML 137\n\n\n\n\n\nI am responsible for management of numerous research and equipment design projects for various faculties and departments at UBC. I design eddy covariance and soil respiration systems that are used at various North American universities as well as by other research institutions. My main research interests are (1) the design of automated measurement systems for long-term environmental measurements and (2) the standardization of measurement and data processing procedures to ensure high quality and reproducibility of research results.\n\nEducation\n\nB.Sc. in Electrical Engineering, University of Belgrade\n\n\nM.A.Sc. in Electrical Engineering, University of British Columbia\n\n\n\n\n\n\n\n\n\n\nName\nDr. Rosemary Howard\n\n\nPosition\nResearch Associate\n\n\nEmail\n\n\n\n\n\n\nPrevious experience\nEducation\n\n\n\n\n\n\n\n\nName\nDr. Paul Moore\n\n\nPosition\nData Scientist\n\n\nEmail\n\n\n\n\n\n\nPrevious experience\nEducation\n\n\n\n\n\n\n\n\nName\nDr. Joyson Ahongshangbam\n\n\nPosition\nPostdoctoral Scholar\n\n\nEmail\njoyson.ahongshangbam@mcgill.ca\n\n\nOrcid\nhttps://orcid.org/0000-0002-2678-6879\n\n\n\n\n\nI am interested in studying the carbon and water cycle in different ecosystems (forest, urban vegetation and wetlands) using observations such as eddy covariance, sap flow and remote sensing techniques. My research includes understanding the responses of carbon and water dynamics with the land cover change, management activities and climate change (drought and heatwave).\n\nPrevious experience\n\nUniversity of Helsinki, Finland, Postdoctoral researcher\n\nEducation\n\nUniversity of Göttingen, Germany, PhD\n\n\nIndian Institute of Remote sensing, India, Masters\n\n\n\n\n\n\n\n\n\n\nName\nDr. Xuicheng Yang\n\n\nPosition\nPostdoctoral Scholar\n\n\nEmail\nxiuchengyang@uvic.ca\n\n\n\n\n\nI am conducting remote sensing analyses to characterize Canada’s tidal marsh ecosystems. I am also synthesizing spatial datasets on the distributions and threats to Canada’s coastal ecosystems, including land use changes and sea level rise, to develop, analyze, and visualize future ecosystem scenarios, with a particular focus on carbon storage.\n\nPrevious experience\n\nUniversity of Connecticut, USA, Postdoctoral Fellow\n\nEducation\n\nUniversity of Strasbourg, France, PhD\n\n\nPeking University, China, Masters\n\n\n\n\n\n\n\n\n\n\nName\nTzu-Yi Lu\n\n\nLevel of Study\nPh.D. Candidate\n\n\n\n\n\nI received my MS in Geography from National Taiwan University in 2017. I am interested in understanding the response of the wetland ecosystem to climate change, especially in quantifying the net exchange of carbon. My previous research investigated the relationship between environmental controls and CO2 flux in low-latitude wetland ecosystems, applying an Artificial Neural Network technique to simulate the variance of carbon exchange by meteorological variables.\n\n\n\n\n\n\n\n\nName\nKatrina Poppe\n\n\nPronouns\nShe/Her/Hers\n\n\nLevel of Study\nPh.D. Candidate\n\n\nEmail\npoppek@student.ubc.ca\n\n\n\n\n\nI earned an MS in Environmental Science from Western Washington University in 2016 and continued at WWU as a Research Associate for several years. My previous research has focused primarily on blue carbon, studying soil carbon sequestration rates in Pacific Northwest estuaries and in United Arab Emirates mangroves, in addition to monitoring and modeling vegetation and sediment dynamics in relation to estuary restoration and sea level rise. I am currently interested in studying greenhouse gas fluxes in Pacific Northwest tidal wetlands – particularly how they respond to ecosystem restoration and climate change – to ultimately better understand the value of tidal wetland management actions as natural climate solutions.\n\n\n\n\n\n\n\n\nName\nSarah Russell\n\n\nLevel of Study\nPh.D. Student\n\n\n\n\n\nI received a BS in Biological Sciences from Wellesley College in 2017, then worked as an ecosystem ecology field technician and research assistant before moving to Vancouver. I am interested in land-atmosphere carbon dynamics and am particularly interested in quantifying the terrestrial carbon sink. My research at UBC involves modeling greenhouse gas fluxes from restored tidal wetlands in the Sacramento-San Joaquin River Delta.\n\n\n\n\n\n\n\n\nName\nJinshuai Li\n\n\nPosition\nVisiting PhD Candidate\n\n\nEmail\njinshuai.li@mail.mcgill.ca\n\n\n\n\n\nI am a PhD candidate at the Institute of Geographic Sciences and Natural Resources, Chinese Academy of Sciences. Currently, I am doing research at McGill University as a visiting student. I am interested in the basic characteristics of the wetland carbon cycle and the main environmental response features. At the present stage, my research focuses on quantifying the environmental response characteristics of methane emissions from wetlands, and based on this, I am trying to construct models to quantify methane emissions and further reduce the simulation errors.\n\n\n\n\n\n\n\n\nName\nVanessa Valenti\n\n\nLevel of Study\nM.Sc. Student\n\n\n\n\n\nI earned a Bachelor’s degree in Geography/Environmental Studies from the University of California, Los Angeles in 2019. Before coming to UBC, I worked as a scientific programmer at the NASA Goddard Space Flight Center, providing visualization and computation support to earth system and atmosphere climate models. I am interested in modelling land-atmosphere exchanges and projecting responses of wetland and forested ecosystems to climate change.\n\n\n\n\n\n\n\n\nName\nDylan Gwilliam\n\n\nLevel of Study\nM.Sc. Student\n\n\n\n\n\nI received my B.Sc. in Environment from McGill in 2024, specialising in Land Surface Processes and Environmental Change. Before coming back to McGill for my Master’s, I worked 9 months in Science and Conservation Planning at the Nature Conservancy of Canada. I am interested in quantifying land-atmosphere greenhouse gas fluxes from wetlands and how they are altered after degradation and restoration. I believe understanding this missing piece of the puzzle can help to better value and protect wetland ecosystems and their role as nature-based climate solutions.",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "People.html#lab-alumni",
    "href": "People.html#lab-alumni",
    "title": "People",
    "section": "Lab Alumni",
    "text": "Lab Alumni\n\n\nJune Skeeter\n(Postdoctoral Scholar)\n\n\nDr. Sung-Ching (Nick) Lee\n(Postdoctoral Scholar)\n\n\n\n\n\nHehan (Zoe) Zhang\n(M.Sc. Student)\n\n\nDarian Ng\n(M.Sc. Student)\n\n\n\n\n\nTin Satriawan\n(M.Sc. Student)\n\n\nMarion Nyberg\n(M.Sc. Student)\n\n\n\n\n\nAylin Barreras-Apodaca\n(Visiting International M.Sc. Student)\n\n\nNicole Choi\n(B.Sc. Student)\n\n\n\n\n\nCristina Mace\n(B.Sc. Student)\n\n\nAdin Litman\n(B.Sc. Student)\n\n\n\n\n\nAzumi Konaka\n(B.Sc. Student)\n\n\nWeiwen Fu\n(B.Sc. Student)\n\n\n\n\n\nHimari Honda (B.Sc. Student) \n\nKelsey McGuire\n(B.Sc. Student)",
    "crumbs": [
      "Home",
      "People"
    ]
  },
  {
    "objectID": "Meetings.html",
    "href": "Meetings.html",
    "title": "Lab Meetings",
    "section": "",
    "text": "Lab meetings are held weekly on Mondays from 12:30-1:30 Pacific time, the schedule is listed below. You can sign up for a day/topic using the google sheet link (contact june for access).\n\n\n\n\nTable 1: Schedule for Fall 2023 Lab meetings.\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nLead\nTopic\n4\n\n\n\n\nJan 12\nJune\nDiscussing WTD corrections at BB\nNA\n\n\nJan 19\n–\nQuick Check-in\nNA\n\n\nJan 26\nJune\nEddy Pro Automation Overview\nNA\n\n\nFeb 2\n–\nQuick Check-in\nNA\n\n\nFeb 9\nZoe\nHow to recompute chamber fluxes + Growing season analysis\nNA\n\n\nFeb 16\n–\nQuick Check-in\nNA\n\n\nFeb 23\nNo Lab Meeting\nReading Break\nNA\n\n\nMar 1\nTed\nprelim gap-filling via SSA results 🤞\nNA\n\n\nMar 8\nNo Lab Meeting\nNA\nNA\n\n\nMar 15\nVanessa\nprobably research proposal discussion/feedback\nNA\n\n\nMar 22\nTzu-Yi\nEGU Practice Presentation\nNA\n\n\nMar 29\nNo Lab Meeting\nGood Friday\nNA\n\n\nApr 5\n–\nQuick Check-in\nNA\n\n\nApr 12\nKatrina\npossibly soil sequestration results\nNA\n\n\nApr 19\nJoyson\nWetland cooling potential - first results\nNA\n\n\nApr 26\n–\nQuick Check-in\nNA\n\n\nMay 3\n–\nQuick Check-in\nNA\n\n\nMay 10\nSarah R\nparitioning results\nNA\n\n\nMay 17\nJune\nQuick Check-in\nNA\n\n\nMay 24\nZoe\nCGU Practice Presentation\nNA\n\n\nMay 31\n–\nQuick Check-in\nVanessa and Tzu-Yi away (field school TA)",
    "crumbs": [
      "Home",
      "Lab Meetings"
    ]
  },
  {
    "objectID": "Join.html",
    "href": "Join.html",
    "title": "Join the Lab!",
    "section": "",
    "text": "Thank you for your interest in joining our lab! I am always happy to hear from students and postdocs interested in our ongoing research activities.\nPlease contact me by email (sara.knox@ubc.ca) for more details. The deadline for applications in geography is mid-December for MSc students and early January for PhD students, however, I encourage you to contact me well before this deadline. Also, please keep in mind the deadlines for external funding opportunities through NSERC. While graduate student support at UBC is available through teaching assistantships and UBC fellowships, I strongly encourage prospective students to apply for external funding.",
    "crumbs": [
      "Home",
      "Join the lab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/3_Data_Cleaning_Principles.html",
    "href": "PipelineDocumentation/3_Data_Cleaning_Principles.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section outlines the data cleaning principles and processes that should be followed closely and applied at each of the three stages, as well as other relevant information.\n\nFigure 3. Chart outlining the data cleaning principles followed in the pipeline.\n\n\n\nPrinciple: During the first stage of data cleaning, we want to keep the best measured data values from each sensor, i.e., a user looking for the best measurements from a particular sensor would want to use the first stage data. Missing data points for periods of more than one half-hour will not be gap-filled in any way. \nAt the most basic level, the first stage collects the raw data, applies min/max filtering, assigns consistent variable names in line with Ameriflux guidelines as far as possible, and moves the relevant files to a Clean folder in preparation for stage two. This folder will be created automatically if it does not already exist.\nThe data are stored as binary files in “single-precision floating-point format” (aka float 32), which importantly means they are readable in most common computer languages and software.\n\n\n\n\n\nPrinciple: The second stage focuses on creating the best measured data values for each particular property/variable. For example, if there are two (or more) collocated temperature sensors at the same height e.g., 2 metres above ground level, the second stage is intended to create the “best” trace using both (all) sensors, with the highest precision/accuracy and the fewest missing points. By default, it does this by averaging the traces, and gap-filling by using linear regression. Optionally, data can be gap-filled using a nearby climate station, or one of your own nearby stations if you have one. (Other gap-filling methods, such as using reanalysis data, are currently under development.)\nIn practice, the second stage collects the first stage data, generates the “best” observation for each variable and moves the relevant files to a “Clean/SecondStage” folder in preparation for the third stage.\n\n\n\n\n\nPrinciple: USTAR filtering, gap-filling, and CO2 flux partitioning.\nThe third stage collects the second stage data and implements USTAR filtering, gap-filling, and flux partitioning procedures. For this we use the R package REddyProc. Biomet.net functions allow Matlab to interface with R, so all three stages are run directly from Matlab. For more machine learning approaches to gap-filling, we also implement the random forest approach described in the paper by Kim et al. (2020). Additional methane gap-filling processes currently not part of the pipeline are described here, with instructions here.\n\nWe achieve these principles by setting up various configuration files used in each stage. This process is described later (section 4.3), but there are a few more steps to complete before that. Next, you will set up your project directory structure and then configure it to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "3. Data Cleaning Principles"
    ]
  },
  {
    "objectID": "PipelineDocumentation/3_Data_Cleaning_Principles.html#data-cleaning-principles",
    "href": "PipelineDocumentation/3_Data_Cleaning_Principles.html#data-cleaning-principles",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section outlines the data cleaning principles and processes that should be followed closely and applied at each of the three stages, as well as other relevant information.\n\nFigure 3. Chart outlining the data cleaning principles followed in the pipeline.\n\n\n\nPrinciple: During the first stage of data cleaning, we want to keep the best measured data values from each sensor, i.e., a user looking for the best measurements from a particular sensor would want to use the first stage data. Missing data points for periods of more than one half-hour will not be gap-filled in any way. \nAt the most basic level, the first stage collects the raw data, applies min/max filtering, assigns consistent variable names in line with Ameriflux guidelines as far as possible, and moves the relevant files to a Clean folder in preparation for stage two. This folder will be created automatically if it does not already exist.\nThe data are stored as binary files in “single-precision floating-point format” (aka float 32), which importantly means they are readable in most common computer languages and software.\n\n\n\n\n\nPrinciple: The second stage focuses on creating the best measured data values for each particular property/variable. For example, if there are two (or more) collocated temperature sensors at the same height e.g., 2 metres above ground level, the second stage is intended to create the “best” trace using both (all) sensors, with the highest precision/accuracy and the fewest missing points. By default, it does this by averaging the traces, and gap-filling by using linear regression. Optionally, data can be gap-filled using a nearby climate station, or one of your own nearby stations if you have one. (Other gap-filling methods, such as using reanalysis data, are currently under development.)\nIn practice, the second stage collects the first stage data, generates the “best” observation for each variable and moves the relevant files to a “Clean/SecondStage” folder in preparation for the third stage.\n\n\n\n\n\nPrinciple: USTAR filtering, gap-filling, and CO2 flux partitioning.\nThe third stage collects the second stage data and implements USTAR filtering, gap-filling, and flux partitioning procedures. For this we use the R package REddyProc. Biomet.net functions allow Matlab to interface with R, so all three stages are run directly from Matlab. For more machine learning approaches to gap-filling, we also implement the random forest approach described in the paper by Kim et al. (2020). Additional methane gap-filling processes currently not part of the pipeline are described here, with instructions here.\n\nWe achieve these principles by setting up various configuration files used in each stage. This process is described later (section 4.3), but there are a few more steps to complete before that. Next, you will set up your project directory structure and then configure it to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "3. Data Cleaning Principles"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_5_Full_Doc_Third_Stage_Cleaning_And_Ameriflux_Output.html",
    "href": "PipelineDocumentation/5_5_Full_Doc_Third_Stage_Cleaning_And_Ameriflux_Output.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section is intended to complement the instructions for third stage cleaning provided in the tutorial in section 4.5. Here we provide further details of outlier removal algorithms and u-star filtering.\nOn this page:\n\nGeneral outline for creating your third stage YML configuration file\nRunning third stage cleaning in Matlab\nThird stage filters and variable names\nOutput to Ameriflux CSV file\nTips\n\n\n\n\n\n\nAs with the first and second stage cleaning, if you are doing this for the first time, we recommend that you obtain the simplified template files from the quick-start instructions in section 4 and follow the tutorial. The third stage cleaning generally requires the least amount of work of the three stages, and here are the steps:\n\nOpen your site-specific SITEID1_config.yml for editing.\nFill in the details for your site in the “Metadata” section at the top of the file. The northOffset can be found in your site GHG biomet file and this information is used for filtering data by wind direction, to comply with eddy-covariance measurement standards.\nMake sure that you are processing the correct fluxes as measured at your site, under Processing -&gt; ThirdStage -&gt; Fluxes, and input NULL for any fluxes not measured at your site.\nCheck the met variable names used for gap-filling are named the same as those output by your second stage cleaning, under Met_Gap_Filling -&gt; Linear_Interpolation -&gt; Fill_Vars.\n\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the Biomet.net library and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs; these site-specific settings will overwrite those in the global_config.yml if they are also defined there.\n\n\n\n\n\n\nIf you already ran the first and second stage cleaning, simply run:\nfr_automated_cleaning(yearIn,'SITEID',7)\nNote that the third stage is run with an argument of ‘7’ (not ‘3’), and this stage is typically more computationally expensive than the first two stages.\nOnce you have set up all the relevant INI and YML configuration files, you can test all stages together using the following Matlab command:\nfr_automated_cleaning(yearIn,'SITEID',[1 2 7])\n\n\n\n\n\n\nThe standalone flux variable names (i.e., FCH4, FC, H, LE) are copied directly from the second stage output, then wind sector and precipitation filters are applied to comply with eddy-covariance measurement theory, with no change to the variable name. For the variable names with suffixes following the flux variables, these suffixes represent different algorithms that we have applied sequentially, in the order that they appear (Table 5.5A).\nAll these filters are set to be applied by default, but most of them can optionally be turned off by setting Run: FALSE in your third stage YML configuration file. As a reminder, algorithms that need parameter inputs have default values in the global_config.yml file which we as a group arrived at by doing sensitivity tests and reading the literature; however, these values can also be overwritten in your site-specific YML file if desired.\nTable 5.5A. Third stage flux output variable definitions, descriptions, and site-specific options.\n\n\n\n\n\n\n\n\n\nSuffix\nDefinition\nDescription\nOptional\n\n\n\n\nNo suffix\nStandard cleaning\nApplies wind direction and precipitation filtering to comply with eddy-covariance theory.\nNo. Runs by default. However, if you wish to, you can edit the wind sector and precipitation filter values, under Standard_Cleaning/wakeFilter, and Standard_Cleaning/precipCutOff.\n\n\n_PI_SC\nSC = Storage Correction\nAdds the storage term to the CO2 flux (NEE = FC + SC).\nYes. Set Run: FALSE if you do not want to add the storage term.\n\n\n_PI_SC_JSZ\nPlus JSZ = z-score filter\nFor outlier removal: uses a sliding window to calculate z-score, i.e., (flux – mean)/(std. dev.), and removes data points with an absolute value greater than the threshold z_thresh set in global_config.yml. window size and z_thresh values can both be overwritten in your site-specific YML file if desired.\nYes. Set Run: FALSE if you do not want to removal outliers with this filter.\n\n\n_PI_SC_JSZ_MAD\nPlus Median of Absolute Deviation (about the median) filter\nFor outlier removal: follows Papale et al. (2006).\nYes. Set Run: FALSE if you do not want to removal outliers with this filter.\n\n\n_PI_SC_JSZ_MAD_RP\nPlus REddyProc applied (u-star filtering)\nCO2 flux partitioning and u-star filtering using REddyProc (Wutzler et al., 2018). Descriptions of suffixes applied to REddyProc output, e.g., _uStar, U95, _orig, and _f, can be found at this link.\nYes. Set Run: FALSE if you do not want to run flux partitioning and u-star filtering using REddyProc.\n\n\n\n\n\n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux. In case you do not want/need to rerun the cleaning (the third stage can be computationally expensive), simply run:\nfr_automated_cleaning(yearIn,'SITEID',8)\nAlternatively, to run all stages:\nfr_automated_cleaning(yearIn,'SITEID',[1 2 7 8])\n\n\n\n\nTIPS\nWe recommend not editing the global_config.yml file directly as it is part of the Biomet.net library and therefore will create issues when you try to update the library (which you should be doing regularly).",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.5 &nbsp; Full Documentation: Third Stage Cleaning and Converting to Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_5_Full_Doc_Third_Stage_Cleaning_And_Ameriflux_Output.html#quick-start-third-stage-cleaning-and-converting-to-ameriflux-output",
    "href": "PipelineDocumentation/5_5_Full_Doc_Third_Stage_Cleaning_And_Ameriflux_Output.html#quick-start-third-stage-cleaning-and-converting-to-ameriflux-output",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section is intended to complement the instructions for third stage cleaning provided in the tutorial in section 4.5. Here we provide further details of outlier removal algorithms and u-star filtering.\nOn this page:\n\nGeneral outline for creating your third stage YML configuration file\nRunning third stage cleaning in Matlab\nThird stage filters and variable names\nOutput to Ameriflux CSV file\nTips\n\n\n\n\n\n\nAs with the first and second stage cleaning, if you are doing this for the first time, we recommend that you obtain the simplified template files from the quick-start instructions in section 4 and follow the tutorial. The third stage cleaning generally requires the least amount of work of the three stages, and here are the steps:\n\nOpen your site-specific SITEID1_config.yml for editing.\nFill in the details for your site in the “Metadata” section at the top of the file. The northOffset can be found in your site GHG biomet file and this information is used for filtering data by wind direction, to comply with eddy-covariance measurement standards.\nMake sure that you are processing the correct fluxes as measured at your site, under Processing -&gt; ThirdStage -&gt; Fluxes, and input NULL for any fluxes not measured at your site.\nCheck the met variable names used for gap-filling are named the same as those output by your second stage cleaning, under Met_Gap_Filling -&gt; Linear_Interpolation -&gt; Fill_Vars.\n\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the Biomet.net library and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs; these site-specific settings will overwrite those in the global_config.yml if they are also defined there.\n\n\n\n\n\n\nIf you already ran the first and second stage cleaning, simply run:\nfr_automated_cleaning(yearIn,'SITEID',7)\nNote that the third stage is run with an argument of ‘7’ (not ‘3’), and this stage is typically more computationally expensive than the first two stages.\nOnce you have set up all the relevant INI and YML configuration files, you can test all stages together using the following Matlab command:\nfr_automated_cleaning(yearIn,'SITEID',[1 2 7])\n\n\n\n\n\n\nThe standalone flux variable names (i.e., FCH4, FC, H, LE) are copied directly from the second stage output, then wind sector and precipitation filters are applied to comply with eddy-covariance measurement theory, with no change to the variable name. For the variable names with suffixes following the flux variables, these suffixes represent different algorithms that we have applied sequentially, in the order that they appear (Table 5.5A).\nAll these filters are set to be applied by default, but most of them can optionally be turned off by setting Run: FALSE in your third stage YML configuration file. As a reminder, algorithms that need parameter inputs have default values in the global_config.yml file which we as a group arrived at by doing sensitivity tests and reading the literature; however, these values can also be overwritten in your site-specific YML file if desired.\nTable 5.5A. Third stage flux output variable definitions, descriptions, and site-specific options.\n\n\n\n\n\n\n\n\n\nSuffix\nDefinition\nDescription\nOptional\n\n\n\n\nNo suffix\nStandard cleaning\nApplies wind direction and precipitation filtering to comply with eddy-covariance theory.\nNo. Runs by default. However, if you wish to, you can edit the wind sector and precipitation filter values, under Standard_Cleaning/wakeFilter, and Standard_Cleaning/precipCutOff.\n\n\n_PI_SC\nSC = Storage Correction\nAdds the storage term to the CO2 flux (NEE = FC + SC).\nYes. Set Run: FALSE if you do not want to add the storage term.\n\n\n_PI_SC_JSZ\nPlus JSZ = z-score filter\nFor outlier removal: uses a sliding window to calculate z-score, i.e., (flux – mean)/(std. dev.), and removes data points with an absolute value greater than the threshold z_thresh set in global_config.yml. window size and z_thresh values can both be overwritten in your site-specific YML file if desired.\nYes. Set Run: FALSE if you do not want to removal outliers with this filter.\n\n\n_PI_SC_JSZ_MAD\nPlus Median of Absolute Deviation (about the median) filter\nFor outlier removal: follows Papale et al. (2006).\nYes. Set Run: FALSE if you do not want to removal outliers with this filter.\n\n\n_PI_SC_JSZ_MAD_RP\nPlus REddyProc applied (u-star filtering)\nCO2 flux partitioning and u-star filtering using REddyProc (Wutzler et al., 2018). Descriptions of suffixes applied to REddyProc output, e.g., _uStar, U95, _orig, and _f, can be found at this link.\nYes. Set Run: FALSE if you do not want to run flux partitioning and u-star filtering using REddyProc.\n\n\n\n\n\n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux. In case you do not want/need to rerun the cleaning (the third stage can be computationally expensive), simply run:\nfr_automated_cleaning(yearIn,'SITEID',8)\nAlternatively, to run all stages:\nfr_automated_cleaning(yearIn,'SITEID',[1 2 7 8])\n\n\n\n\nTIPS\nWe recommend not editing the global_config.yml file directly as it is part of the Biomet.net library and therefore will create issues when you try to update the library (which you should be doing regularly).",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.5 &nbsp; Full Documentation: Third Stage Cleaning and Converting to Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_Motivation.html",
    "href": "PipelineDocumentation/1_Motivation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Collecting and handling eddy covariance data presents challenges on many fronts, including site and instrument selection, high frequency data processing, data post-processing, and QA/QC. While there are many online resources that can help with all of these different challenges of eddy covariance measurements (see examples below), we focus on the approach that the EcoFlux Lab uses for flux data post-processing and QA/QC. Here we provide a detailed outline of our procedures for data post-processing QA/QC, gap-filling, and CO2 flux partitioning. We also provide data visualization tools. We draw on expertise from regional networks and FLUXNET, and leverage widely used tools in the field such as REddyProc and other recently developed machine learning-based gap-filling resources. By following these procedures, you will be able to develop a robust data pipeline to ensure high quality data that you can submit to your regional flux networks. Data standardization and reproducibility helps minimize errors, enhances data reliability, and enables the integration of data sets from diverse ecosystems into global networks like FLUXNET.\nNote that in this pipeline, we are focused on data post-processing, rather than the processing of high frequency (e.g., 20 Hz) data. While we do provide resources on high frequency flux processing, this is not within the main scope of this data pipeline. For more information on site set-up, instrument selection, and processing of high frequency data, please contact Xiangmin Sun from the FLUXNET CH4 and N2O processing committee.\nWhile the data pipeline described here requires some coding knowledge, the whole pipeline can be run without having to modify any existing code. You only need to edit some initialization files and then execute the code.\nAdditional resources:\n\nFlux Data Post-Processing and QA/QC\nFluxcourse Educational Materials\nVideos on the FLUXNET website",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_Motivation.html#motivation-the-importance-of-flux-data-standardization-and-reproducibility",
    "href": "PipelineDocumentation/1_Motivation.html#motivation-the-importance-of-flux-data-standardization-and-reproducibility",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Collecting and handling eddy covariance data presents challenges on many fronts, including site and instrument selection, high frequency data processing, data post-processing, and QA/QC. While there are many online resources that can help with all of these different challenges of eddy covariance measurements (see examples below), we focus on the approach that the EcoFlux Lab uses for flux data post-processing and QA/QC. Here we provide a detailed outline of our procedures for data post-processing QA/QC, gap-filling, and CO2 flux partitioning. We also provide data visualization tools. We draw on expertise from regional networks and FLUXNET, and leverage widely used tools in the field such as REddyProc and other recently developed machine learning-based gap-filling resources. By following these procedures, you will be able to develop a robust data pipeline to ensure high quality data that you can submit to your regional flux networks. Data standardization and reproducibility helps minimize errors, enhances data reliability, and enables the integration of data sets from diverse ecosystems into global networks like FLUXNET.\nNote that in this pipeline, we are focused on data post-processing, rather than the processing of high frequency (e.g., 20 Hz) data. While we do provide resources on high frequency flux processing, this is not within the main scope of this data pipeline. For more information on site set-up, instrument selection, and processing of high frequency data, please contact Xiangmin Sun from the FLUXNET CH4 and N2O processing committee.\nWhile the data pipeline described here requires some coding knowledge, the whole pipeline can be run without having to modify any existing code. You only need to edit some initialization files and then execute the code.\nAdditional resources:\n\nFlux Data Post-Processing and QA/QC\nFluxcourse Educational Materials\nVideos on the FLUXNET website",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_3_Quick_Start_Create_First_Stage_INI_File.html",
    "href": "PipelineDocumentation/4_3_Quick_Start_Create_First_Stage_INI_File.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to start cleaning your data for the first stage. Note that the example assumes you have already created Flux and Met databases for the same site.\nOn this page:\n\nDownload INI/config template files\nInstructions to create first stage INI file\nTips\n\n\n\n\n\n\nThe following download provides templates for all stages of cleaning, so you will have the second and third stage INI/config files for later; this section only covers the first stage.\nTemplate and Sample INI/config files\n\nDownload the files above; unzip the contents, and copy the TEMPLATE files in the Template_INI_config_files folder to the location shown in figure 4.3A:\n\nFigure 4.3A. Directory tree showing location of template INI files inside relevant SITEID folder.\nRename the TEMPLATE files you just copied, replacing “TEMPLATE” with your site ID (SITEID1 in this example; this filename format is required).\nThe files in the Tutorial_filled_sample_files folder are completed, “filled-in” versions of the TEMPLATE files that work with the sample eddy-covariance and met data available for download in section 4.2.\n\n\n\n\n\n\n\n\n\nOpen your site-specific SITEID1_FirstStage.ini file for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better), your site ID, and the timezone information. Difference_GMT_to_local_time is the timezone that your site is located in, and Timezone is the timezone that your data is stored in (UTC, standard time, or daylight saving time). Both these parameters are currently necessary in order to output the correct timezone for Ameriflux requirements (local standard time).\nNext, scroll down (skipping “Global Variables” sections for now) to where you see:\n %----------------------------------------------------\n %--&gt; Insert met variables here\n %----------------------------------------------------\nBeginning with air temperature, define the input data to each trace given in the INI file. For example, if you have an output variable that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 4.3B; yellow highlighted text). During this first cleaning stage this trace will be renamed as TA_1_1_1 using the variableName parameter, following the Ameriflux naming convention.\n\nFigure 4.3B. Air temperature trace (TA_1_1_1) as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab’s datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs). \n\nOnce you have done this for a few variables, test the data cleaning in Matlab, using the fr_automated_cleaning.m function as follows:\n fr_automated_cleaning(yearIn,'SITEID',1)\nwhere yearIn is a numeric array containing the year(s) of data you want to clean (it is possible to clean multiple years at once but we will keep things simple with one year for now), SITEID would be SITEID1 from the earlier example, and ‘1’ represents the first stage of data cleaning.\nProvided everything worked with no errors, you should now see output data matching the variables you defined in a new Clean folder within your Met folder.\n\n\n\nIf you do NOT have a four-component net radiometer such as a CNR4 at your site, you can skip all of step 5 and go to step 6. If you do have a CNR4 or similar, we have provided an “include” INI file that loads relevant radiation variables for you. We strongly advise that you do not edit this or any other include file. Instead, scroll to the bottom of your site-specific INI file where the include files are called.\n\nUncomment* (e.g., delete % - NOT #) the line containing RAD_FirstStage_include.ini, so that the block of code looks like this:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n%#include EddyPro_Common_FirstStage_include.ini\n%#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n%#include EddyPro_LI7700_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\n*Recall that this file is read by Matlab, so comments are created using %; the # sign must be present to call the include file.\nNext, scroll almost to the top of the file where you see:\n%----------------------------------------------------------\n% Global variable specification (trace-specific)\n%----------------------------------------------------------\n%--&gt; Radiation sensor information: \n%--&gt; If using CNR4 with RAD_FirstStage_include file, add inputFileNames etc., for example:\n%globalVars.Trace.SW_IN_1_1_1.inputFileName     = {'MET_CNR4_SWi_Avg'}\n\nglobalVars.Trace.SW_IN_1_1_1.inputFileName              = {''}\nglobalVars.Trace.SW_IN_1_1_1.inputFileName_dates        = [datenum(1900,1,1) datenum(2999,12,31)]\nglobalVars.Trace.SW_IN_1_1_1.instrument                 = 'CNR4'\nglobalVars.Trace.SW_IN_1_1_1.instrumentSN               = ''\nFor SW_IN_1_1_1 (incoming shortwave radiation), add:\n\nThe inputFileName from your database;\nEdit the start date for your data record in inputFileName_dates;\nEdit instrument (if necessary); and\nAdd the serial number (instrumentSN).\n\nAll these parameters are used for the other radiation variables defined below this one, apart from inputFileName, which you will need to input for each variable. Also, if you have a PAR radiometer, you can use the PPFD global variables in this radiation section in the same way.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this change. This step will add the radiation variables into the same Met folder as in step 4.\n\nNext, we will clean your Flux variables. As previously mentioned, if you followed step 5, we have provided “include” files that load most information on common traces for you.\n\nTo use these files, scroll to the bottom of your first stage INI file. For anyone with EddyPro output, you need to use EddyPro_Common_FirstStage_include.ini, which contains all variables common to all IRGAs. Then, also uncomment the other line(s) for your specific IRGA(s).\nFor example, let’s assume you are using a LiCor LI-7200 and LI-7700 at your site. Then you would uncomment the relevant include files, as follows:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n#include EddyPro_LI7700_FirstStage_include.ini\n%#include RAD_FirstStage_include.ini\nNow scroll up to:\n%---------------------------------------\n% Global variables (instrument-specific)\n%---------------------------------------\n% The instrument traces will all need to have the same\n% instrumentType field 'LI7200', 'LI7700'...\n% and come from the same software (EddyPro)\n\n% If using LI7200 with EddyPro_LI7200_FirstStage_include file:\nglobalVars.Instrument.IRGA.Enable                       = 1     % Required variable 0/1\nglobalVars.Instrument.IRGA.instrument                   = 'LI-7200' % Edit if using LI7500\nglobalVars.Instrument.IRGA.instrumentSN                 = ''    \nglobalVars.Instrument.IRGA.inputFileName_dates          = [datenum(1900,1,1) datenum(2999,12,31)]\nEnter/edit the relevant parameter details for your IRGA(s) (instrument, serial number, start date of measurements). Do the same for your anemometer and full eddy-covariance (EC) system instruments (e.g., CSAT3 plus IRGA). ‘otherTraces’ includes all instrument types not previously defined within global variables, and must be enabled, with measurement record dates defined.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this addition. You should see your first-stage cleaned data appear in a new Clean folder within your Flux folder.\n\nAt this stage, you can start to add any Met or Flux variables that may not already be included in the template files. You can do this by copy-pasting the [Trace]...[End] code block (like the one in figure 4.3B) and editing the parameter inputs accordingly.\nAlso, for example, if you have more than one air temperature measurement, you would create more traces to assign these and use the Ameriflux naming convention to distinguish and define their relative positions (figure 4.3C).\n\nFigure 4.3C. Second air temperature trace (TA_1_2_1) defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\n\n\n\n\nTips\n\n\nPay attention to the output display; it is informative.\n\n\nWe recommend using the “quick-look” visualization tools at any stage of cleaning to check that your data looks as expected (e.g., the filenames are correct and the retained values conform to your minMax bounds).\n\n\nSee section 5.3 for lots more guidance and information on first stage INI files. For cases where you do not have input data for all traces defined in any include file that you use, see “Special Cases” in section 7 on troubleshooting.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.3 &nbsp; Quick Start Tutorial: Create your First Stage INI File for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_3_Quick_Start_Create_First_Stage_INI_File.html#quick-start-tutorial-create-your-first-stage-ini-file-for-data-cleaning",
    "href": "PipelineDocumentation/4_3_Quick_Start_Create_First_Stage_INI_File.html#quick-start-tutorial-create-your-first-stage-ini-file-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to start cleaning your data for the first stage. Note that the example assumes you have already created Flux and Met databases for the same site.\nOn this page:\n\nDownload INI/config template files\nInstructions to create first stage INI file\nTips\n\n\n\n\n\n\nThe following download provides templates for all stages of cleaning, so you will have the second and third stage INI/config files for later; this section only covers the first stage.\nTemplate and Sample INI/config files\n\nDownload the files above; unzip the contents, and copy the TEMPLATE files in the Template_INI_config_files folder to the location shown in figure 4.3A:\n\nFigure 4.3A. Directory tree showing location of template INI files inside relevant SITEID folder.\nRename the TEMPLATE files you just copied, replacing “TEMPLATE” with your site ID (SITEID1 in this example; this filename format is required).\nThe files in the Tutorial_filled_sample_files folder are completed, “filled-in” versions of the TEMPLATE files that work with the sample eddy-covariance and met data available for download in section 4.2.\n\n\n\n\n\n\n\n\n\nOpen your site-specific SITEID1_FirstStage.ini file for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better), your site ID, and the timezone information. Difference_GMT_to_local_time is the timezone that your site is located in, and Timezone is the timezone that your data is stored in (UTC, standard time, or daylight saving time). Both these parameters are currently necessary in order to output the correct timezone for Ameriflux requirements (local standard time).\nNext, scroll down (skipping “Global Variables” sections for now) to where you see:\n %----------------------------------------------------\n %--&gt; Insert met variables here\n %----------------------------------------------------\nBeginning with air temperature, define the input data to each trace given in the INI file. For example, if you have an output variable that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 4.3B; yellow highlighted text). During this first cleaning stage this trace will be renamed as TA_1_1_1 using the variableName parameter, following the Ameriflux naming convention.\n\nFigure 4.3B. Air temperature trace (TA_1_1_1) as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab’s datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs). \n\nOnce you have done this for a few variables, test the data cleaning in Matlab, using the fr_automated_cleaning.m function as follows:\n fr_automated_cleaning(yearIn,'SITEID',1)\nwhere yearIn is a numeric array containing the year(s) of data you want to clean (it is possible to clean multiple years at once but we will keep things simple with one year for now), SITEID would be SITEID1 from the earlier example, and ‘1’ represents the first stage of data cleaning.\nProvided everything worked with no errors, you should now see output data matching the variables you defined in a new Clean folder within your Met folder.\n\n\n\nIf you do NOT have a four-component net radiometer such as a CNR4 at your site, you can skip all of step 5 and go to step 6. If you do have a CNR4 or similar, we have provided an “include” INI file that loads relevant radiation variables for you. We strongly advise that you do not edit this or any other include file. Instead, scroll to the bottom of your site-specific INI file where the include files are called.\n\nUncomment* (e.g., delete % - NOT #) the line containing RAD_FirstStage_include.ini, so that the block of code looks like this:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n%#include EddyPro_Common_FirstStage_include.ini\n%#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n%#include EddyPro_LI7700_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\n*Recall that this file is read by Matlab, so comments are created using %; the # sign must be present to call the include file.\nNext, scroll almost to the top of the file where you see:\n%----------------------------------------------------------\n% Global variable specification (trace-specific)\n%----------------------------------------------------------\n%--&gt; Radiation sensor information: \n%--&gt; If using CNR4 with RAD_FirstStage_include file, add inputFileNames etc., for example:\n%globalVars.Trace.SW_IN_1_1_1.inputFileName     = {'MET_CNR4_SWi_Avg'}\n\nglobalVars.Trace.SW_IN_1_1_1.inputFileName              = {''}\nglobalVars.Trace.SW_IN_1_1_1.inputFileName_dates        = [datenum(1900,1,1) datenum(2999,12,31)]\nglobalVars.Trace.SW_IN_1_1_1.instrument                 = 'CNR4'\nglobalVars.Trace.SW_IN_1_1_1.instrumentSN               = ''\nFor SW_IN_1_1_1 (incoming shortwave radiation), add:\n\nThe inputFileName from your database;\nEdit the start date for your data record in inputFileName_dates;\nEdit instrument (if necessary); and\nAdd the serial number (instrumentSN).\n\nAll these parameters are used for the other radiation variables defined below this one, apart from inputFileName, which you will need to input for each variable. Also, if you have a PAR radiometer, you can use the PPFD global variables in this radiation section in the same way.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this change. This step will add the radiation variables into the same Met folder as in step 4.\n\nNext, we will clean your Flux variables. As previously mentioned, if you followed step 5, we have provided “include” files that load most information on common traces for you.\n\nTo use these files, scroll to the bottom of your first stage INI file. For anyone with EddyPro output, you need to use EddyPro_Common_FirstStage_include.ini, which contains all variables common to all IRGAs. Then, also uncomment the other line(s) for your specific IRGA(s).\nFor example, let’s assume you are using a LiCor LI-7200 and LI-7700 at your site. Then you would uncomment the relevant include files, as follows:\n%----------------------------------------------------------\n% Call #include ini files\n%----------------------------------------------------------\n%--&gt; Must be at end of .ini file\n%--&gt; Comment out include files that are not needed\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n%#include EddyPro_LI7500_FirstStage_include.ini\n#include EddyPro_LI7700_FirstStage_include.ini\n%#include RAD_FirstStage_include.ini\nNow scroll up to:\n%---------------------------------------\n% Global variables (instrument-specific)\n%---------------------------------------\n% The instrument traces will all need to have the same\n% instrumentType field 'LI7200', 'LI7700'...\n% and come from the same software (EddyPro)\n\n% If using LI7200 with EddyPro_LI7200_FirstStage_include file:\nglobalVars.Instrument.IRGA.Enable                       = 1     % Required variable 0/1\nglobalVars.Instrument.IRGA.instrument                   = 'LI-7200' % Edit if using LI7500\nglobalVars.Instrument.IRGA.instrumentSN                 = ''    \nglobalVars.Instrument.IRGA.inputFileName_dates          = [datenum(1900,1,1) datenum(2999,12,31)]\nEnter/edit the relevant parameter details for your IRGA(s) (instrument, serial number, start date of measurements). Do the same for your anemometer and full eddy-covariance (EC) system instruments (e.g., CSAT3 plus IRGA). ‘otherTraces’ includes all instrument types not previously defined within global variables, and must be enabled, with measurement record dates defined.\nRun fr_automated_cleaning(yearIn,'SITEID',1) again to test this addition. You should see your first-stage cleaned data appear in a new Clean folder within your Flux folder.\n\nAt this stage, you can start to add any Met or Flux variables that may not already be included in the template files. You can do this by copy-pasting the [Trace]...[End] code block (like the one in figure 4.3B) and editing the parameter inputs accordingly.\nAlso, for example, if you have more than one air temperature measurement, you would create more traces to assign these and use the Ameriflux naming convention to distinguish and define their relative positions (figure 4.3C).\n\nFigure 4.3C. Second air temperature trace (TA_1_2_1) defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\n\n\n\n\nTips\n\n\nPay attention to the output display; it is informative.\n\n\nWe recommend using the “quick-look” visualization tools at any stage of cleaning to check that your data looks as expected (e.g., the filenames are correct and the retained values conform to your minMax bounds).\n\n\nSee section 5.3 for lots more guidance and information on first stage INI files. For cases where you do not have input data for all traces defined in any include file that you use, see “Special Cases” in section 7 on troubleshooting.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.3 &nbsp; Quick Start Tutorial: Create your First Stage INI File for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_2_Download_Biomet_Library.html",
    "href": "PipelineDocumentation/2_2_Download_Biomet_Library.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nInstructions to download the Biomet.net repository\nInstructions to clone the Biomet.net repository from a terminal\nBiomet tips\n\nThe Biomet.net repository contains libraries of computer code (in Matlab, R, and Python) related to running the data cleaning pipeline. It contains (a) the scripts to run the first, second, and third cleaning stages, as well as conversion to AmeriFlux format, and (b) many functions to help clean, process, analyze and visualize data throughout all the stages of data processing and cleaning.\nIn principle, users should not need to edit this code library. Instead, as far as possible, you will interface with it using various configuration files unique to your own project(s) and site(s). This process is described later.\nFirst, you need to either download or clone the repository. Instructions for both are given below; if you do not have Git installed on your computer and do not anticipate adding any of your own code to the Biomet.net library, then you can download the repository, as follows:\n\n\n\n\n\nGo to the Biomet.net library webpage.\nClick on the green “Code” button, and then click “Download ZIP” (figure 2.2A):\n\nFigure 2.2A. Screenshot of Biomet.net repository on GitHub, showing how to download.\nMove the downloaded Biomet.net-main.zip file to a convenient location on your computer, e.g., within your C: drive for PCs, or within /Users/&lt;username&gt;/ for Macs.\nImportantly, after unzipping the file, rename the unzipped directory from Biomet.net-main to Biomet.net.\nWe recommend repeating these steps periodically so that you remain up to date with our pipeline developments, keeping only the most recently downloaded Biomet.net folder on your computer. It should always be in the same location so that Matlab knows where to look for the library (see section 2.4 for details).\n\n\n\n\nAlternatively, if you prefer to clone the directory, you will need to have Git installed on your computer (section 2.1). Then you can follow these instructions:\n\n\n\nMake sure you are in the directory (folder) where you wish the repository to live, for example, your computer C: drive for PCs, or /Users/&lt;username&gt;/ for Macs, or something similar depending on your preferences.\nNext, follow the instructions on this website to clone the repository.\nIf you have any difficulties or are unsure of git procedures in general, see this online tutorial.\nAlways keep your local copy of the Biomet.net code up to date by periodically using the git pull command every few days, or when you know there has been a change to the code. You must be in the repository directory for this to work.\n\nNOTE: Editing existing files or saving new ones to the main branch of Biomet.net should generally be avoided. Reminder: if you wish to contribute your own code to the Biomet.net library, see section 2.1.\n\n\n\nTIPS\n\n\n\nReminder: we highly recommend redownloading or recloning (git pull) the Biomet.net library at least once per week.\n\n\nIf you suddenly get an unexpected error when cleaning, using files that e.g. were previously working and the solution is not obvious, try redownloading or recloning the library, and then rerunning the cleaning.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.2 &nbsp; Download Biomet.net Library"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_2_Download_Biomet_Library.html#download-or-clone-the-biomet.net-library-git-repository-to-your-local-computer",
    "href": "PipelineDocumentation/2_2_Download_Biomet_Library.html#download-or-clone-the-biomet.net-library-git-repository-to-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nInstructions to download the Biomet.net repository\nInstructions to clone the Biomet.net repository from a terminal\nBiomet tips\n\nThe Biomet.net repository contains libraries of computer code (in Matlab, R, and Python) related to running the data cleaning pipeline. It contains (a) the scripts to run the first, second, and third cleaning stages, as well as conversion to AmeriFlux format, and (b) many functions to help clean, process, analyze and visualize data throughout all the stages of data processing and cleaning.\nIn principle, users should not need to edit this code library. Instead, as far as possible, you will interface with it using various configuration files unique to your own project(s) and site(s). This process is described later.\nFirst, you need to either download or clone the repository. Instructions for both are given below; if you do not have Git installed on your computer and do not anticipate adding any of your own code to the Biomet.net library, then you can download the repository, as follows:\n\n\n\n\n\nGo to the Biomet.net library webpage.\nClick on the green “Code” button, and then click “Download ZIP” (figure 2.2A):\n\nFigure 2.2A. Screenshot of Biomet.net repository on GitHub, showing how to download.\nMove the downloaded Biomet.net-main.zip file to a convenient location on your computer, e.g., within your C: drive for PCs, or within /Users/&lt;username&gt;/ for Macs.\nImportantly, after unzipping the file, rename the unzipped directory from Biomet.net-main to Biomet.net.\nWe recommend repeating these steps periodically so that you remain up to date with our pipeline developments, keeping only the most recently downloaded Biomet.net folder on your computer. It should always be in the same location so that Matlab knows where to look for the library (see section 2.4 for details).\n\n\n\n\nAlternatively, if you prefer to clone the directory, you will need to have Git installed on your computer (section 2.1). Then you can follow these instructions:\n\n\n\nMake sure you are in the directory (folder) where you wish the repository to live, for example, your computer C: drive for PCs, or /Users/&lt;username&gt;/ for Macs, or something similar depending on your preferences.\nNext, follow the instructions on this website to clone the repository.\nIf you have any difficulties or are unsure of git procedures in general, see this online tutorial.\nAlways keep your local copy of the Biomet.net code up to date by periodically using the git pull command every few days, or when you know there has been a change to the code. You must be in the repository directory for this to work.\n\nNOTE: Editing existing files or saving new ones to the main branch of Biomet.net should generally be avoided. Reminder: if you wish to contribute your own code to the Biomet.net library, see section 2.1.\n\n\n\nTIPS\n\n\n\nReminder: we highly recommend redownloading or recloning (git pull) the Biomet.net library at least once per week.\n\n\nIf you suddenly get an unexpected error when cleaning, using files that e.g. were previously working and the solution is not obvious, try redownloading or recloning the library, and then rerunning the cleaning.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.2 &nbsp; Download Biomet.net Library"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_3_Other_Biomet_Plotting_Tools.html",
    "href": "PipelineDocumentation/6_3_Other_Biomet_Plotting_Tools.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This function is part of the Biomet.net library and is intended as a manual on how to tell Matlab to read data from your newly created database, learning by example (one code section at a time, sequentially). The following notes help to describe and explain each example:\n\nLoad one trace and plot it:\n\n\nUse the biomet_path function, e.g., pth = biomet_path(2022, 'DSM', 'MET') to give you the filepath to your data, in this case the Met data for the DSM site for all of 2022; you should not define the filepath yourself but always use this function.\nUse the read_bor function to load the time vector from the clean_tv file, then convert it to a datetime object using the datetime function.\nUse the read_bor function again to load the trace of your choice from the filepath you defined using biomet_path, e.g., x = read_bor(fullfile(pth,'MET_CNR4_Net_Avg')) will load the MET_CNR4_Net_Avg trace from the DSM Met folder for 2022, using the previously defined path.\nYou can now plot your variable x with a nicely formatted time vector.\n\n\nCompare two traces:\n\n\nThis is most useful for comparing traces from different cleaning stages, once you have cleaned your data - which we have not yet addressed. We will cover this again later in section\nUse biomet_path again to define the filepath to the Second Stage clean data, e.g., pth = biomet_path(2022,'DSM','Clean/SecondStage')\n\n\n\nOther functions for visualizing data:\n\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons.\nguiPlotTraces\nplotApp and RShiny apps: see sections 6.1 and 6.2, respectively.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization",
      "6.3 &nbsp; Other Biomet.net Plotting Tools"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_3_Other_Biomet_Plotting_Tools.html#other-biomet.net-plotting-tools",
    "href": "PipelineDocumentation/6_3_Other_Biomet_Plotting_Tools.html#other-biomet.net-plotting-tools",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This function is part of the Biomet.net library and is intended as a manual on how to tell Matlab to read data from your newly created database, learning by example (one code section at a time, sequentially). The following notes help to describe and explain each example:\n\nLoad one trace and plot it:\n\n\nUse the biomet_path function, e.g., pth = biomet_path(2022, 'DSM', 'MET') to give you the filepath to your data, in this case the Met data for the DSM site for all of 2022; you should not define the filepath yourself but always use this function.\nUse the read_bor function to load the time vector from the clean_tv file, then convert it to a datetime object using the datetime function.\nUse the read_bor function again to load the trace of your choice from the filepath you defined using biomet_path, e.g., x = read_bor(fullfile(pth,'MET_CNR4_Net_Avg')) will load the MET_CNR4_Net_Avg trace from the DSM Met folder for 2022, using the previously defined path.\nYou can now plot your variable x with a nicely formatted time vector.\n\n\nCompare two traces:\n\n\nThis is most useful for comparing traces from different cleaning stages, once you have cleaned your data - which we have not yet addressed. We will cover this again later in section\nUse biomet_path again to define the filepath to the Second Stage clean data, e.g., pth = biomet_path(2022,'DSM','Clean/SecondStage')\n\n\n\nOther functions for visualizing data:\n\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons.\nguiPlotTraces\nplotApp and RShiny apps: see sections 6.1 and 6.2, respectively.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization",
      "6.3 &nbsp; Other Biomet.net Plotting Tools"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_Quick_Start_Tutorial.html",
    "href": "PipelineDocumentation/4_Quick_Start_Tutorial.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "If you are a first-time user of this data-cleaning pipeline, we recommend that you follow this “quick-start” tutorial (all of section 4). We provide sample data and template files along with instructions for creating your first and second stage INI files, editing the relevant third stage configuration files, and outputting your clean data in the correct format for submission to Ameriflux.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_Quick_Start_Tutorial.html#quick-start-tutorial---recommended-for-first-time-users",
    "href": "PipelineDocumentation/4_Quick_Start_Tutorial.html#quick-start-tutorial---recommended-for-first-time-users",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "If you are a first-time user of this data-cleaning pipeline, we recommend that you follow this “quick-start” tutorial (all of section 4). We provide sample data and template files along with instructions for creating your first and second stage INI files, editing the relevant third stage configuration files, and outputting your clean data in the correct format for submission to Ameriflux.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_Full_Doc_Features_Details_Advanced.html",
    "href": "PipelineDocumentation/5_Full_Doc_Features_Details_Advanced.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section contains the full documentation for more advanced users, such as features, further details, and other useful information. The subsection numbering mirrors that in section 4 for easy reference.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_Full_Doc_Features_Details_Advanced.html#full-documentation-features-details-and-other-useful-information-for-advanced-users",
    "href": "PipelineDocumentation/5_Full_Doc_Features_Details_Advanced.html#full-documentation-features-details-and-other-useful-information-for-advanced-users",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section contains the full documentation for more advanced users, such as features, further details, and other useful information. The subsection numbering mirrors that in section 4 for easy reference.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_1_Quick_Start_Set_Up_Structure_Config.html",
    "href": "PipelineDocumentation/4_1_Quick_Start_Set_Up_Structure_Config.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nInstructions for setting up project directory structure and configuring Matlab\nShort description of new directory structure and contents\n\nThis section provides a quick-start summary for setting up your project directory structure and configuring Matlab to work with this structure. \nYou need the file path to your project root folder for these next steps. This project directory must be on your local computer: avoid putting it on an external drive or cloud drive (like OneDrive or GDrive) as this will cause problems in the steps that follow. Additionally, avoid putting it in the Biomet.net directory.\nYou will also need to create a site ID that relates to the flux site you are working with. This is often an acronym based on the site name (e.g., Burns Bog has siteID = 'BB', Richmond Brackish Marsh has siteID = 'RBM'). Note that the siteID must be all upper case.\n\n\n\n\n\nIf you have multiple flux sites, choose one site to begin working with. Using the Matlab command line, define your projectPath and siteID as in the following example:\nFor example, if the name of your project is My_Micromet, and your chosen siteID is SITEID1, you would enter the following commands in the Matlab command window:\nMac example:\n projectPath = '/Users/&lt;username&gt;/Projects/My_Micromet/';\n siteID = 'SITEID1';\nPC example:\n projectPath = 'C:\\Projects\\My_Micromet\\';\n siteID = 'SITEID1';\nNext, in Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned), as follows:\n create_TAB_ProjectFolders(projectPath,siteID)\nBoth input arguments are of type “string”. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info.\nAs the function name suggests, this will create some folders on your computer and also transfer some necessary small files. In your project root directory (e.g., My_Micromet in figure 4.1A), you should now see three new directories with the following names: (1) Database, (2) Matlab, (3) Sites.\n\nFigure 4.1A. Directory tree showing contents of project folder after running the create_TAB_ProjectFolders Matlab function.\n[Note: if you are working with an earlier version of Matlab than 2023b, you may get an error when running create_TAB_ProjectFolders (above). In that case, download this zip file, unzip, and put the contents of “My_Micromet_Folder” within your project directory (underneath My_Micromet); make sure your directory structure looks like figure 4.1A.]\nFinally, run the following command:\n set_TAB_project(projectPath)\nThis process sets up the Biomet.net toolbox to work with your project.\n\n\n\n\n\n\nFor now, we will describe only the new directories and files necessary for you to begin data cleaning. \n\n\n\nThe new Database directory contains a Calculation_Procedures directory, and within Calculation_Procedures, there is one called TraceAnalysis_ini.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your siteID (e.g., SITEID1 in figure 4.1B), as follows:\n\nFigure 4.1B. Directory tree showing contents of TraceAnalysis_ini folder after running the create_TAB_ProjectFolders Matlab function.\nEventually (but not yet!), this Database directory will also contain the following:\n\nInitial database created using your raw data;\nYour site-specific INI files that configure how the pipeline scripts will clean the data;\nThe cleaned data once the INI files have been created and the pipeline has been run.\n\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory (Figure 4.1C), under the appropriate siteID. The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 4.1C. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created “behind the scenes” in step 2 above, then used during step 3;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes and may also be useful to you later when you want to visualize your data.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.1 &nbsp; Quick Start Tutorial: Project Directory Structure and Matlab Configuration"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_1_Quick_Start_Set_Up_Structure_Config.html#quick-start-tutorial-project-directory-structure-and-matlab-configuration",
    "href": "PipelineDocumentation/4_1_Quick_Start_Set_Up_Structure_Config.html#quick-start-tutorial-project-directory-structure-and-matlab-configuration",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nInstructions for setting up project directory structure and configuring Matlab\nShort description of new directory structure and contents\n\nThis section provides a quick-start summary for setting up your project directory structure and configuring Matlab to work with this structure. \nYou need the file path to your project root folder for these next steps. This project directory must be on your local computer: avoid putting it on an external drive or cloud drive (like OneDrive or GDrive) as this will cause problems in the steps that follow. Additionally, avoid putting it in the Biomet.net directory.\nYou will also need to create a site ID that relates to the flux site you are working with. This is often an acronym based on the site name (e.g., Burns Bog has siteID = 'BB', Richmond Brackish Marsh has siteID = 'RBM'). Note that the siteID must be all upper case.\n\n\n\n\n\nIf you have multiple flux sites, choose one site to begin working with. Using the Matlab command line, define your projectPath and siteID as in the following example:\nFor example, if the name of your project is My_Micromet, and your chosen siteID is SITEID1, you would enter the following commands in the Matlab command window:\nMac example:\n projectPath = '/Users/&lt;username&gt;/Projects/My_Micromet/';\n siteID = 'SITEID1';\nPC example:\n projectPath = 'C:\\Projects\\My_Micromet\\';\n siteID = 'SITEID1';\nNext, in Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned), as follows:\n create_TAB_ProjectFolders(projectPath,siteID)\nBoth input arguments are of type “string”. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info.\nAs the function name suggests, this will create some folders on your computer and also transfer some necessary small files. In your project root directory (e.g., My_Micromet in figure 4.1A), you should now see three new directories with the following names: (1) Database, (2) Matlab, (3) Sites.\n\nFigure 4.1A. Directory tree showing contents of project folder after running the create_TAB_ProjectFolders Matlab function.\n[Note: if you are working with an earlier version of Matlab than 2023b, you may get an error when running create_TAB_ProjectFolders (above). In that case, download this zip file, unzip, and put the contents of “My_Micromet_Folder” within your project directory (underneath My_Micromet); make sure your directory structure looks like figure 4.1A.]\nFinally, run the following command:\n set_TAB_project(projectPath)\nThis process sets up the Biomet.net toolbox to work with your project.\n\n\n\n\n\n\nFor now, we will describe only the new directories and files necessary for you to begin data cleaning. \n\n\n\nThe new Database directory contains a Calculation_Procedures directory, and within Calculation_Procedures, there is one called TraceAnalysis_ini.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your siteID (e.g., SITEID1 in figure 4.1B), as follows:\n\nFigure 4.1B. Directory tree showing contents of TraceAnalysis_ini folder after running the create_TAB_ProjectFolders Matlab function.\nEventually (but not yet!), this Database directory will also contain the following:\n\nInitial database created using your raw data;\nYour site-specific INI files that configure how the pipeline scripts will clean the data;\nThe cleaned data once the INI files have been created and the pipeline has been run.\n\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory (Figure 4.1C), under the appropriate siteID. The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 4.1C. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created “behind the scenes” in step 2 above, then used during step 3;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes and may also be useful to you later when you want to visualize your data.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.1 &nbsp; Quick Start Tutorial: Project Directory Structure and Matlab Configuration"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html",
    "href": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "For those users processing flux data in EddyPro, here we provide general guidelines as to what we recommend using for input parameters.\nThis EddyPro API helps you automate and parallelize EddyPro processing. Within this git repository, you can find our recommended input parameters for the following (linked here):\n\nLi-Cor closed-path systems;\nLi-Cor open-path systems.\n\nIf you are not using Li-Cor instruments or EddyPro then we suggest following this working group: FLUXNET CH4 and N2O processing committee.",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility",
      "1.1 &nbsp; Note: High Frequency Data Processing"
    ]
  },
  {
    "objectID": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html#a-note-on-high-frequency-data-processing",
    "href": "PipelineDocumentation/1_1_Note_High_Freq_Data_Processing.html#a-note-on-high-frequency-data-processing",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "For those users processing flux data in EddyPro, here we provide general guidelines as to what we recommend using for input parameters.\nThis EddyPro API helps you automate and parallelize EddyPro processing. Within this git repository, you can find our recommended input parameters for the following (linked here):\n\nLi-Cor closed-path systems;\nLi-Cor open-path systems.\n\nIf you are not using Li-Cor instruments or EddyPro then we suggest following this working group: FLUXNET CH4 and N2O processing committee.",
    "crumbs": [
      "Pipeline Documentation",
      "1. Motivation: The Importance of Flux Data Standardization and Reproducibility",
      "1.1 &nbsp; Note: High Frequency Data Processing"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_6_Install_Python_Optional.html",
    "href": "PipelineDocumentation/2_6_Install_Python_Optional.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. Python can be used in a number of eddy covariance data packages (e.g., some of these flux data post-processing and QA/QC resources).\nPython is also necessary for those interested in running high-frequency data reprocessing described in this EddyPro API or gap-filling fluxes using the approach described in this paper: “Gap-filling eddy covariance methane fluxes: Comparison of machine learning model predictions and uncertainties at FLUXNET-CH4 wetlands”.\nHowever, you can skip this step if you do not plan to use any of the resources above.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.6 &nbsp; Install Software: Python (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_6_Install_Python_Optional.html#python-installation-on-your-local-computer-optional",
    "href": "PipelineDocumentation/2_6_Install_Python_Optional.html#python-installation-on-your-local-computer-optional",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. Python can be used in a number of eddy covariance data packages (e.g., some of these flux data post-processing and QA/QC resources).\nPython is also necessary for those interested in running high-frequency data reprocessing described in this EddyPro API or gap-filling fluxes using the approach described in this paper: “Gap-filling eddy covariance methane fluxes: Comparison of machine learning model predictions and uncertainties at FLUXNET-CH4 wetlands”.\nHowever, you can skip this step if you do not plan to use any of the resources above.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.6 &nbsp; Install Software: Python (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_3_Full_Doc_First_Stage_INI_Files.html",
    "href": "PipelineDocumentation/5_3_Full_Doc_First_Stage_INI_Files.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides extensive details beyond the first-time set up in section 4, to help more advanced users. If you are creating INI files for the first time, we highly recommend following our quick-start tutorial: instructions for INI file creation begin in section 4.3.\nOn this page:\n\nWhat are INI files?\nGeneral outline for creating first stage INI file\nRunning first stage cleaning in Matlab\nProperties and parameters: first stage INI\nTags for dependent variables\nGlobal variables and include files\nOutlier detection\nOverwrite feature\nMore about Evaluate and postEvaluate: order of operations\nCreating INI file from Ameriflux data\nProgramming syntax rules for first and second stage INI files\nTips\n\n\n\n\n\n\nThe INI files for a given flux site dictate how data is transferred from its raw format into a standardized, clean, and gap-filled format that can be used for scientific analysis. They provide instructions to a set of MATLAB and R scripts used to process data. Even though flux sites can be similar in set up, they are usually unique in some way (different sensors, loggers, record lengths, etc.), and the INI files provide a way to deal with these differences while standardizing the data across sites.\nThere are three files, one for each data cleaning stage. Before starting to create your own INI files, it is important that you complete all of section 2, read section 3, and complete sections 4.1 and 4.2 (which each have supplementary information in sections 5.1 and 5.2, respectively), so that you have the best chance of everything running smoothly.\n\n\n\n\n\n\nIf you are doing this for the first time, you can obtain simplified template files from the quick-start instructions in section 4.3, and we recommend that you follow the tutorial in that section. These are the general steps to create your first stage INI (which usually requires the most work of the three stages):\n\nUsing your duplicated first stage INI file or downloaded template file, rename it using your unique measurement site ID. For example, the first stage INI file for a site with siteID = ‘DSM’ is named DSM_FirstStage.ini. This exact naming convention is important for the pipeline libraries to locate and use the file.\nEdit this first stage INI file, adding just a few variables at a time and test as you go (see subsection “Running first stage cleaning in Matlab” later on this page), before adding more variables. This will make troubleshooting much easier. The tutorial in section 4.3 gives step-by-step instructions for adding variables.\nDetails on INI file properties and parameters are listed in table 5.1 on this page.\nIn addition to the data cleaning principles previously outlined in section 3, keep in mind the following guidelines:\n\nSelect traces that are needed for future data analysis. Not all the measured variables from a site need to be here, only the ones that will be used in future analysis or those needed to improve cleaning, such as diagnostic variables.\nEach first stage trace name should follow Ameriflux guidelines including positional qualifiers where relevant. Filenames from your raw database can be renamed here.\nThe original values can be altered; calibrations can be applied; units can be changed.\nApply basic filtering: (a) values can be removed if they exceed minMax thresholds; (b) values can be clamped to the thresholds if they exceed clampedMinMax values.\nYou can create dependencies between different traces. If one trace has some data points removed, all its dependent traces will also have those data points removed.\nMore complex user-defined processing can be applied to the trace using the very useful “Evaluate” option (also available in second stage cleaning). Matlab functions (user-written or Biomet.net) can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” string. \n\nExample for air temperature trace: if you have an output variable from a Campbell Scientific data logger that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 5.3A; yellow highlighted text). In this example, it would be renamed in this stage using the variableName parameter, as TA_1_1_1, following the Ameriflux naming convention.\n\nFigure 5.3A. Air temperature trace (TA_1_1_1) as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab’s datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs).\nFor details of all the parameters and their definitions, see the “Properties and parameters: first stage INI” subsection below.\n\nIf you have more than one air temperature measurement, you would create more traces to assign these, and use the Ameriflux naming convention to distinguish and define their relative positions (figure 5.3B). \n\nFigure 5.3B. Second air temperature trace (TA_1_2_1) defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\n\n\n\n\n\n\n\nOnce you have a few variables in your INI file, test it by running the fr_automated_cleaning.m function in Matlab (part of Biomet.net library), using the following command:\nfr_automated_cleaning_(yearIn,'SITEID',1)\nwhere the arguments are defined as follows:\nTable 5.3A. Argument definitions for fr_automated_cleaning function.\n\n\n\n\n\n\n\n\nField\nDescription\nType\n\n\n\n\nyearIn\nyear(s) you wish to clean data for\ninteger or integer array\n\n\nSITEID\nmeasurement site ID, e.g., ‘DSM’\nstring uppercase\n\n\ncleaning stage to run\n1 = first stage, 2 = second stage, 7 = third stage, 8 = convert to AmeriFlux CSV file\ninteger\n\n\n\nFor example, to run first stage cleaning for the DSM site for 2022, you would type: fr_automated_cleaning(2022,'DSM',1).\nWith this function, you can clean multiple years of data, e.g., 2020:2023, and once you have your INI files set up for later cleaning stages you can also run multiple stages e.g., [1 2 7]. Earlier stages must have been run at least once before running subsequent stages, so the appropriate data files exist as input for the next stage.\n\n\n\n\n\n\n\nTable 5.3B. First stage INI file properties and parameters.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nHeader/Comments\n“%” character indicates the beginning of a comment. Program will not process any characters that follow “%”. Use comments to add information and to better document the site. Each line of the INI file can be followed by a comment. Refer to the sample INI file for DSM site.\n\n\nSite_name\nName of the site. Any text can go here.\n\n\nSiteID\nThis is the name attributed to the site in the database (e.g., DSM). Must be uppercase.\n\n\nDifference_GMT_to_local_time\nTime difference between GMT time, that database is kept in, and the standard time at the site location. local_time+Difference_GMT_to_local_time -&gt; GMT time.\n\n\n[Trace]\nMarks the beginning of a new variable. The section has to end with the keyword [End].\n\n\nvariableName\nName of variable for first stage, following the Ameriflux naming convention. The variable with this name will show up in the subfolder “Clean” under the same folder where the original database file came from.\n\n\ntitle\nDescriptive variable name for plots/visualization.\n\n\ninputFileName\n{inputFileName} The name of the database file that contains data for this trace. The brackets are mandatory. The file name can include folder(s), e.g., 'Met/Tair', and the paths are relative to the main site path (./Database/yyyy/SITEID), so the above example translates into this filepath: './Database/yyyy/SITEID/Met/Tair'.Over the lifetime of a measurement site, the data logger programs can change and a sensor measurement that was assigned to a variable may change. To allow for different variable names over the site history the inputFileName can be given as: {'inputFileName1','inputFileName2'}. In this case, the parameter inputFileName_dates must be present and reflect this (see next parameter description). Advanced: if there is a need to load up a data file from an alternative site, the path can be constructed as follows: ../../SITEID2/dataFolder. This syntax ../../ moves the path pointer up two directory levels to /Database/yyyy and from there SITEID2/dataFolder takes the program to the correct folder.\n\n\ninputFileName_dates\n[datenum_start1 datenum_end1; datenum_start2 datenum_end2] The start and end dates of data periods for each of the inputFileNames, using the Matlab datenum function. If there are multiple inputFileNames per the example in the previous parameter description, e.g., {'inputFileName1','inputFileName2'} then the program needs to know the time periods when the data assigned to the variableName should come from inputFileName1, and when from inputFileName2. In that case, this field is mandatory. If the inputFileName parameter contains only one file name, this inputFileName_dates parameter is optional, but it is still a good practice to use it anyway for documentation purposes and in case other filenames are added in the future. The last datenum_end is usually set far into the future, e.g., datenum(2999,1,1).\n\n\nmeasurementType\nUsually 'Met' or 'Flux' (must be of Matlab type char). Mandatory parameter that sets the input and output trace folders. The input folder defaults to: SITEID/measurementType/inputFileName. If relative paths are used for the inputFileName parameter defined previously, the pipeline code “assumes” that the current folder is SITEID/measurementType, so the relative path is referenced to that. The output folder for the first stage cleaned trace is always SITEID/measurementType/Clean. Note: the measurementType must not be missing (empty), otherwise the data will be saved to SITEID/Clean/variableName which is incorrect and will cause errors in future cleaning stages.\n\n\nunits\nMeasurement units for this trace must be data type char. Important!\n\n\ninstrument\nThe name of the sensor that measures this trace, e.g., 'HMP155A'\n\n\ninstrumentSN\nSerial number of the sensor, if available.\n\n\nEvaluate\nOptional user-defined function. Examples: can be used to derive variables from available data; in flag variables to remove bad data; or for calculating new useful variables, e.g., Evaluate = 'TA_1_1_1 = shiftMyData(clean_tv,TA_1_1_1,datenum(2021,11,07,03,00,0),60);'. \n\n\npostEvaluate\nOptional user-defined function, intended for more complex cases. Same functionality as Evaluate property, but postEvaluate is executed after all the other Trace properties. \n\n\nloggedCalibration\nUsed together with currentCalibration (see next parameter). If you need to change the linear calibration for the trace, these coefficients are used to convert the trace values from engineering units to their original/raw units. Then the correct calibration coefficients (currentCalibration) are used. This can also be used to change the units. The format is [gain offset startDatenum endDatenum], where startDatenum and endDatenum refer to the time span that this particular set of coefficients should be applied. For example, with no change for data starting on 1 January 2020, the code would read loggedCalibration = [1 0 datenum(2020,1,1) datenum(2999,1,1)]. You can apply multiple calibrations to different time periods, separated by semicolons. Note: all calibration values need to be on the same line of code, i.e., no line-breaks are allowed in the INI file!\n\n\ncurrentCalibration\nCorrect(ed) linear calibration coefficients. Used together with loggedCalibrations (see notes for previous parameter for more details).\n\n\ncomments\nAny useful comments relating to this trace and its handling in the INI file, such as why certain flags are applied.\n\n\nminMax\n[min max] Minimum and maximum numerical thresholds for filtering. The values outside of this range will be set to NaN.\n\n\nclamped_minMax\n[cMin cMax] Similar to minMax but instead of setting the data points outside the range to NaN, it truncates their value to the cMin or cMax. (e.g., RH: [0 100]).Note: this parameter is not mandatory, however, when used, please make sure that the minMax property boundaries are wider than the boundaries of clamped_minMax because the minMax property is applied first. This parameter is useful for variables such as relative humidity, e.g., minMax = [-1 110] used with clamped_minMax=[0 100].\n\n\nzeroPt\nValue to indicate missing data. Many programs nowadays use -9999 to indicate bad/missing data points.\n\n\ndependent\nFilter-dependent variables based on specified trace. The current trace can have multiple dependents that need to be separated by commas, e.g., dependent = 'trace1','trace2','trace3'.For example, when using the LI-7200 pump, all the traces that depend on the LI-7200 are dependent on the pump trace. So, for the LI-7200: dependent = 'CO2','H2O'. Then the CO2 trace should have dependent = 'FC',... and so on. You can write these out manually where necessary or where this only occurs once or twice, but we highly recommend using the “tags” feature for common dependencies that are recurring.\n\n\n[End]\nMarks the end of the properties section for each trace.\n\n\n\n Note: Other properties that a user wants to use later on in their own programs (or in the “Evaluate” statements) can be added to each of the traces. The function that processes the INI files (read_ini_files.m) will add the property and its assigned value to the trace structure, but the rest of the Trace Analysis programs will ignore it. The user can then parse the trace info in their own programs (or within “Evaluate” statements) and take advantage of this feature.\n\n\n\n\n\n\nAs mentioned in the “dependent” parameter description in Table 5.1, you can use the tags feature to ensure that your INI file “catches” and filters all common dependent variables. This feature utilizes the Biomet library function tags_Standard.m. You should list the relevant tag in the “dependent” property field for it to work.\nFor example, given the standard tags, if you put 'tag_H2O_All' in the dependent field for a trace, all traces listed under that tag will then be dependents of that trace. Tags can refer to other tags. You can also create site-specific custom tags by creating a SITEID_CustomTags.m, making sure to follow the same format as the tags in tags_Standard.m. (This optional file should live in the Derived_Variables folder; see figure 4.7 in section 5.1 for the location).\nTags in your site-specific custom file will overwrite tags of the same name in the tags_Standard.m file, and Matlab will warn you that this is occurring (by beeping and writing out a message to the screen).\n\n\n\n\n\n\nTo simplify entering and editing the required parameters into the first stage INI file, there is an option to apply the same setting to many traces at once, using the “global variables” feature. At the top of the first stage INI template file (obtained in section 4.3), you will notice sections with headers containing “Global variables”. These are always defined at the beginning of the first stage INI file.\nThe main advantage of this feature is that it allows you to define a standard set of variables in templates called “include” files, which can be loaded into each site-specific INI file using the #include statement. We have provided samples of typical Met and EddyPro include files in the quick-start tutorial.\nAnother advantage is that you can apply changes to multiple traces all at once if needed, by making only one edit at the top of the file and then referencing this addition. This way, you do not risk missing multiple traces needing the same edit, so long as it is referenced correctly.\nNote: only the main body of your first stage INI file, not your “include” files, should contain global variables.\nThere are two types of global variables, as follows:\n\nInstrument-specific (globalVars.Instrument):\n\n\nFor the instrument-specific global variables, each trace has an instrumentType field. Currently we use five default instrument types (six if we consider an empty field [] as a type): LI7200, LI7700, Anemometer, EC, and otherTraces.\nExample:\nglobalVars.Instrument.LI7200.instrumentSN  = '72H-1029'\napplies this serial number to every trace which includes instrumentType = 'LI7200'.\nYou can also create your own unique sensor option for instrumentType, if you need to apply a setting to multiple traces.\notherTraces has been depreciated so if an existing INI file contains this code (e.g., from an old template file), you can either set it to zero or remove those lines of code altogether (otherTraces.Enable = 0). If this is set to 1, you will get warnings when you run cleaning.\n\n\nTrace-specific (globalVars.Trace):\n\n\nExample:\nglobalVars.Trace.CH4.currentCalibration \n                  = [1000 0 datenum(2021,1,1) datenum(2999,1,1)]\napplies this current calibration to the trace with variableName = 'CH4'.\nDetailed explanation: in the INI file, the trace “CH4” has the field currentCalibration set to empty: currentCalibration = []. There is a global variable for the same trace with the correct currentCalibration field:\nglobalVars.Trace.CH4.currentCalibration \n                  = [1000 0 datenum(2021,1,1) datenum(2999,1,1)]\nThe final result is that the field currentCalibration for the trace “CH4” gets set to: currentCalibration = [1000 0 datenum(2021,1,1) datenum(2999,1,1)].\n\n\nOther (globalVars.other):\n\n\nFor the third type of global variable, e.g., globalVars.other, currently you can use this to carry out single-point interpolation, for cases where just one half-hourly data point is missing:\nglobalVars.other.singlePointInterpolation = 'on'  % 'no_interp' - skip interpolation [default], 'on' - do single missing point interpolation for all traces\n\nAdding include files to your First Stage INI file\nIn sections 4.1 and 4.2, we introduced include files and how they are obtained. These template files define a standard set of variables that can be loaded into your first stage INI file. Specifically, the files relate to EddyPro output and radiation components. We suggest browsing through these files to become familiar with the content. If you wish to change or add any settings, we strongly recommend leaving the template “include” files untouched and using the global variables feature to create or overwrite the relevant setting, as previously described.\nMore information about include files:\n\nEddyPro_Common_FirstStage_include.ini: includes traces common to EddyPro output, listed in EddyPro file output order;\nEddyPro_LI7200_FirstStage_include.ini: when an LI-7200 IRGA is being used, this file adds LI-7200 specific traces, listed in EddyPro file output order;\nEddyPro_LI7500_FirstStage_include.ini: when an LI-7500 IRGA is being used, this file adds LI-7500 specific traces, listed in EddyPro file output order;\nEddyPro_LI7500_FirstStage_include.ini: when an LI-7500 IRGA is being used, this file adds LI-7500 specific traces, listed in EddyPro file output order;\nRAD_FirstStage_include.ini: includes radiation components.\n\nTo include any of these files, add the following line of code to the very bottom of your first stage INI file: #include &lt;relevant_file&gt;_include.ini. If you are using more than one, put them on multiple lines of code, still at the very bottom of the INI file, as follows:\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\nHow the global variables algorithm works:\n\nThe INI file trace parameters, i.e., all fields between [TRACE] and [END] in the INI file, are loaded up.\nThe program then cycles through all the traces: for each trace, first it checks if the instrumentType field matches one of the globalVars.Instrument fields, e.g., for instrumentType = 'LI7700' measuring “CH4”, if there is a global.Instrument.LI7700 field, then all its fields would be applied to the CH4 trace, either creating a new field or overwriting the existing fields; i.e., the content of global.Instrument.LI7700.instrumentSN would replace the content of the CH4 field instrumentSN. Secondly, it checks if the trace variableName matches any of the globalVars.Trace fields, e.g. Trace “CH4”.\nThe program continues reading the INI file and applies all the settings from the [TRACE]-[END] section.\nNext, it cycles through all the globalVars.Instrument fields and creates the fields or overwrites them if they already exist.\nFinally, it goes through all the globalVars.Trace fields and creates or overwrites the fields.\nThen the program moves to the next trace and repeats steps 2–5.\n\n\n\n\n\n\n\nAs outlined earlier on this page, the Evaluate feature can be used to implement Matlab code. This can be useful for removing outliers from your data in the first stage. For example, in the Biomet library there are some existing functions to help with this, such as:\n\nremove_spikes_diurnal_nonParametric.m\nrun_std_dev.m\n\nAlternatively, you could include your own code to do this, depending on what you need.\n\n\n\n\n\n\nSometimes we need to overwrite multiple properties for one or more traces that have already been created e.g. in an include file. The global variables feature allows this, however, once this section becomes long with many trace property tweaks, it can become very hard to troubleshoot, and in these cases having all the information for one trace together is more desirable.\nInstead of using global variables you can duplicate the full trace ([Trace] ... [End]), and in your site-specific first stage INI file, put this duplicate after the line of code where the include file is called that contains your original trace. Note the additional Overwrite property highlighted in yellow:\n\nFigure 5.3C. Location within site-specific INI file to put duplicate trace for overwriting a trace previously defined in an include INI file. In this case, we want to overwrite the CH4_MIXING_RATIO trace that was originally defined in EddyPro_LI7700_FirstStage_include.ini. Yellow highlighting shows the syntax for the “overwrite” property.\nThere are three overwrite options:\n\n0 = do not overwrite with this trace. This is the default setting. If you do not include the Overwrite parameter, the pipeline assumes this option. If you have a duplicate trace, you will get an error during cleaning (Example 1):\n\n1 = overwrite traces having the same variableName and also with Overwrite = 0 setting. This puts the duplicate data where the original data was, i.e., complete overwrite, first trace gone. Use this setting if you want your duplicate T2 available to use in a later variable such as T3, T4, T5, etc. (Example 2):\n\n2 = overwrite traces having the same variableName and with Overwrite = 0 setting. This takes advantage of the “position” of the duplicate trace. Use this setting if you want a later variable such as T3 or T4 available to use in T2.\n\n\n\n\n\n\n\n\nMore complex user-defined processing can be applied in the first stage to any trace using the very useful “Evaluate” and new “postEvaluate” options. Matlab functions (user-written or from Biomet.net) can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” or “postEvaluate” strings. They can be used to derive variables from available data, flag variables to remove bad data, or to calculate new useful variables. Here is an Evaluate example for removing outliers from a trace:\nEvaluate = 'wlen=24;thres=4;TA_1_1_1 = run_std_dev(TA_1_1_1,clean_tv,wlen,thres);'\nThe Evaluate property is executed for all traces before any other cleaning properties, e.g., minMax, calibration, etc.. In contrast, the newer postEvaluate property is executed in the first stage after all other cleaning is done.\nGenerally speaking, the order of operations is:\nEvaluate –&gt; other cleaning (e.g., minMax, calibration) –&gt; postEvaluate.\nThis is regardless of the order that they appear  within [Trace] ... [End]. The following series of examples shows how the Evaluate and postEvaluate statements work in relation to the other cleaning properties. For each example the input (Original) is an array of ones, and both the “Original” and first-stage “Clean” data are plotted in the result.\nExample 1: Only minMax and calibration, no Evaluate or postEvaluate statements\n\nResult: x = 2x [calibrated]\n\n\nExample 2: Evaluate, minMax and calibration, no postEvaluate statement\n\nResult: x = (x - 1)*2 [Evaluate first, then calibrate]\n\n\nExample 3: minMax and calibration, postEvaluate statement, no Evaluate statement\n\nResult: x = 2x - 1 [calibrate, then postEvaluate]\n\n\nExample 4: minMax and calibration, both Evaluate and postEvaluate statements\n\nResult: x = (x - 1)*2 - 1 [Evaluate, then calibrate, then postEvaluate]\n\n\n\n\n\n\n\n\n\nSome programming rules that you must follow for the first and second stage INI files to be successfully read by the pipeline scripts:\n\n1. We enforce using uppercase for site IDs to avoid problems with running data cleaning on Mac vs. Windows.\n2. All traces must be enclosed in [Trace] and [End] blocks.\n3. All assignments can be on multiple lines but should be enclosed in single quotes.\n4. Comments must begin with a percentage sign (%).\n5. All fields must be in Matlab format.\n6. All parameter assignments must be to strings in single quotes, or numeric expressions, e.g., threshold_const = 6, threshold_const = [6], variableName = 'Some Name'.\n7. For the first stage, the partial path must be included with the inputFileName when you locate the raw data trace in the database. (Using biomet_path function only returns the path: /year/SITEID/measType/)\n8. First stage necessary fields are: variableName, inputFileName, measurementType, units, title, and minMax.\n9. Second stage necessary fields are: variableName, title, units.\n\n\n\nTIPS\nOnce your INI file is complete, if desired you can use the simplify_FirstStageIni() function to clean it up. This may be helpful if you used a legacy INI file as an example; the function removes obsolete fields and makes things more concise.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.3 &nbsp; Full Documentation: First Stage INI files"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_3_Full_Doc_First_Stage_INI_Files.html#full-documentation-first-stage-ini-files",
    "href": "PipelineDocumentation/5_3_Full_Doc_First_Stage_INI_Files.html#full-documentation-first-stage-ini-files",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides extensive details beyond the first-time set up in section 4, to help more advanced users. If you are creating INI files for the first time, we highly recommend following our quick-start tutorial: instructions for INI file creation begin in section 4.3.\nOn this page:\n\nWhat are INI files?\nGeneral outline for creating first stage INI file\nRunning first stage cleaning in Matlab\nProperties and parameters: first stage INI\nTags for dependent variables\nGlobal variables and include files\nOutlier detection\nOverwrite feature\nMore about Evaluate and postEvaluate: order of operations\nCreating INI file from Ameriflux data\nProgramming syntax rules for first and second stage INI files\nTips\n\n\n\n\n\n\nThe INI files for a given flux site dictate how data is transferred from its raw format into a standardized, clean, and gap-filled format that can be used for scientific analysis. They provide instructions to a set of MATLAB and R scripts used to process data. Even though flux sites can be similar in set up, they are usually unique in some way (different sensors, loggers, record lengths, etc.), and the INI files provide a way to deal with these differences while standardizing the data across sites.\nThere are three files, one for each data cleaning stage. Before starting to create your own INI files, it is important that you complete all of section 2, read section 3, and complete sections 4.1 and 4.2 (which each have supplementary information in sections 5.1 and 5.2, respectively), so that you have the best chance of everything running smoothly.\n\n\n\n\n\n\nIf you are doing this for the first time, you can obtain simplified template files from the quick-start instructions in section 4.3, and we recommend that you follow the tutorial in that section. These are the general steps to create your first stage INI (which usually requires the most work of the three stages):\n\nUsing your duplicated first stage INI file or downloaded template file, rename it using your unique measurement site ID. For example, the first stage INI file for a site with siteID = ‘DSM’ is named DSM_FirstStage.ini. This exact naming convention is important for the pipeline libraries to locate and use the file.\nEdit this first stage INI file, adding just a few variables at a time and test as you go (see subsection “Running first stage cleaning in Matlab” later on this page), before adding more variables. This will make troubleshooting much easier. The tutorial in section 4.3 gives step-by-step instructions for adding variables.\nDetails on INI file properties and parameters are listed in table 5.1 on this page.\nIn addition to the data cleaning principles previously outlined in section 3, keep in mind the following guidelines:\n\nSelect traces that are needed for future data analysis. Not all the measured variables from a site need to be here, only the ones that will be used in future analysis or those needed to improve cleaning, such as diagnostic variables.\nEach first stage trace name should follow Ameriflux guidelines including positional qualifiers where relevant. Filenames from your raw database can be renamed here.\nThe original values can be altered; calibrations can be applied; units can be changed.\nApply basic filtering: (a) values can be removed if they exceed minMax thresholds; (b) values can be clamped to the thresholds if they exceed clampedMinMax values.\nYou can create dependencies between different traces. If one trace has some data points removed, all its dependent traces will also have those data points removed.\nMore complex user-defined processing can be applied to the trace using the very useful “Evaluate” option (also available in second stage cleaning). Matlab functions (user-written or Biomet.net) can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” string. \n\nExample for air temperature trace: if you have an output variable from a Campbell Scientific data logger that represents your 2-m air temperature measurement named MET_HMP_T_2m_Avg, you would assign this variable name to the inputFileName parameter (figure 5.3A; yellow highlighted text). In this example, it would be renamed in this stage using the variableName parameter, as TA_1_1_1, following the Ameriflux naming convention.\n\nFigure 5.3A. Air temperature trace (TA_1_1_1) as defined in a first stage INI file, with input from MET_HMP_T_2m_Avg variable.\nOther fields that need editing are highlighted in peach:\n\nGive a descriptive title to your trace;\nInput the start date of measurements for this variable in matlab’s datenum format (YYYY,MM,DD);\nInput the source folder into measurementType (Met/Flux/other; these should match up with different measurement systems, i.e., Met for the Campbell Scientific data example, and Flux for EddyPro data);\nCheck the units;\nIf known, input the instrument model and serial number (SN);\nImportantly, choose minMax bounds that are appropriate for the climate of your site (values outside this range will become NaNs).\nFor details of all the parameters and their definitions, see the “Properties and parameters: first stage INI” subsection below.\n\nIf you have more than one air temperature measurement, you would create more traces to assign these, and use the Ameriflux naming convention to distinguish and define their relative positions (figure 5.3B). \n\nFigure 5.3B. Second air temperature trace (TA_1_2_1) defined in the same first stage INI file, in this case for a different vertical position (height of 350 cm), with input from MET_HMP_T_350cm_Avg variable.\n\n\n\n\n\n\n\n\nOnce you have a few variables in your INI file, test it by running the fr_automated_cleaning.m function in Matlab (part of Biomet.net library), using the following command:\nfr_automated_cleaning_(yearIn,'SITEID',1)\nwhere the arguments are defined as follows:\nTable 5.3A. Argument definitions for fr_automated_cleaning function.\n\n\n\n\n\n\n\n\nField\nDescription\nType\n\n\n\n\nyearIn\nyear(s) you wish to clean data for\ninteger or integer array\n\n\nSITEID\nmeasurement site ID, e.g., ‘DSM’\nstring uppercase\n\n\ncleaning stage to run\n1 = first stage, 2 = second stage, 7 = third stage, 8 = convert to AmeriFlux CSV file\ninteger\n\n\n\nFor example, to run first stage cleaning for the DSM site for 2022, you would type: fr_automated_cleaning(2022,'DSM',1).\nWith this function, you can clean multiple years of data, e.g., 2020:2023, and once you have your INI files set up for later cleaning stages you can also run multiple stages e.g., [1 2 7]. Earlier stages must have been run at least once before running subsequent stages, so the appropriate data files exist as input for the next stage.\n\n\n\n\n\n\n\nTable 5.3B. First stage INI file properties and parameters.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nHeader/Comments\n“%” character indicates the beginning of a comment. Program will not process any characters that follow “%”. Use comments to add information and to better document the site. Each line of the INI file can be followed by a comment. Refer to the sample INI file for DSM site.\n\n\nSite_name\nName of the site. Any text can go here.\n\n\nSiteID\nThis is the name attributed to the site in the database (e.g., DSM). Must be uppercase.\n\n\nDifference_GMT_to_local_time\nTime difference between GMT time, that database is kept in, and the standard time at the site location. local_time+Difference_GMT_to_local_time -&gt; GMT time.\n\n\n[Trace]\nMarks the beginning of a new variable. The section has to end with the keyword [End].\n\n\nvariableName\nName of variable for first stage, following the Ameriflux naming convention. The variable with this name will show up in the subfolder “Clean” under the same folder where the original database file came from.\n\n\ntitle\nDescriptive variable name for plots/visualization.\n\n\ninputFileName\n{inputFileName} The name of the database file that contains data for this trace. The brackets are mandatory. The file name can include folder(s), e.g., 'Met/Tair', and the paths are relative to the main site path (./Database/yyyy/SITEID), so the above example translates into this filepath: './Database/yyyy/SITEID/Met/Tair'.Over the lifetime of a measurement site, the data logger programs can change and a sensor measurement that was assigned to a variable may change. To allow for different variable names over the site history the inputFileName can be given as: {'inputFileName1','inputFileName2'}. In this case, the parameter inputFileName_dates must be present and reflect this (see next parameter description). Advanced: if there is a need to load up a data file from an alternative site, the path can be constructed as follows: ../../SITEID2/dataFolder. This syntax ../../ moves the path pointer up two directory levels to /Database/yyyy and from there SITEID2/dataFolder takes the program to the correct folder.\n\n\ninputFileName_dates\n[datenum_start1 datenum_end1; datenum_start2 datenum_end2] The start and end dates of data periods for each of the inputFileNames, using the Matlab datenum function. If there are multiple inputFileNames per the example in the previous parameter description, e.g., {'inputFileName1','inputFileName2'} then the program needs to know the time periods when the data assigned to the variableName should come from inputFileName1, and when from inputFileName2. In that case, this field is mandatory. If the inputFileName parameter contains only one file name, this inputFileName_dates parameter is optional, but it is still a good practice to use it anyway for documentation purposes and in case other filenames are added in the future. The last datenum_end is usually set far into the future, e.g., datenum(2999,1,1).\n\n\nmeasurementType\nUsually 'Met' or 'Flux' (must be of Matlab type char). Mandatory parameter that sets the input and output trace folders. The input folder defaults to: SITEID/measurementType/inputFileName. If relative paths are used for the inputFileName parameter defined previously, the pipeline code “assumes” that the current folder is SITEID/measurementType, so the relative path is referenced to that. The output folder for the first stage cleaned trace is always SITEID/measurementType/Clean. Note: the measurementType must not be missing (empty), otherwise the data will be saved to SITEID/Clean/variableName which is incorrect and will cause errors in future cleaning stages.\n\n\nunits\nMeasurement units for this trace must be data type char. Important!\n\n\ninstrument\nThe name of the sensor that measures this trace, e.g., 'HMP155A'\n\n\ninstrumentSN\nSerial number of the sensor, if available.\n\n\nEvaluate\nOptional user-defined function. Examples: can be used to derive variables from available data; in flag variables to remove bad data; or for calculating new useful variables, e.g., Evaluate = 'TA_1_1_1 = shiftMyData(clean_tv,TA_1_1_1,datenum(2021,11,07,03,00,0),60);'. \n\n\npostEvaluate\nOptional user-defined function, intended for more complex cases. Same functionality as Evaluate property, but postEvaluate is executed after all the other Trace properties. \n\n\nloggedCalibration\nUsed together with currentCalibration (see next parameter). If you need to change the linear calibration for the trace, these coefficients are used to convert the trace values from engineering units to their original/raw units. Then the correct calibration coefficients (currentCalibration) are used. This can also be used to change the units. The format is [gain offset startDatenum endDatenum], where startDatenum and endDatenum refer to the time span that this particular set of coefficients should be applied. For example, with no change for data starting on 1 January 2020, the code would read loggedCalibration = [1 0 datenum(2020,1,1) datenum(2999,1,1)]. You can apply multiple calibrations to different time periods, separated by semicolons. Note: all calibration values need to be on the same line of code, i.e., no line-breaks are allowed in the INI file!\n\n\ncurrentCalibration\nCorrect(ed) linear calibration coefficients. Used together with loggedCalibrations (see notes for previous parameter for more details).\n\n\ncomments\nAny useful comments relating to this trace and its handling in the INI file, such as why certain flags are applied.\n\n\nminMax\n[min max] Minimum and maximum numerical thresholds for filtering. The values outside of this range will be set to NaN.\n\n\nclamped_minMax\n[cMin cMax] Similar to minMax but instead of setting the data points outside the range to NaN, it truncates their value to the cMin or cMax. (e.g., RH: [0 100]).Note: this parameter is not mandatory, however, when used, please make sure that the minMax property boundaries are wider than the boundaries of clamped_minMax because the minMax property is applied first. This parameter is useful for variables such as relative humidity, e.g., minMax = [-1 110] used with clamped_minMax=[0 100].\n\n\nzeroPt\nValue to indicate missing data. Many programs nowadays use -9999 to indicate bad/missing data points.\n\n\ndependent\nFilter-dependent variables based on specified trace. The current trace can have multiple dependents that need to be separated by commas, e.g., dependent = 'trace1','trace2','trace3'.For example, when using the LI-7200 pump, all the traces that depend on the LI-7200 are dependent on the pump trace. So, for the LI-7200: dependent = 'CO2','H2O'. Then the CO2 trace should have dependent = 'FC',... and so on. You can write these out manually where necessary or where this only occurs once or twice, but we highly recommend using the “tags” feature for common dependencies that are recurring.\n\n\n[End]\nMarks the end of the properties section for each trace.\n\n\n\n Note: Other properties that a user wants to use later on in their own programs (or in the “Evaluate” statements) can be added to each of the traces. The function that processes the INI files (read_ini_files.m) will add the property and its assigned value to the trace structure, but the rest of the Trace Analysis programs will ignore it. The user can then parse the trace info in their own programs (or within “Evaluate” statements) and take advantage of this feature.\n\n\n\n\n\n\nAs mentioned in the “dependent” parameter description in Table 5.1, you can use the tags feature to ensure that your INI file “catches” and filters all common dependent variables. This feature utilizes the Biomet library function tags_Standard.m. You should list the relevant tag in the “dependent” property field for it to work.\nFor example, given the standard tags, if you put 'tag_H2O_All' in the dependent field for a trace, all traces listed under that tag will then be dependents of that trace. Tags can refer to other tags. You can also create site-specific custom tags by creating a SITEID_CustomTags.m, making sure to follow the same format as the tags in tags_Standard.m. (This optional file should live in the Derived_Variables folder; see figure 4.7 in section 5.1 for the location).\nTags in your site-specific custom file will overwrite tags of the same name in the tags_Standard.m file, and Matlab will warn you that this is occurring (by beeping and writing out a message to the screen).\n\n\n\n\n\n\nTo simplify entering and editing the required parameters into the first stage INI file, there is an option to apply the same setting to many traces at once, using the “global variables” feature. At the top of the first stage INI template file (obtained in section 4.3), you will notice sections with headers containing “Global variables”. These are always defined at the beginning of the first stage INI file.\nThe main advantage of this feature is that it allows you to define a standard set of variables in templates called “include” files, which can be loaded into each site-specific INI file using the #include statement. We have provided samples of typical Met and EddyPro include files in the quick-start tutorial.\nAnother advantage is that you can apply changes to multiple traces all at once if needed, by making only one edit at the top of the file and then referencing this addition. This way, you do not risk missing multiple traces needing the same edit, so long as it is referenced correctly.\nNote: only the main body of your first stage INI file, not your “include” files, should contain global variables.\nThere are two types of global variables, as follows:\n\nInstrument-specific (globalVars.Instrument):\n\n\nFor the instrument-specific global variables, each trace has an instrumentType field. Currently we use five default instrument types (six if we consider an empty field [] as a type): LI7200, LI7700, Anemometer, EC, and otherTraces.\nExample:\nglobalVars.Instrument.LI7200.instrumentSN  = '72H-1029'\napplies this serial number to every trace which includes instrumentType = 'LI7200'.\nYou can also create your own unique sensor option for instrumentType, if you need to apply a setting to multiple traces.\notherTraces has been depreciated so if an existing INI file contains this code (e.g., from an old template file), you can either set it to zero or remove those lines of code altogether (otherTraces.Enable = 0). If this is set to 1, you will get warnings when you run cleaning.\n\n\nTrace-specific (globalVars.Trace):\n\n\nExample:\nglobalVars.Trace.CH4.currentCalibration \n                  = [1000 0 datenum(2021,1,1) datenum(2999,1,1)]\napplies this current calibration to the trace with variableName = 'CH4'.\nDetailed explanation: in the INI file, the trace “CH4” has the field currentCalibration set to empty: currentCalibration = []. There is a global variable for the same trace with the correct currentCalibration field:\nglobalVars.Trace.CH4.currentCalibration \n                  = [1000 0 datenum(2021,1,1) datenum(2999,1,1)]\nThe final result is that the field currentCalibration for the trace “CH4” gets set to: currentCalibration = [1000 0 datenum(2021,1,1) datenum(2999,1,1)].\n\n\nOther (globalVars.other):\n\n\nFor the third type of global variable, e.g., globalVars.other, currently you can use this to carry out single-point interpolation, for cases where just one half-hourly data point is missing:\nglobalVars.other.singlePointInterpolation = 'on'  % 'no_interp' - skip interpolation [default], 'on' - do single missing point interpolation for all traces\n\nAdding include files to your First Stage INI file\nIn sections 4.1 and 4.2, we introduced include files and how they are obtained. These template files define a standard set of variables that can be loaded into your first stage INI file. Specifically, the files relate to EddyPro output and radiation components. We suggest browsing through these files to become familiar with the content. If you wish to change or add any settings, we strongly recommend leaving the template “include” files untouched and using the global variables feature to create or overwrite the relevant setting, as previously described.\nMore information about include files:\n\nEddyPro_Common_FirstStage_include.ini: includes traces common to EddyPro output, listed in EddyPro file output order;\nEddyPro_LI7200_FirstStage_include.ini: when an LI-7200 IRGA is being used, this file adds LI-7200 specific traces, listed in EddyPro file output order;\nEddyPro_LI7500_FirstStage_include.ini: when an LI-7500 IRGA is being used, this file adds LI-7500 specific traces, listed in EddyPro file output order;\nEddyPro_LI7500_FirstStage_include.ini: when an LI-7500 IRGA is being used, this file adds LI-7500 specific traces, listed in EddyPro file output order;\nRAD_FirstStage_include.ini: includes radiation components.\n\nTo include any of these files, add the following line of code to the very bottom of your first stage INI file: #include &lt;relevant_file&gt;_include.ini. If you are using more than one, put them on multiple lines of code, still at the very bottom of the INI file, as follows:\n#include EddyPro_Common_FirstStage_include.ini\n#include EddyPro_LI7200_FirstStage_include.ini\n#include RAD_FirstStage_include.ini\nHow the global variables algorithm works:\n\nThe INI file trace parameters, i.e., all fields between [TRACE] and [END] in the INI file, are loaded up.\nThe program then cycles through all the traces: for each trace, first it checks if the instrumentType field matches one of the globalVars.Instrument fields, e.g., for instrumentType = 'LI7700' measuring “CH4”, if there is a global.Instrument.LI7700 field, then all its fields would be applied to the CH4 trace, either creating a new field or overwriting the existing fields; i.e., the content of global.Instrument.LI7700.instrumentSN would replace the content of the CH4 field instrumentSN. Secondly, it checks if the trace variableName matches any of the globalVars.Trace fields, e.g. Trace “CH4”.\nThe program continues reading the INI file and applies all the settings from the [TRACE]-[END] section.\nNext, it cycles through all the globalVars.Instrument fields and creates the fields or overwrites them if they already exist.\nFinally, it goes through all the globalVars.Trace fields and creates or overwrites the fields.\nThen the program moves to the next trace and repeats steps 2–5.\n\n\n\n\n\n\n\nAs outlined earlier on this page, the Evaluate feature can be used to implement Matlab code. This can be useful for removing outliers from your data in the first stage. For example, in the Biomet library there are some existing functions to help with this, such as:\n\nremove_spikes_diurnal_nonParametric.m\nrun_std_dev.m\n\nAlternatively, you could include your own code to do this, depending on what you need.\n\n\n\n\n\n\nSometimes we need to overwrite multiple properties for one or more traces that have already been created e.g. in an include file. The global variables feature allows this, however, once this section becomes long with many trace property tweaks, it can become very hard to troubleshoot, and in these cases having all the information for one trace together is more desirable.\nInstead of using global variables you can duplicate the full trace ([Trace] ... [End]), and in your site-specific first stage INI file, put this duplicate after the line of code where the include file is called that contains your original trace. Note the additional Overwrite property highlighted in yellow:\n\nFigure 5.3C. Location within site-specific INI file to put duplicate trace for overwriting a trace previously defined in an include INI file. In this case, we want to overwrite the CH4_MIXING_RATIO trace that was originally defined in EddyPro_LI7700_FirstStage_include.ini. Yellow highlighting shows the syntax for the “overwrite” property.\nThere are three overwrite options:\n\n0 = do not overwrite with this trace. This is the default setting. If you do not include the Overwrite parameter, the pipeline assumes this option. If you have a duplicate trace, you will get an error during cleaning (Example 1):\n\n1 = overwrite traces having the same variableName and also with Overwrite = 0 setting. This puts the duplicate data where the original data was, i.e., complete overwrite, first trace gone. Use this setting if you want your duplicate T2 available to use in a later variable such as T3, T4, T5, etc. (Example 2):\n\n2 = overwrite traces having the same variableName and with Overwrite = 0 setting. This takes advantage of the “position” of the duplicate trace. Use this setting if you want a later variable such as T3 or T4 available to use in T2.\n\n\n\n\n\n\n\n\nMore complex user-defined processing can be applied in the first stage to any trace using the very useful “Evaluate” and new “postEvaluate” options. Matlab functions (user-written or from Biomet.net) can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” or “postEvaluate” strings. They can be used to derive variables from available data, flag variables to remove bad data, or to calculate new useful variables. Here is an Evaluate example for removing outliers from a trace:\nEvaluate = 'wlen=24;thres=4;TA_1_1_1 = run_std_dev(TA_1_1_1,clean_tv,wlen,thres);'\nThe Evaluate property is executed for all traces before any other cleaning properties, e.g., minMax, calibration, etc.. In contrast, the newer postEvaluate property is executed in the first stage after all other cleaning is done.\nGenerally speaking, the order of operations is:\nEvaluate –&gt; other cleaning (e.g., minMax, calibration) –&gt; postEvaluate.\nThis is regardless of the order that they appear  within [Trace] ... [End]. The following series of examples shows how the Evaluate and postEvaluate statements work in relation to the other cleaning properties. For each example the input (Original) is an array of ones, and both the “Original” and first-stage “Clean” data are plotted in the result.\nExample 1: Only minMax and calibration, no Evaluate or postEvaluate statements\n\nResult: x = 2x [calibrated]\n\n\nExample 2: Evaluate, minMax and calibration, no postEvaluate statement\n\nResult: x = (x - 1)*2 [Evaluate first, then calibrate]\n\n\nExample 3: minMax and calibration, postEvaluate statement, no Evaluate statement\n\nResult: x = 2x - 1 [calibrate, then postEvaluate]\n\n\nExample 4: minMax and calibration, both Evaluate and postEvaluate statements\n\nResult: x = (x - 1)*2 - 1 [Evaluate, then calibrate, then postEvaluate]\n\n\n\n\n\n\n\n\n\nSome programming rules that you must follow for the first and second stage INI files to be successfully read by the pipeline scripts:\n\n1. We enforce using uppercase for site IDs to avoid problems with running data cleaning on Mac vs. Windows.\n2. All traces must be enclosed in [Trace] and [End] blocks.\n3. All assignments can be on multiple lines but should be enclosed in single quotes.\n4. Comments must begin with a percentage sign (%).\n5. All fields must be in Matlab format.\n6. All parameter assignments must be to strings in single quotes, or numeric expressions, e.g., threshold_const = 6, threshold_const = [6], variableName = 'Some Name'.\n7. For the first stage, the partial path must be included with the inputFileName when you locate the raw data trace in the database. (Using biomet_path function only returns the path: /year/SITEID/measType/)\n8. First stage necessary fields are: variableName, inputFileName, measurementType, units, title, and minMax.\n9. Second stage necessary fields are: variableName, title, units.\n\n\n\nTIPS\nOnce your INI file is complete, if desired you can use the simplify_FirstStageIni() function to clean it up. This may be helpful if you used a legacy INI file as an example; the function removes obsolete fields and makes things more concise.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.3 &nbsp; Full Documentation: First Stage INI files"
    ]
  },
  {
    "objectID": "PipelineDocumentation/PipelineDocumentation.html",
    "href": "PipelineDocumentation/PipelineDocumentation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Compiled by: Rosie Howard, 2024\nEdits and contributions: Zoran Nesic, Sara Knox, June Skeeter, Paul Moore, Karolina Bajda, and other EcoFlux Lab members and affiliates.\nRevisions:\n- June 2025: Further detailed “full” documentation added, all moved to lab website.\n- March 2025: Full documentation added including pipeline features and further information to supplement the tutorial.\n- February 2025: “Recently Added Features” section 7.2 added. Full documentation still in progress.\n- October 2024: These are the “quick-start” instructions for new users of the data-cleaning pipeline. The full documentation will be made available at a later date.\n\n\n\n\nRecently added pipeline features\nLink to Overview of Data Cleaning Pipeline slides presented during FLUXNET-CH4 V2.0 Workshop (on Day 1: Afternoon, and Day 2: Morning and Afternoon).\nLink to YouTube playlist of ALL presentations from FLUXNET-CH4 V2.0 Workshop.\n\n\n\n\n\nMotivation: The Importance of Flux Data Standardization and Reproducibility\n 1.1  Note on EddyPro Processing of High Frequency Data\nSoftware Installation\n 2.1  Install Software: Git, and Create Github account (optional)\n 2.2  Download Biomet.net Library\n 2.3  Install Software: Matlab\n 2.4  Configure Matlab for Biomet.net\n 2.5  Install Software: R/RStudio\n 2.6  Install Software: Python (optional)\nData Cleaning Principles\n\n\n\nQuick Start Tutorial - Recommended for First-Time Users\n 4.1  Quick Start Tutorial: Project Directory Structure and Matlab Configuration\n 4.2  Quick Start Tutorial: Create Database and Visualize\n 4.3  Quick Start Tutorial: Create First Stage INI File\n 4.4  Quick Start Tutorial: Create Second Stage INI File\n 4.5  Quick Start Tutorial: Third Stage and Ameriflux Output\nFull Documentation: Features, Details, and Other Useful Information for Advanced Users\n 5.1  Full Documentation: Project Directory Structure and Matlab Configuration\n 5.2  Full Documentation: Create Database and Visualize\n 5.3  Full Documentation: First Stage INI Files\n 5.4  Full Documentation: Second Stage INI Files\n 5.5  Full Documentation: Third Stage and Ameriflux Output\nData Visualization\n 6.1  Matlab plotApp\n 6.2  R-Shiny App\n 6.3  Other Biomet Plotting Tools\nTroubleshooting and FAQ\n 7.1  Recommended Software Versions\n 7.2  Recently Added Features",
    "crumbs": [
      "Pipeline Documentation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/PipelineDocumentation.html#data-cleaning-pipeline-documentation-and-instructions",
    "href": "PipelineDocumentation/PipelineDocumentation.html#data-cleaning-pipeline-documentation-and-instructions",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Compiled by: Rosie Howard, 2024\nEdits and contributions: Zoran Nesic, Sara Knox, June Skeeter, Paul Moore, Karolina Bajda, and other EcoFlux Lab members and affiliates.\nRevisions:\n- June 2025: Further detailed “full” documentation added, all moved to lab website.\n- March 2025: Full documentation added including pipeline features and further information to supplement the tutorial.\n- February 2025: “Recently Added Features” section 7.2 added. Full documentation still in progress.\n- October 2024: These are the “quick-start” instructions for new users of the data-cleaning pipeline. The full documentation will be made available at a later date.\n\n\n\n\nRecently added pipeline features\nLink to Overview of Data Cleaning Pipeline slides presented during FLUXNET-CH4 V2.0 Workshop (on Day 1: Afternoon, and Day 2: Morning and Afternoon).\nLink to YouTube playlist of ALL presentations from FLUXNET-CH4 V2.0 Workshop.\n\n\n\n\n\nMotivation: The Importance of Flux Data Standardization and Reproducibility\n 1.1  Note on EddyPro Processing of High Frequency Data\nSoftware Installation\n 2.1  Install Software: Git, and Create Github account (optional)\n 2.2  Download Biomet.net Library\n 2.3  Install Software: Matlab\n 2.4  Configure Matlab for Biomet.net\n 2.5  Install Software: R/RStudio\n 2.6  Install Software: Python (optional)\nData Cleaning Principles\n\n\n\nQuick Start Tutorial - Recommended for First-Time Users\n 4.1  Quick Start Tutorial: Project Directory Structure and Matlab Configuration\n 4.2  Quick Start Tutorial: Create Database and Visualize\n 4.3  Quick Start Tutorial: Create First Stage INI File\n 4.4  Quick Start Tutorial: Create Second Stage INI File\n 4.5  Quick Start Tutorial: Third Stage and Ameriflux Output\nFull Documentation: Features, Details, and Other Useful Information for Advanced Users\n 5.1  Full Documentation: Project Directory Structure and Matlab Configuration\n 5.2  Full Documentation: Create Database and Visualize\n 5.3  Full Documentation: First Stage INI Files\n 5.4  Full Documentation: Second Stage INI Files\n 5.5  Full Documentation: Third Stage and Ameriflux Output\nData Visualization\n 6.1  Matlab plotApp\n 6.2  R-Shiny App\n 6.3  Other Biomet Plotting Tools\nTroubleshooting and FAQ\n 7.1  Recommended Software Versions\n 7.2  Recently Added Features",
    "crumbs": [
      "Pipeline Documentation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_4_Quick_Start_Create_Second_Stage_INI_File.html",
    "href": "PipelineDocumentation/4_4_Quick_Start_Create_Second_Stage_INI_File.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to create your second stage INI file, in order to carry out the second stage data cleaning. The following steps assume that you have already created a first stage INI file and run first stage cleaning for your flux site.\nOn this page:\n\nInstructions to create second stage INI file\nQuick summary of second stage features\n\n\n\n\n\n\n\nOpen your site-specific SITEID1_SecondStage.ini for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better) and your site ID.\nThe TEMPLATE_SecondStage.ini file that you downloaded and copied already includes a set of fundamental climate variables and the bare minimum flux variables needed to run the third stage. We advise testing the file with only these variables first. Note that even if you do not change the content of any traces during the second stage, you must still include any trace that you wish to be available for the third stage (and beyond, e.g., Ameriflux submission), i.e., they must be passed along the pipeline.\nTest the second stage data cleaning in Matlab, as follows:\n fr_automated_cleaning(yearIn,'SITEID1',2)\nNote the third and final input parameter is 2 instead of 1, denoting second stage. You should see a new Clean folder directly under the site ID directory in your Database, for whichever year(s) you ran it for. Within that Clean folder, there will be a SecondStage folder (figure 4.4A).\n\nFigure 4.4A. Directory tree showing location of data output (in this example, variable1, variable2, …) from second stage cleaning.\nYou can add any other relevant variables into the second stage INI file. Again, use the visualization tools to check your data looks as expected, then you are ready to progress to third stage cleaning.\n\n\n\n\n\n\n\n\nCombine multiple traces into one using the calc_avg_trace function. These traces must already have been defined in the first stage.\nUse the “Evaluate” option to operate on your input traces.\n\nExample: the TA_1_1_1 trace you defined in the first stage has some missing data and there is a nearby reliable climate station with a long record. In the first stage you also defined (and filtered etc.) data from this climate station in a variable called TA_OTHER_SOURCE. Then in the second stage, you can use calc_avg_trace within the Evaluate parameter, as follows:\nEvaluate = 'TA_1_1_1 = calc_avg_trace(clean_tv,TA_1_1_1,TA_OTHER_SOURCE,-1)';\nThe function regresses the two variables provided and uses the resulting linear fit to fill the gap(s) in the first variable. Note that fluxes are gap-filled in stage three.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.4 &nbsp; Quick Start Tutorial: Create Your Second Stage INI File for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_4_Quick_Start_Create_Second_Stage_INI_File.html#quick-start-tutorial-create-your-second-stage-ini-file-for-data-cleaning",
    "href": "PipelineDocumentation/4_4_Quick_Start_Create_Second_Stage_INI_File.html#quick-start-tutorial-create-your-second-stage-ini-file-for-data-cleaning",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section will show you how to create your second stage INI file, in order to carry out the second stage data cleaning. The following steps assume that you have already created a first stage INI file and run first stage cleaning for your flux site.\nOn this page:\n\nInstructions to create second stage INI file\nQuick summary of second stage features\n\n\n\n\n\n\n\nOpen your site-specific SITEID1_SecondStage.ini for editing.\nNear the top of the file, insert the full name of your site (more descriptive is better) and your site ID.\nThe TEMPLATE_SecondStage.ini file that you downloaded and copied already includes a set of fundamental climate variables and the bare minimum flux variables needed to run the third stage. We advise testing the file with only these variables first. Note that even if you do not change the content of any traces during the second stage, you must still include any trace that you wish to be available for the third stage (and beyond, e.g., Ameriflux submission), i.e., they must be passed along the pipeline.\nTest the second stage data cleaning in Matlab, as follows:\n fr_automated_cleaning(yearIn,'SITEID1',2)\nNote the third and final input parameter is 2 instead of 1, denoting second stage. You should see a new Clean folder directly under the site ID directory in your Database, for whichever year(s) you ran it for. Within that Clean folder, there will be a SecondStage folder (figure 4.4A).\n\nFigure 4.4A. Directory tree showing location of data output (in this example, variable1, variable2, …) from second stage cleaning.\nYou can add any other relevant variables into the second stage INI file. Again, use the visualization tools to check your data looks as expected, then you are ready to progress to third stage cleaning.\n\n\n\n\n\n\n\n\nCombine multiple traces into one using the calc_avg_trace function. These traces must already have been defined in the first stage.\nUse the “Evaluate” option to operate on your input traces.\n\nExample: the TA_1_1_1 trace you defined in the first stage has some missing data and there is a nearby reliable climate station with a long record. In the first stage you also defined (and filtered etc.) data from this climate station in a variable called TA_OTHER_SOURCE. Then in the second stage, you can use calc_avg_trace within the Evaluate parameter, as follows:\nEvaluate = 'TA_1_1_1 = calc_avg_trace(clean_tv,TA_1_1_1,TA_OTHER_SOURCE,-1)';\nThe function regresses the two variables provided and uses the resulting linear fit to fill the gap(s) in the first variable. Note that fluxes are gap-filled in stage three.",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.4 &nbsp; Quick Start Tutorial: Create Your Second Stage INI File for Data Cleaning"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_SoftwareInstallation.html",
    "href": "PipelineDocumentation/2_SoftwareInstallation.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides instructions on installing the software and configuring the libraries needed for running the data-cleaning pipeline.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_SoftwareInstallation.html#software-installation",
    "href": "PipelineDocumentation/2_SoftwareInstallation.html#software-installation",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides instructions on installing the software and configuring the libraries needed for running the data-cleaning pipeline.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation"
    ]
  },
  {
    "objectID": "Pubs_list.html#theses",
    "href": "Pubs_list.html#theses",
    "title": "Publications",
    "section": "Theses",
    "text": "Theses"
  },
  {
    "objectID": "Pubs_list.html#research-talks-poster-presentations",
    "href": "Pubs_list.html#research-talks-poster-presentations",
    "title": "Publications",
    "section": "Research Talks & Poster Presentations",
    "text": "Research Talks & Poster Presentations"
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html",
    "href": "Documentation/Fieldwork/Cleaning.html",
    "title": "Cleaning Procedures",
    "section": "",
    "text": "This page details the steps for cleaning the 7700 and 7200. Always do the 7700 first, followed by the 7200. You must clean the sensors before calibration.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#getting-started",
    "href": "Documentation/Fieldwork/Cleaning.html#getting-started",
    "title": "Cleaning Procedures",
    "section": "Getting Started",
    "text": "Getting Started\n\nConnect the laptop to the site system using the ethernet cable and dongle to the top box on the outside of the scaffolding.\n\nNOTE: remote connection via vinimet & mobile hotspot can also work in a pinch.\n\n\nTO DO: Add pics of logger box at each each site for reference.\n\nRetract the boom to access the flux sensors. Loosen the two bolts securing the boom &gt; pull quick release tab&gt; slide boom in &gt; Be mindful of cables\n\nFor BB1 & DSM: place stepladder on plywood on the ground. Make sure it’s solid.\nFor BB2 & RBM: loosen the wing nut &gt; remove the fastening boot &gt; lift up sensors to reduce force needed to pull out the bolt &gt; rotate the arm to bring the sensors within reach from platform. Be careful this can be quite difficult.\n\n\n\n\nBB1 & DSM Cleaning\n\n\n\n\n\n\nBB2 & RBM Cleaning\n\n\n\n\n\n\n\n\nDo a visual inspection of the sensors and note/address any obvious issues.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7700",
    "href": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7700",
    "title": "Cleaning Procedures",
    "section": "Cleaning the LI 7700",
    "text": "Cleaning the LI 7700\n\nOn the laptop, open the program ‘LI-7700’. Click “Connect” &gt;&gt; Ethernet &gt;&gt; Select Instrument &gt;&gt; Connect. Open 2nd data page and look at the single graph with signal strength\n\nTO DO: Add Screenshot\n\nWipe top and bottom windows with windshield cleaner (wet then dry cloth) until signal strength is sufficient:\n\n\n\n\n\n\nTO DO: Replace w/ another (different) image on cleaning mirrors?\n\nDesired LI 7700 Signal Strength by Site\n\n\n\nSite\nValue\n\n\n\n\nBB1\nHigh 50’s\n\n\nBB2\nAround 80\n\n\nDMS\nAround 70\n\n\nRBM\n…\\\n\n\n\n\nDisconnect from the 7700 and close the LI-7700 program.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7200",
    "href": "Documentation/Fieldwork/Cleaning.html#cleaning-the-li-7200",
    "title": "Cleaning Procedures",
    "section": "Cleaning the LI 7200",
    "text": "Cleaning the LI 7200\n\nPower down the flux system. Inside the power box, flip the 3 switches as shown in the table below. The power is off when the red light is on.\n\n\nEC Sensor Power Channels\n\n\n\nSite\nLI-7200 flow module (pump)\nLI-7700\nLI-7550\n\n\n\n\nBB1\n12\n7\n2\n\n\nBB2\n3\n4\n5\n\n\nDMS\n3\n4\n5\n\n\nRBM\nTBD\nTBD\nTBD\n\n\n\n\nTake the intake tube off by loosening the nut with a wrench. The tube can then just hang loosely from the tower, disconnected from the 7200. Then loosen the knobs on top of 7200 to release the top and take side part off to open the 7200 sensor head. This can can hang next to the instrument while cleaning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWipe top and bottom windows with windshield cleaner (wet then dry cloth). Then clean intake filter with a wipe.\n\nYou may need to replace with a spare clean cap intake. If so: remove the foil wrap, loosen the metal ring with pliers and push it all the way to the tube, remove and replace the cap, use pliers to put the metal ring back on the cap\n\nPower system back up using switches listed in the table above.\n\nNOTE: The start up sequence is: 1) LI-7200 pump, 2) LI-7700, 3) LI-7550. Allow 90 seconds between each sensor when powering up. This order is important for proper start up.\nInside the Li-7550 box, the USB logging light should be flashing\n\nOn the laptop open the LI 7x00 program and connect the the 7500 to check that the signal strength is sufficient:\n\n\n\nDesired LI 7200 Signal Strength by Site\n\n\n\nSite\nValue\n\n\n\n\nBB1\nAbout 100\n\n\nBB2\nAbout 103\n\n\nDMS\nAbout 102\n\n\nRBM\n…",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Cleaning procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DataEntry.html",
    "href": "Documentation/Fieldwork/DataEntry.html",
    "title": "Data Entry",
    "section": "",
    "text": "This page features links to various spots to input the hand-collected data when out in the field (e.g., Water Table Depths). The sheets are all hosted on the Lab’s Google Drive under the Project’s tab.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Data entry"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DataEntry.html#sites",
    "href": "Documentation/Fieldwork/DataEntry.html#sites",
    "title": "Data Entry",
    "section": "Sites",
    "text": "Sites\n\nBurns Bog 1\n\n\n\n\n\n\n\nWater Table Height\n\nAnother Link?\n\n\n\n\nBurns Bog 2\n\n\n\n\n\n\n\nWater Table Height\n\nAnother Link?",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Data entry"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html",
    "href": "Documentation/Fieldwork/DailyMonitoring.html",
    "title": "Daily Site Monitoring",
    "section": "",
    "text": "Below are instructions for visual checks of the data that should be done every day. Web plots for each site can be found by going to:",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html#check-critical-variables",
    "href": "Documentation/Fieldwork/DailyMonitoring.html#check-critical-variables",
    "title": "Daily Site Monitoring",
    "section": "Check Critical Variables",
    "text": "Check Critical Variables\n\nMotor Power (%) & Flow Fate for LI-7200 pump\n\nKeep checking if this one changes over time (over weeks).\n\nFlow drive % should be 50 - 90%\nHigher power means more air-flow resistance in the tubing indicating either clogged up filter (if we use those), dirty rain cap wire-mesh or damaged tubing\n\nWe aim for 30-min averages flow rate of slpm = 15 L/min;\n\nBelow 13 L/min we begin to worry, (data quality drops);\nBelow 10 L/min we need to fix it ASAP (data quality questionable or data missing) by cleaning up the intake tube cap\n\n\n\n\n\nGood Flow at BB1\n\n\n\n\n\nBad Flow at BB2\n\n\n\n\nThermocouples\n\nLI-7200: these two values should be very close to each other (&lt;1 C difference)\n\nIn (t_in_LI_7200)\nOut (t_out_LI_7200)\n\nLI-7700\n\nAir temperature\n\n\n\n\nClimate Data\n\nCheck if there are new data & if the values are reasonable (for all sites)\n\nClick on “last day” / “last week” to filter the time series\nIf you see BB1 and BB2 precip data don’t align with each other, there’s a possibility that the tipping bucket is clogged.\n\n\n\n\nFlux Data\nFor Fluxes and Gas Concentrations check if there are new data & if the values are reasonable. These will only be calculated if smart flux is running properly.\nGas Concentrations:\n\nCO2 mixing ratio (dry) - during daytime should be around 380-420 ppm (umol/mol), during night time it can be quite high depending on the site (sometimes &gt; 600 ppm)\nCH4 mixing ratio (dry) - should be around 2 ppm\n\nFluxes:\n\nSensible & Latent heat - Typically between -100 and + 450 w m-2\n\nShould generally sum ~ 70%-80% of net radiation\n\nFCO2 & FCH4 - site dependent, but look for gaps, extreme spikes, systematic jumps\n\nSignal strength\n\nView over a longer period to check for any trends. Schedule a site visit if it’s decreasing rapidly (&lt;10%). It’s normal for 7700’s signal strength to drop during rainy periods, but it’s not normal for 7200’s to drop that frequently (if it does, check the 7200 head’s o-ring).\n\nVoltage\n\nLook at data over the past month and tell June or Rick if the battery voltage goes below 24 V at midnight.\n\nThis usually happens in the winter when we have a few days of clouds & rain.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/DailyMonitoring.html#remote-login-procedures",
    "href": "Documentation/Fieldwork/DailyMonitoring.html#remote-login-procedures",
    "title": "Daily Site Monitoring",
    "section": "Remote Login Procedures",
    "text": "Remote Login Procedures\nConnect to VPN, go to Remote Desktop & connect to vinimet.geog.ubc.ca (or it’s better to do this from your personal computer so you won’t kick off or get kicked off by other people using Vinimet). Note that doing this on your personal computer requires a PC.\n\nLI 7200\nOpen the LI-7x00 software on desktop and connect using the right IP Address\n\n\n\nConnecting to the BB1 7200\n\n\n\nOnce on the main page check:\n\nThe logger status → should be logging\nUSB Free Space → make sure there’s still enough space in the USB (usually the USB stick can handle 6-weeks data)\nThe SmartFlux module has to be connected to the system, make sure it’s not “none”\nHead pressure - should be between -0.8 to -3.8 kPa\nFlow drive - should be around 50% to 90%\nFlow rate - should be around 13-15 l/m\n\n\n\n\n\nChecking the BB1 7200\n\n\nClick on the “Diagnostics” tab - check if all parameters are “OK”\n\n\n\nChecking the BB1 7200 Diagnostics\n\n\n\n\nLI 7700\nOpen the LI-7700 software on desktop and use the same IP address as for the 7200\n\n\n\nConnecting to the BB1 7200\n\n\nCheck 7700 Optics RH. If it’s &gt;15%, we should replace the internal chemicals.\n\n\n\nChecking the BB1 7700\n\n\n\nCheck SmartFlux internal memory every 2-3 months\n\nInstructions to check memory are under Note_SmartFlux internal memory_ssh connection.docx\nInstructions to clear memory are under Note_SmartFlux internal memory_updater.docx\n\n\nInfo from here",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Daily site monitoring"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html",
    "href": "Documentation/LabInfo.html",
    "title": "Information for Current Lab Members",
    "section": "",
    "text": "This page provides information and links to documentation that are relevant to current members of the lab",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#information-for-new-lab-members",
    "href": "Documentation/LabInfo.html#information-for-new-lab-members",
    "title": "Information for Current Lab Members",
    "section": "Information for new lab members",
    "text": "Information for new lab members\n\nAdd your info to the ‘Lab contact information’ doc in Google Drive\nReview the Grad Expectations document \nCoordinate a day/time for your bi-weekly lab meeting with Sara\nMake sure you’ve been added to the Google Drive account and Slack Workspace\nSend bio and picture to Sara so that she can add you to the lab website\nReview program requirements (see below). More information on potential courses can be found here.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#program-requirements",
    "href": "Documentation/LabInfo.html#program-requirements",
    "title": "Information for Current Lab Members",
    "section": "Program requirements",
    "text": "Program requirements\n\nPhD (typical timeline)\n\nFall term Year 1\n\nComplete memo of expectations  in September.\nCourses\nGEOS 500 - develop literature Review and preliminary proposal\n\nSpring term Year 1\n\nCourses\nWork with supervisor to refine preliminary proposal/research gap. Identify committee members\nComplete spring review - present preliminary proposal, assess progress, discuss timeline for comprehensive exams, set tentative topics, confirm committee, confirm any further coursework\n\nSummer Year 1\n\nFieldwork/research\nFormalise comps reading lists and topics. More details on the comps policy can be found here.\n\nFall term Year 2\n\nComplete memo of expectations  in September.\nFieldwork/research\nPrepare and distribute proposal\n\nSpring term Year 2\n\nFieldwork/research\nComprehensive Exam (written exam followed by oral examination two weeks later)\n\nComplete annual review by end of spring/early summer\n\n\nSummer Year 2\n\nFieldwork/research\nFormal meeting to approve Proposal (or previous spring)\n\nFall term Year 3\n\nComplete memo of expectations  in September.\nFieldwork/research\n\nSpring term Year 3\n\nComplete memo of expectations by end of May\nComplete annual review by end of spring/early summer\nFieldwork/research\n\nSummer term Year 3\n\nFieldwork/research\n\nFall term Year 4\n\nComplete memo of expectations  in September.\nResearch\nCheck doctoral deadlines to plan out timing of defense\n\nBe mindful of the time required to complete the examination process\nThe External Examiner nomination form needs to be submitted ~5 months before the expected defense date. Nomination is the supervisor’s responsibility. Candidates should keep tabs on the process to ensure timely submission, but are not allowed to know who the examiner is.\n\n\nSpring term Year 4\n\nComplete annual review by end of spring/early summer\nResearch\nShare thesis with Supervisory Committee\n\nCommittee may take a while read/approve document\nIncorporate changes/edits\n\nNominate University Examiners - Supervisor’s responsibility, but candidate should keep tabs to ensure the\nSubmit thesis to GPS for external examination\n\nPlan for a defense 6-8 weeks after submitting\n\nShould anticipate that the examiner will take 4-6 weeks to review the thesis.\nThe earliest possible defense date is 1 week after the external report is submitted.\n\nSummer term Year 4\n\nOral exam\n\nNote: Exam must be scheduled at least 4 weeks in advance. *Make necessary revisions/edits\nDue within 1 month of defense\n\n\n\n\n\nMSc (typical timeline)\n\nFall term Year 1\n\nComplete memo of expectations  in September.\nCourses\nGEOS 500 - develop literature Review and preliminary proposal\n\nSpring term Year 1\n\nCourses\nWork with supervisor to refine preliminary proposal/research gap. Identify committee members\nComplete spring review - present preliminary proposal, assess progress\n\nSummer Year 1\n\nFieldwork/research\n\nFall term Year 2\n\nComplete memo of expectations  in September.\nFieldwork/research\n\nSpring term Year 2\n\nFieldwork/research\nComplete annual review by end of spring/early summer\nPresent thesis at Spring Symposium\n\nSummer Year 2\n\nThesis defense & submit final thesis (guidelines for the thesis can be found here). And see ‘msc-thesis-defence-revised.pdf’ doc for details.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/LabInfo.html#leaving-the-lab-checklist",
    "href": "Documentation/LabInfo.html#leaving-the-lab-checklist",
    "title": "Information for Current Lab Members",
    "section": "Leaving the lab checklist",
    "text": "Leaving the lab checklist\nWe’re sad to see you go, but we’re excited for your next adventure! However, before you leave, make sure to:\n\nUpload your code to github (or share it with Sara directly). Make sure to follow the guidelines for reproducible data flow so that we can run all your code after you leave!\nMake sure to share all relevant data with Sara. Ideally this would be in the data folder in each of your project/chapter folders.\nReturn all keys, equipment, field gear that belong to be lab/university.",
    "crumbs": [
      "Documentation",
      "Information for Current Lab Members"
    ]
  },
  {
    "objectID": "Documentation/DataWorkflows.html",
    "href": "Documentation/DataWorkflows.html",
    "title": "Data Pipeline",
    "section": "",
    "text": "Data flow schematic\nMicromet Data Flow (currently for BB1, BB2, DSM, and RBM)  Note: Manitoba sites have SmartFlux stream only\n\n\n\n\n\n\n\nDrive/Folder structure on vinimet\n\nP:\\Sites\\Site (data as received from the sites)\n              \\Met\n                  \\5min (for BB only)\n              \\Flux\n              \\HighFrequencyData\nP:\\Database (everything in read_bor format)\n            \\yyyy\\Sites\n                       \\Met (straight from CRBasic -&gt; read_bor)\n                           \\Clean (First stage clean)\n                       \\Flux (from EddyPro summary, site files -&gt; read_bor & then overwrite with EddyPro recalculated output) \n                           \\Clean (First stage clean)\n                       \\Clean\\SecondStage \n                             \\ThirdStage\n                             \\Ameriflux\n                             \\ThirdStage_REddyProc_RF_Fast (simple uncertainty analysis)\n                             \\ThirdStage_REddyProc_RF_Full (full uncertainty analysis)\nP:\\Micromet_web (note that this mirrors what’s on host remote.geog.ubc.ca)\n                \\www \n                    \\data (html & javascript for site level web plotting)\n                    \\webdata\\resources\\csv (csv files for web plotting)\nGoogle drive contains:\n\nPictures (to remove and host on google photos)\nDocuments\n\n\n\n\nData stream notes\nThe data processing is run from LoggerNet Task manager through batch files stored under c:\\UBC_Flux\\BiometFTPsite. The batch files are named: Site_automatic_processing.bat. The generic Matlab scripts that’s called from these batch files is: run_BB_db_update([],{‘Site’},0)\nMore Details\n\nMET Files\n\nEvery 4 hrs (e.g., BB1 4:20, BB2 4:10) Loggernet Task Master downloads MET data to P:\\Sites\\Site\\Met\\Site_MET.dat (or Site_biomet.dat)\n\nNote Site is used generically to refer to each site (e.g., BB, BB2, DSM, RBM, Hogg, Young)\nOnce per day 1:10 am\n\n‘Rename files for all sites’ - Called from Loggernet Task Master\n\nRenames Site_MET.dat to Site_MET.yyyyddd\n-nosplash /r “renam_csi_dat_files”('P:Sites\\Site\\Met');…for each site; exit;”\n\nSite_Process_CSI_to_database - Called from Loggernet Task Master\n\n-nosplash -minimize /r “run_BB_db_update([],{‘Site’});”\n  function run_BB_db_update(yearln)\n    dv = datevec(now);\n    arg_defualt('yearln', dv(1));\n    sites = {'Site'};\n    db_update_BB_site(yearln, sites);\n    exit\n\ndb_update_BB_site\n\nUpdate Site matlab database with new logger files to P:\\Database\\yyyy\\Site\\Met\nCall C:\\UBC_PC_Setup\\PC_specific\\BB_webupdate to create .csv files used by webplots -&gt; `P:_web\nCall C:\\Ubc_flux\\BiometFTPsite\\BB_Web_Update.bat to move csv files to webserver\n\n\nLocation of data\n\nP: - local drive on vinimet\nP:\\Micromet_web\\www\\webdata\\resources\\csv - location of CSV files\nP:\\Micromet_web\\www\\data - local versions of js, html files\nP:\\Sites\\Sites - raw data\n\n\n\n\nWeb-plotting pipeline\nFirst, go from the bottom of the chart in the first page upwards, find out which step stopped working then use the followiwng instructions to troubleshoot.\nEC Data\n\nSmartFlux files (*_EP-Summary.txt) arrive in the folders as below: \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB\\Flux \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB2\\Flux\n\ninsert image \n\nTroubleshooting for Step 1:\n\nIf there is NO recent data files (.zip or .txt) at all, then:\n\nCheck if you can still connect to LI-7200 to determine if it is an Internet issue or not.\nCheck if the LoggerNet task for data downloads failed. When all tasks are executed properly, the status of the tasks should be shown like this:\n\ninsert image \n\nCheck if the SmartFlux system at site skips processing or not by using WinSCP connecting to it (e.g., BB_site or BB2_site). The second figure below shows the BB2 SmartFlux have skipped processing of Feb. 1 (there is no summary file for that day):\n\ninsert images \n\n\n\nIf SmartFlux files arrive, the task “XX_automatic_processing” (as shown in the figure above) that runs Matlab program will process all data (CSI and SmartFlux) and update the web files too.\n\nTroubleshooting for Step 2:\n\nIf you suspect that this task failed, then:\n\nCheck the dates of the last updated web csv files (\\\\VINIMET.GEOG.UBC.CA\\Micromet_web\\www\\webdata\\resources\\csv).\nCheck a few flux-related csv files to see if the last datestamps are less than 6 hours old.\nIf the files are NOT up-to-date, manually rerun the Matlab program by using run_BB_db_update([]{'BB2},0)\n\nIf you confirm the task is working fine or you’ve re-run the Matlab program, then:\n\nCheck if those files were uploaded to the web server by starting WinSCP to connect to ‘remote.geog.ubc.ca’ and see if the files are there.\n\n\n\n\nMet Data\nData from dataloggers goes to \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB\\Met\\ or \\\\vinimet.geog.ubc.ca\\Projects\\Sites\\BB2\\Met\\.\n\nTroubleshooting:\n\nCheck if the files have arrived or not.\nCheck the LoggerNet task for data downloads.\nCheck connection to the logger\nCheck the data of the last updates Biomet database files (\\\\VINIMET.GEOG.UBC.CA\\Database)\nCheck a few MET-related csv files to see if the last datestamps are less than 6 hours old.\nManually re-run the task in Matlab by using run_BB_db_update([]{'BB2},0).\nCheck if those csv files were uploaded to the web server.\n\n\nContinue adding info from here\n\n\nOther Documentation & Tasks\nThe following documents in the General procedures, settings and protocols folder in Google Drive may also be of use:\n\nUpdating webplots → Micromet web plotting.docx\nDocuments for dealing with full internal memory for SmartFlux → Note_SmartFlux internal memory_ssh connection.docx & Note_SmartFlux internal memory_updater.docx\n7200 lab calibration → Note_LI-7200 Lab calibration procedure.docx\nGas tank calibration → Report_Gas tank calibration.docx\n\nNOTE: Note that the troubleshooting reports folder contains documentation on troubleshooting that is relevant to all sites.\n\n\nCreating a local copy of the database\nTo create a local copy of the Micromet Lab database or create your own database following the Micromet Lab database structure, following the instructions provided here",
    "crumbs": [
      "Documentation",
      "Data pipeline"
    ]
  },
  {
    "objectID": "Research.html#overview",
    "href": "Research.html#overview",
    "title": "Research",
    "section": "Overview",
    "text": "Overview\nThe EcoFlux lab focuses on measuring and modelling greenhouse gas, water, and energy fluxes across a range of spatial and temporal scales. We combine field-based measurements, remote sensing, and modelling to investigate land-atmosphere interactions in our rapidly changing world, with a current emphasis on wetland ecosystems.\nWetlands provide numerous benefits. Among the numerous ecosystem services provided by wetlands climate regulation is identified as one of their most important benefits to society. Wetland ecosystems play an important role in the global carbon cycle; they provide the ideal environment for long-term storage of atmospheric carbon dioxide, yet they are also the largest single source of methane. Climate change has the potential to increase greenhouse gas (GHG) emissions from wetlands, however, the consequences of rising temperatures on wetland GHG exchange remains uncertain. Furthermore, preventing further wetland loss and restoring wetland ecosystems has been identified by researchers and governments as important in limiting future emissions to help meet climate goals. The EcoFlux Lab takes an interdisciplinary approach to provide a better understanding of how wetland responses to climate variability and restoration can feedback to slow or accelerate future climate change. Our work combines state-of-the-art field-based measurements, remote sensing, and modelling to provide new insights into the controls of wetland GHG fluxes across a range of spatial and temporal scales and quantify the potential climate benefits of wetland restoration and conservation. This research is key to better predicting current and future contributions of wetlands to climate change, which is highly relevant for policies aiming to limit the level of global temperature rise.",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Research.html#current-ecoflux-research-projects",
    "href": "Research.html#current-ecoflux-research-projects",
    "title": "Research",
    "section": "Current EcoFlux Research Projects",
    "text": "Current EcoFlux Research Projects\n\nCarbon Fluxes in Restored Wetlands\n\nIn collaboration with several other research groups at UBC, my lab’s research is focused on measuring GHG, water, and energy fluxes over Burns Bog, a restored peatland in Metro Vancouver. This site is a raised domed peat bog that is undergoing re-wetting as a restoration management strategy following peat harvesting and associated drainage. While restoration can help recover important ecosystems services provided by wetlands, it can also affect the exchange of greenhouse gases, water and energy between the surface and the atmosphere.\n\nBy conducting year-round eddy covariance measurements, we are assessing the biogeochemical and biophysical impacts of peatland restoration, and the impacts of forest fire smoke on carbon cycling and energy fluxes in restored peatlands. This research is also being done in collaboration with researchers and staff at Metro Vancouver who are interested in quantifying the climate mitigation potential of wetland restoration and conservation. \nCollaborators:  UBC Ecohydrology Research Group  UBC Biometeorology and Soil Physics Group  Dan Moore’s Research Group, UBC Geography \nNews on Burns Bog:  April 20, 2023 - Up in smoke: Human activities are fuelling wildfires that burn essential carbon-sequestering peatlands  June 16, 2021 - David Suzuki: For climate’s sake, save the peat!  June 29, 2020 - Carbon Neutral Achievement Sets Stage for Bold Climate Action \n\n\nBlue Carbon Research\n\nBlue carbon is the carbon sequestered and stored in coastal ecosystems including tidal marshes, mangroves, and seagrasses. Coastal ecosystems are among the strongest carbon sinks in the biosphere. This coupled with their potential for low methane emissions, has generated widespread interest in these ecosystems for climate change mitigation and adaptation. However, measuring and modelling carbon exchanges in tidal wetlands presents unique challenges due to highly dynamic atmospheric and hydrological fluxes, as well as sensitivities to both terrestrial and marine influences. Through iGROW, my research group recently installed two eddy covariance flux towers (CA-DSM and CA-RBM) in tidal wetlands along the Pacific Coast of Canada to quantify and model the net carbon and GHG balance of these marshes, which represent an important data gap in ecosystem-scale measurements of GHG exchange from tidal marshes. In partnership West Coast Environmental Law, we are also working to translate this blue carbon science into policy-relevant information for municipalities and provincial governments. \n\nOur research group is also involved in the NSF funded Coastal Carbon Research Coordination Network. Specifically, we are involved in the Methane Working Group, which aims to compile all methane flux data from coastal habitats (excluding mangroves) in the CONUS to parameterize and validate a set of nested process-based methane models. \nAdditionally, we are part of a U.S. Department of Energy grant “High-frequency Data Integration for Landscape Model Calibration of Carbon Fluxes Across Diverse Tidal Marshes” focused on leveraging eddy covariance observations to improve landscape-scale models of carbon fluxes across tidal marshes. This grant brings together university and government researchers from across the U.S. \nI am also a co-investigator on a collaborative research effort to model the current and future mitigation capacity of Canada’s blue carbon ecosystems, which was recently funded by an NSERC Alliance grant. The project is led by Julia Baum (University of Victoria), and includes four co-principal investigators and seventeen collaborators from across the country which are contributing to this national natural ocean climate solutions research effort. The project brings together a consortium of four universities, three government agencies, and four eNGOs. \nPrimary Collaborators:  Oikawa Lab, California State University, East Bay  The U.S. Geological Survey  The Baum Lab, University of Victoria  O’Connor Lab, The University of British Columbia \n\n\nCarbon Cycling in the Prairie Pothole Region of Canada\n\nWith growing interest in wetland management and restoration as a Natural Climate Solution, improved estimates of wetland carbon sequestration and GHG fluxes across Canadian wetland types are strongly needed. In partnership with Ducks Unlimited Canada, we focus on wetlands in the Prairie Pothole Region of western Canada since these ecosystems are understudied relative to other wetland types in Canada, yet they play important roles in carbon cycling and climate regulation. Specifically, this study leverages existing funding and infrastructure to investigate carbon and GHG dynamics across two wetland sites, with the aims of measuring and modeling GHG fluxes in prairie wetlands in Manitoba.\nPrimary Collaborators:  Ducks Unlimited Canada - Pascal Badiou \nPrairie Wetlands in the News:  May 19, 2022 - Project intended to demonstrate wetlands’ greenhouse gas impact  April 20, 2022 - DUC analyzing wetlands on farms and ranches for carbon capture  April, 2022 - DUC Carbon Tower Project",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Research.html#past-ecoflux-research-projects",
    "href": "Research.html#past-ecoflux-research-projects",
    "title": "Research",
    "section": "Past EcoFlux Research Projects",
    "text": "Past EcoFlux Research Projects\n\nFLUXNET-CH4 – a global database of eddy covariance CH4 flux measurements and wetland synthesis for CH4\nNatural wetlands emit approximately 30% of global CH4 emissions, as their waterlogged soils create ideal conditions for CH4 production. They are also the largest, and most uncertain, natural source of CH4 to the atmosphere. Direct observations of local CH4 emissions with high measurement frequency are important for constraining CH4 budgets, for understanding the responses of CH4 fluxes to environmental factors and climate, and for creating validation datasets for the land-surface models used to infer global CH4 budgets. However, unlike well-coordinated efforts for synthesizing CO2 flux-tower observations (e.g., FLUXNET), no such network and data synthesis effort existed previously for CH4. \n\nFLUXNET-CH4 is an initiative coordinated through the Global Carbon Project in close partnership with AmeriFlux and EuroFlux, to compile a global database of eddy covariance CH4 flux measurements to answer regional and global questions related to the global CH4 cycle. Through this activity, we coordinated the collection, aggregation, standardization, and post-processing of global CH4 data from the flux tower community. FLUXNET-CH4 Version 1.0 includes data from 81 sites, representing freshwater, coastal, upland, natural, and managed ecosystems. \nAdditionally, through a USGS Powell Center Working Group, we leveraged the FLUXNET-CH4 to provide novel insights into the controls and timing of wetland CH4 emissions for North America and globally, inform and validate biogeochemical models, and upscale wetland CH4 flux measurements globally. \nCollaborators:  Jackson Lab, Stanford University  NASA Goddard  Global Carbon Project  Stanford Machine Learning Group \n\n\nQuantifying carbon benefits of tidal wetland restoration in the Delta: Decision support using a robust, integrated and data-driven model\n\nThis research was funded through the California Department of Fish and Wildlife and focused on advancing remote sensing data in tidally flooded marshes and using the data to improve modeling of greenhouse gases in these systems. The modeling tool will be made publicly available through Google Earth Engine. This research was a collaboration between UBC, California State University, East Bay, UC Berkeley, and the U.S. Geological Survey.\nCollaborators:  Oikawa Lab, California State University, East Bay  The U.S. Geological Survey  The Landscape Research Group, UC Berkeley",
    "crumbs": [
      "Home",
      "Research"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html",
    "href": "Documentation/UsingGit.html",
    "title": "Using Git",
    "section": "",
    "text": "We use git/github to manage our codebase. Version control is an essential to successfully managing of any project with multiple contributors! There can be a steep learning curve when you are first getting started with Git, but the time and hassle you will save your future self by learning will be enormous!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#what-is-a-repository",
    "href": "Documentation/UsingGit.html#what-is-a-repository",
    "title": "Using Git",
    "section": "What is a Repository?",
    "text": "What is a Repository?\nA repository (or repo) is just a collection of code, files, data, etc. that are being tracked by Git. Generally, all components of a repo should work towards a common end, whether that is an expansive goal such as managing all code used in our processing pipeline, maintaining the versions of code running on our data loggers, or the backend of our organizations website",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#what-is-a-branch",
    "href": "Documentation/UsingGit.html#what-is-a-branch",
    "title": "Using Git",
    "section": "What is a Branch?",
    "text": "What is a Branch?\n\n\nBranches can be thought of like limbs on a tree, or perhaps a more accurate analogy can be taken from hydrology, by comparing them to an anbranching river channel.\n\nBranches are separate “streams” of code that break off of the main “channel”. Some branches may be dead ends that don’t end up going anywhere. While some branches may lead lead to improvements that can be incorporated back into the main branch of the codebase. Others may break off from the main project entirely.\n\n\n\n\n\nAn anabranching section of the Mackenzie River Delta.\n\n\n\n\n\nBranch Protection Rules\nWe use branch protection rules to protect the main codebase. All changes to the main branch of a protected repository, must be submitted via a “pull request” using a separate branch. This allows for review of the changes by other users before changes are incorporated. It adds an extra level of security by protecting the main branch of code from the erroneous keystrokes of novice and expert users alike! These rules are act as a “gate” that helps ensure any changes to operational code are intentional and agreed upon by relevant parties. If you try to push (force) a change to the main branch of a protected repository on our GitHub, you’ll be given an error message and told to submit a pull request instead! More info can be found here",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#common-git-tasks",
    "href": "Documentation/UsingGit.html#common-git-tasks",
    "title": "Using Git",
    "section": "Common Git Tasks",
    "text": "Common Git Tasks\n\nCreating a Repo From Scratch\nSay you have a collection of code that you want to add to a local repository. The syntax would be as follows:\ngit init -b main\ngit add some_program.py\ngit commit -am \"First Commit: Adding my initial code to repo\"\nWhat is happening here?\ninit: Creates an empty repository with a “branch” (-b) named “main”\nadd: indicates that “some_program.py” will be tracked within the repo. Other files in the repo will not be tracked unless you explicitly add them. Alternatively bash git add . would all all files within the repo. There are more complex patterns that can be setup to explicitly exclude/include some files using “.gitignore” files, but that’s a topic for another day.\ncommit: Tells git to “save” a permanent copy of all (-a) tracked files in the repo in there current state and leave a commit message (-m) to leave important notes about the commit. Commit messages are extremely helpful, both for reminding yourself what happened, and indicating to collaborators what’s included in a commit.\n\nYou should avoid leaving short/nondescript commit messages such as “updated code”, because that doesn’t help anyone\nIt is good practice to make a commit any time you make an important change to your project!\n\n\nAdding Your New Repository to GitHub\nGo to github.com and follow the steps outlined in the images bellow:\n\n\n\n\n\n1. Create a new repository\n\n\n\n\n\n\n2. Give it the same name as your local repo. Accept all defaults then click create.\n\n\n\n\n\n\n\n3. The next window will open an empty repo. Copy the url go back to your local git repo and execute the commands listed bellow.\n\n\ngit remote add origin https://github.com/June-Skeeter/SomeTest.git\ngit push --set-upstream origin main\nThese commands will tell your local git where the remote (on the cloud) version of your repository is located and then “push” your local copy to the cloud. If you go back to the github repo and refresh the webpage, you’ll see that whatever files were in your local copy are now visible remotely!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#working-with-an-existing-repository",
    "href": "Documentation/UsingGit.html#working-with-an-existing-repository",
    "title": "Using Git",
    "section": "Working With an Existing Repository",
    "text": "Working With an Existing Repository\nMore often than not, you’ll end up working with an existing repository. You may need to make some changes, which can be added back to the main branch (following the branch protection rules!) or you may just need to run some fully developed code locally on your computer. For example, maybe you need to do some processing for the HOGG site in Manitoba. You can “download” a local copy of the repo and create a branch for your local changes as follows:\ngit clone https://github.com/ubc-micromet/Calculation_Procedures.git\ngit checkout -b MyHoggUpdates\nWhat is happening here?\nclone:: Tells git to download a local copy a repository from a remote location (i.e. a github URL) to your computer.\ncheckout: Tells git to create an new branch (-b) with the name “MyHoggUpdates”. This branch will contain all of your own adjustments, but the main branch on your local machine will not be altered! If you break something (e.g., delete an important code block), its easy to go back to the main branch. Just type bash git checkout main to go back in to the main branch. Note only need to add bash -b after bash checkout when creating a new branch. Git will automatically revert all files in the repo the the main version.\n\nContributing your Changes\nProvided you have permission to write to the repository (this is granted on a per-user basis by repo “owners”), you can contribute any changes you make by pushing a copy of your branch to github. Say you added a new file “new_HOGG_file.ini”, you could add it, commit it, then push to git hub as follows:\ngit add new_HOGG_file.ini\ngit commit -am \"A relevant description of your changes\"\ngit push --set-upstream origin MyHoggUpdates\nNote you only need to run the --set-upstream origin Test once per branch. The second, third, hundredth push you can simply type git push\n\n\nPulling Remote Changes\nSay there were some important updates to the main remote branch that you need to incorporate to code on your local branch. Starting from within MyHoggUpdates You could:\ngit checkout main\ngit pull\ngit checkout MyHoggUpdates\ngit merge main\npull downloads all the changes from the main branch on github to your local machine.\nmerge will automatically incorporate any changes for files that don’t conflict with changes you’ve made to files locally. i.e., if you’re working on “new_HOGG_file.ini”, but someone else changed “existing_BB_file.ini” then the updates to “existing_BB_file.ini” will be brought in without causing any “merge conflicts”. If you also made an edit to “existing_BB_file.ini” that does not match the incoming change, it will create a merge conflict, that needs to be resolved manually.",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#other-useful-commands-and-features",
    "href": "Documentation/UsingGit.html#other-useful-commands-and-features",
    "title": "Using Git",
    "section": "Other Useful Commands and Features",
    "text": "Other Useful Commands and Features\ngit status\nstatus will list which if any files have been changed since the last commit and any files that have been added but are not tracked.\ngit restore existing_BB_file.ini\nrestore: will revert “existing_BB_file.ini” back to it’s state in the most recent commit. This is useful if you broke something and want to “go back” to the way things were. You can give a specific file name after\ngit revert HEAD~n\nrevert allows you go back any “n” number of commits, bash Head~2 would go back to the state you had two commits ago.\nIssues: Opening issues on GitHub allows us to delegate work, discuss task and keep notes on various features.\n\nThey can be labeled as bugs, upgrade requests, etc.\n\nIssues can be assigned to one or more members of the organization, and you can leave comments to discuss issues.\nAn issue remains open until they are “closed”.\n\nGitHub Pages: GitHub can be used to render static sites (like this one). Its a super useful skill/resource. But that’s a whole separate tutorial for another time!",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/UsingGit.html#i-broke-something",
    "href": "Documentation/UsingGit.html#i-broke-something",
    "title": "Using Git",
    "section": "I broke something",
    "text": "I broke something\nSometimes you break something and need to know how to fix it. Pardon the profanity … but Oh Shit, Git is a really helpful resource.",
    "crumbs": [
      "Documentation",
      "Using Git"
    ]
  },
  {
    "objectID": "Documentation/index.html",
    "href": "Documentation/index.html",
    "title": "Overview",
    "section": "",
    "text": "This page is intended to serve as a catchall with general information for the lab.\n\nResources for current members of the lab.\nTips and tricks for coding and using github.\nLinks to relevant information.\nOverview of the data pipeline and database.",
    "crumbs": [
      "Documentation",
      "Overview"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/index.html",
    "href": "Documentation/Fieldwork/index.html",
    "title": "Fieldwork maintenance and procedures",
    "section": "",
    "text": "This page covers general procedures for daily monitoring and field site maintenance/calibration. If you need to sechedule a field visit - the availability of lab members by day is shown below in Table 1.\n\nUnder normal operating conditions, maintenance trips should occur every 4-6 weeks for each site. Further trips can be scheduled as needed. Ensure you are review the key information before commencing field work. You can checklists below to help prepare for field visits and ensure you are following standard procedures while in the field.\n\n\n\n\n\nTable 1: Availability of Lab Members for Fieldwork.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\n9\n\n\n\n\nJune\nNot Available\nAvailable\nNot Available\nAvailable\nIf Necessary\nAvailable\nAvailable\nNA\n\n\nTed\nBefore noon\nBefore 2pm*\nNot Available\nBefore 2pm*\nAvailable\nAvailable\nNot Available\n*after Feb 26, before that “Not Available”\n\n\nVanessa\nNot Available\nBefore 1pm\nNot Available\nNot Available\nAvailable\nAvailable\nAvailble\nNA\n\n\nSarah R\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nAvailable\nNA\n\n\nTzu-Yi\nAvailable\nAvailable\nAvailable every other week\nNot Available\nIf Necessary\nAvailable\nAvailable\nNA\n\n\nHimari\nAvailable\nNot Available\nNot Available\nNot Available\nNot Available\nAvailable\nAvailable\nNA",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html",
    "href": "Documentation/Fieldwork/Calibration.html",
    "title": "Calibration Procedures",
    "section": "",
    "text": "This page details the steps for calibrating the 7200 and 7700. Always do the 7200 first, followed by the 7700. You must clean the sensors before calibration.\nTO DO: Should we ‘formally’ require calibration on each site visit?",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#getting-started",
    "href": "Documentation/Fieldwork/Calibration.html#getting-started",
    "title": "Calibration Procedures",
    "section": "Getting Started",
    "text": "Getting Started\n\nMake sure the flow module, tubing, 7700 calibration shroud, and necessary wrenches are ready.\n\nNOTE: At BB1 & BB2, it is in the equipment box, for DSM & RBM you must bring to the field.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#calibrating-the-7200",
    "href": "Documentation/Fieldwork/Calibration.html#calibrating-the-7200",
    "title": "Calibration Procedures",
    "section": "Calibrating the 7200",
    "text": "Calibrating the 7200\n\nOn the laptop open the program ‘Li 7X00’ and connect to instrument. Click the LI-7200 icon and choose ‘calibration’.\nSave the old calibration coefficients. Click “Config Files” tab &gt;&gt; “Save Configuration” &gt;&gt; check all and “continue”.\n\nPlace the file in this directory “C:”. The LI 7200 serial numbers are listed in the table below.\n\nThe naming template is:  yyyymmdd(site visit date)_Configuration_Before_calibrations.l7x\n\nScreenshots should also be taken before and after calibrating and saved in the same directory.\n\n\n\nLI 7200 Serial Numbers\n\n\n\nSite\nValue\n\n\n\n\nBB1\n0816\n\n\nBB2\n0815\n\n\nDMS\n1029\n\n\nRBM\n…\n\n\n\n\nCheck the head serial number information in “LI-7200” &gt; “Calibration” &gt; “Coefficients”. Make sure the head serial number on the sensor matches that in the software (see table above).\n\n\n\n\n\n\n\nConnect the calibration flow tube to the 7200 head. At BB2, DSM & RMB - there is a special intake novel that is always connected to the intake tube? At BB1 you need to do manually disconnect the intake tube first?\n\nTO DO: Add pics for each site?\n\n\nSet CO2 zero\n\nConnect the flow meter to the N2 gas tank and open main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow.\n\nNOTE: REVERSED directions (on is right, off is left)\n\nWatch plot on the program: CO2 should drop to 0. On the calibration screen – green flags show that CO2 concentrations are steady. You may need to change the scale on the chart.\n\nClick “Zero CO2” &gt; “OK”\nClick “Zero H2O &gt; “OK”\nCheck graph values continuously throughout the process to make sure values are correct\n\nClose regulator then close the main N2 tank value and disconnect the flow meter.\n\n\n\nSet CO2 span\n\nMove flow meter to CO2 tank and open the main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank) — To Do: Confirm value?\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow.\n\nNOTE: REVERSED directions (on is right, off is left)\n\nEnter CO2 concentration (ppm), Confirm the exact value on the tank. Then Watch plot on the program: CO2 should increase until it stabilizes. On the calibration screen – green flags show that CO2 concentrations are steady. You may need to change the scale on the chart again.\n\nClick Span CO2” &gt; “OK”\n\nDO NOT Click Span H2O\n\n\nClose regulator then close the main CO2 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\n\n\n\nCheck the Coefficients\n\nYou can find the calibration results in “LI-7200” &gt; “Calibration” &gt; “Manual” tab. Reference the table below for acceptable values. Take a screenshot (alt + print screen) of the manual tab and save it to the Micromet Google drive:\nMicromet Lab/Projects/(**Flux Site**)/Flux-tower/Calibrations/LI-7200/SNXXXX (reference serial number table above)\n\n\nOptimal Calibration Constant Values\n\n\n\n\n\n\n\n\nConstant\nValue\nNotes\n\n\n\n\nCO2 Zero\n0.85 ~ 1.1\nZero is primarily affected by temperature, and the state of the internal chemicals\n\n\nCO2 Span\n0.97 ~ 1.03\nA value outside this range indicates is a warning sign for me that something is not correct with either the instrument (wrong head, bad sensor) or with the tank (not accurately calibrated).\n\n\nH2O Zero\n0.9 ~ 1.2\nSet in lab (dobule check frequency?)\n\n\nH2O Span\n0.9 ~ 1.1\nSet in lab (dobule check frequency?)\n\n\n\n\n\n\n\n\n\nSave the after calibration coefficients. Click “Config Files” tab &gt;&gt; “Save Configuration” &gt;&gt; check all and “continue”. Use same naming/saving convention as above.\nDisconnect calibration tubing from 7200 intake. All the connections on the tubing that stays connected to the ‘T’ on the 7200 head should be wrench tight when finished with calibrations.\nExit the program",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/Calibration.html#calibrating-the-7700",
    "href": "Documentation/Fieldwork/Calibration.html#calibrating-the-7700",
    "title": "Calibration Procedures",
    "section": "Calibrating the 7700",
    "text": "Calibrating the 7700\nTO DO: Ask - should we save screenshots for 7700 calibrations?\n\nOpen the program ‘LI-7700’ and go to the data page 1 &gt;&gt; 1 chart CH4 (umol/mol).\nUse calibration cylinder to cover the 7700. Make sure to remove 7700 head cap and washer tube first then orient the tube with the straps on top and slide tube over the instrument.\nAttach the small black tube into the LI-7700 cover and attach other end to the black tube with the flow meter (same one as used for 7200). Make sure to check union connections.\n\n\nSet CH4 zero\n\nConnect the flow meter to the N2 gas tank and open main valve (on is left, off is right).\n\nNOTE: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\nOpen regulator slowly until the flow rate is around 18-20 L/min. You should hear the flow. NOTE: REVERSED directions (on is right, off is left)\nWatch plot on the program: CH4 should drop to 0 (or ~.13). On the calibration screen – green flags show that it’s steady Change scale if needed to check that the trace is flat/steady\n\nClick “Zero CH4” &gt; “Apply”\n\nClose regulator then close the main N2 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\n\n\n\nSet CH4 span\n\nMove flow meter to CH4 tank and open the main valve (on is left, off is right).\n\nNote: PSI should be ~ 1200 (not necessary to the procedure, but good to note as it will decrease over time and indicated when we need a new gas tank)\n\n\nTO DO: Confirm value?\n\nEnter CH4 concentration (ppm), Confirm the exact value on the tank. Then Watch plot on the program: CH4 should increase until it stabilizes. On the calibration screen – green flags show that CH4 concentrations are steady. You may need to change the scale on the chart again.\n\nClick “Span CH4” &gt; “Apply”\n\nClose regulator then close the main CH4 tank value and disconnect the flow meter.\n\nCheck how much gas we have left & take a picture\n\nRemove the calibration shield, make sure to disconnect the tubing first. Then make sure to replace the 7700 head cap and washer tube first.\n\nTO DO: Check the Coefficients",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Calibration procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html",
    "title": "Fieldwork Protocol & Procedures",
    "section": "",
    "text": "This page covers general procedures for doing maintenance and calibration at our flux sites. Under normal operating conditions, maintenance trips should occur every 4-6 weeks for each site. Further trips can be scheduled as needed. Ensure you are review the key information before commencing field work. You can checklists below to help prepare for field visits and ensure you are following standard procedures while in the field.",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#planning-a-trip",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#planning-a-trip",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Planning a Trip",
    "text": "Planning a Trip\nMake a clear list of tasks & goals. If you are going to one of the one of the flux stations you can check the field logs. See what was done on the last visit and check for notes to see if there is anything that needs to be done.\n\nBB1&2\nDSM\nRBM\nUse the checklists below to make sure you have all the supplies you need and complete all crucial tasks",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#scheduling-communication",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#scheduling-communication",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Scheduling & Communication",
    "text": "Scheduling & Communication\nLet the lab know when you will be going into the field and make sure you have at least one person joining you.\n\nYou can use the fieldwork slack channel to discuss scheduling\nBook the Micromet Truck using the google calendar\n\nContact Rick for calendar access or to request alternative vehicle options.\n\nIf you are going into Burns Bog:\n\nBoth participants must be permit holders\nYou must also call Metro Vancouver to check in and check out: 604-520-6442",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/Fieldwork/FieldworkProcedures.html#safety",
    "href": "Documentation/Fieldwork/FieldworkProcedures.html#safety",
    "title": "Fieldwork Protocol & Procedures",
    "section": "Safety",
    "text": "Safety\nEnsure that you are familiar with the lab’s Safety Plan\n\nIn Summer: Be cautions of fire safety:\n\nDon’t park on gras, bring water sprayer spray and fire extinguisher.\n\nIn Winter: Be mindful of the weather forecasts:\n\nDress appropriately, bring extra socks, a head lamp, gloves, etc.\nCheck the tides when going to RBM or DSM",
    "crumbs": [
      "Documentation",
      "Fieldwork maintenance and procedures",
      "Fieldwork Protocol & Procedures"
    ]
  },
  {
    "objectID": "Documentation/ReproducibleDataFlow.html",
    "href": "Documentation/ReproducibleDataFlow.html",
    "title": "Tips for Reproducible Data Flow",
    "section": "",
    "text": "R resources\nHere is some helpful info on setting up a project in R. I’ve adapted the figure from that link to better suit the needs of our research group (see below).\n\n\n\n\n\nOther helpful resources can be found in the ‘R_dynamic_document_overview_html.html’ and ‘0_set_up_workspace.html’ doc in the ‘Coding resources/R resources’ in the Google Drive. These were written by Dan Moore, but are not meant to be distributed publicly.",
    "crumbs": [
      "Documentation",
      "Tips for Reproducible Data Flow"
    ]
  },
  {
    "objectID": "acknowledgement.html",
    "href": "acknowledgement.html",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "",
    "text": "The University of British Columbia and the city of Vancouver are on the traditional, ancestral, and unceded territory of the Coast Salish Peoples. Specifically the UBC Vancouver campus is on xʷməθkʷəy̓əm (Musqueam) land.",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "acknowledgement.html#field-sites",
    "href": "acknowledgement.html#field-sites",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "Field Sites",
    "text": "Field Sites\nOur field sites are located in the Statl̕əẃ (Sto:lo) River Delta (aka the Fraser River Delta). This map below shows the overlapping territories and conveys the multiplicity of occupancy of First Nations wh in and around the Statl̕əẃ delta. This area encompass the traditional territories of a number of First Nations, including the:\n\n\n\nxʷməθkʷəy̓əm\nStó:lō\nsc̓əwaθenaɁɬ təməxʷ (Tsawwassen)\nkwantlen\nSemiahmoo\nStz’uminus\nÁ,LEṈENEȻ ȽTE (W̱SÁNEĆ)\nsq̓əc̓iy̓aɁɬ təməxʷ (Katzie)\nAnd others",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "acknowledgement.html#what-does-it-mean",
    "href": "acknowledgement.html#what-does-it-mean",
    "title": "Territorial and Cultural Acknowledgement",
    "section": "What does it mean?",
    "text": "What does it mean?\n\n\nTraditional and Ancestral Recognize by whom the lands were traditionally used and/or occupied and the cultures have been handed down from generation to generation. The area around UBC was used by many different people, including the xʷməθkʷəy̓əm, əl̓ilwətaɁɬ, and Skwxwú7mesh-ulh Nations. The map to the left shows the overlapping territories of these nations, along with others in the region.\nThese people are are part of a broader linguistic / cultural group of Coast Salish speaking people. The xʷməθkʷəy̓əm and əl̓ilwətaɁɬ speak dialects of Hul’q’umi’num’ / Halq’eméylem / hən̓q̓əmin̓əm̓ and the Skwxwú7mesh-ulh speak Sḵwx̱wú7mesh sníchim.\n\n\n\n\n\n\n\nUnceded: Refers to land that was not turned over to the government by a treaty or other agreement. Over 95% of the land in BC, and many lands elsewhere in the world were never ceded by treaty. Without treaties, these lands remain the sovereign territory of the First Nations that call them home. Yet at the same time, the lands have been claimed by Canada and these First Nations living on these lands lack a framework to express their sovereignty. This by no means absolves the Canadian government of their crimes where lands were “ceded” by treaty. Treaties were more frequently reached by coercion than negotiation. The RCMP was created specifically to force indigenous people off their lands by any means necessary.",
    "crumbs": [
      "Home",
      "Territorial and Cultural Acknowledgement"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_3_Install_Matlab.html",
    "href": "PipelineDocumentation/2_3_Install_Matlab.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Currently, it is necessary to use Matlab to run the data cleaning scripts.\n\nFirst, check which Matlab version is currently recommended for the pipeline code, in section 7.1. While the data pipeline will most likely work with other recent versions of Matlab, we make no guarantees of cross compatibility.\nVisit this website for details on how to install Matlab or contact your systems administrator for help.\nMake sure you install Matlab with the following toolboxes:\n\nCurve fitting\nGlobal Optimization\nOptimization\nSignal Processing\nStatistics and Machine Learning.\n\nAdditional toolboxes may be available depending on your institutional/personal licence. However, having every available toolbox installed is likely a waste of your disk space, and is not required for the data pipeline.\nIf you already have the correct version of Matlab on your local computer, you can install the necessary toolboxes using the “Add-Ons” button in the “Home” tab within Matlab.\nWhen you have Matlab along with the relevant toolboxes installed, you are ready to configure it to use with the Biomet.net library.\n\nNOTE: You can have more than one version of Matlab on your computer if you prefer to use another version of Matlab for other projects.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.3 &nbsp; Install Software: Matlab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_3_Install_Matlab.html#install-matlab-on-your-local-computer",
    "href": "PipelineDocumentation/2_3_Install_Matlab.html#install-matlab-on-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Currently, it is necessary to use Matlab to run the data cleaning scripts.\n\nFirst, check which Matlab version is currently recommended for the pipeline code, in section 7.1. While the data pipeline will most likely work with other recent versions of Matlab, we make no guarantees of cross compatibility.\nVisit this website for details on how to install Matlab or contact your systems administrator for help.\nMake sure you install Matlab with the following toolboxes:\n\nCurve fitting\nGlobal Optimization\nOptimization\nSignal Processing\nStatistics and Machine Learning.\n\nAdditional toolboxes may be available depending on your institutional/personal licence. However, having every available toolbox installed is likely a waste of your disk space, and is not required for the data pipeline.\nIf you already have the correct version of Matlab on your local computer, you can install the necessary toolboxes using the “Add-Ons” button in the “Home” tab within Matlab.\nWhen you have Matlab along with the relevant toolboxes installed, you are ready to configure it to use with the Biomet.net library.\n\nNOTE: You can have more than one version of Matlab on your computer if you prefer to use another version of Matlab for other projects.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.3 &nbsp; Install Software: Matlab"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_2_Recently_Added_Features.html",
    "href": "PipelineDocumentation/7_2_Recently_Added_Features.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This page contains significant features recently added to the data cleaning pipeline - most recent at the top.\n\n\n\nNew Third Stage YML template:\nWe improved error messaging for third stage cleaning, including passing more information to the log file and to the Matlab output display. You need to be using the most recent site-specific YML template for the third stage to run. The updated template is available in section 4.3 under “Download INI/config template files”.\nNew Overwrite feature:\nSometimes we need to overwrite multiple properties for one or more traces that have already been created e.g. in an include file. The global variables feature allows this, however, once this section becomes long with many trace property tweaks, it can become very hard to troubleshoot, and in these cases having all the information for one trace together is more desirable.\nInstead of using global variables you can duplicate the full trace ([Trace] ... [End]), and in your site-specific first stage INI file, put this duplicate after the line of code where the include file is called that contains your original trace. Note the additional Overwrite property highlighted in yellow:\n\nFigure 7.2A. Location within site-specific INI file to put duplicate trace for overwriting a trace previously defined in an include INI file. In this case, we want to overwrite the CH4_MIXING_RATIO trace that was originally defined in EddyPro_LI7700_FirstStage_include.ini. Yellow highlighting shows the syntax for the “overwrite” property.\nThere are three overwrite options:\n\n0 = do not overwrite with this trace. This is the default setting. If you do not include the Overwrite parameter, the pipeline assumes this option. If you have a duplicate trace you will get an error during cleaning (Example 1):\n\n1 = overwrite traces having the same variableName and also with Overwrite = 0 setting. This puts the duplicate data where the original data was, i.e., complete overwrite, first trace gone. Use this setting if you want your duplicate T2 available to use in a later variable such as T3, T4, T5, etc. (Example 2):\n\n2 = overwrite traces having the same variableName and with Overwrite = 0 setting. This takes advantage of the “position” of the duplicate trace. Use this setting if you want a later variable such as T3 or T4 available to use in T2.\n\n\n\nNew postEvaluate property:\nMore complex user-defined processing can be applied in the first stage to any trace using the very useful “Evaluate” and new “postEvaluate” options. Matlab functions (user-written or from Biomet.net) can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” or “postEvaluate” strings. They can be used to derive variables from available data, flag variables to remove bad data, or to calculate new useful variables. Here is an Evaluate example for removing outliers from a trace:\nEvaluate = 'wlen=24;thres=4;TA_1_1_1 = run_std_dev(TA_1_1_1,clean_tv,wlen,thres);'\nThe Evaluate property is executed for all traces before any other cleaning properties, e.g., minMax, calibration, etc.. In contrast, the newer postEvaluate property is executed in the first stage after all other cleaning is done.\nGenerally speaking, the order of operations is: Evaluate –&gt; other cleaning (e.g., minMax, calibration) –&gt; postEvaluate. This is regardless of the order that they appear  within [Trace] ... [End]. The following series of examples shows how the Evaluate and postEvaluate statements work in relation to the other cleaning properties. For each example the input (Original) is an array of ones, and both the “Original” and first-stage “Clean” data are plotted in the result.\nExample 1: Only minMax and calibration, no Evaluate or postEvaluate statements\n\nResult: x = 2x [calibrated]\n\n\nExample 2: Evaluate, minMax and calibration, no postEvaluate statement\n\nResult: x = (x - 1)*2 [Evaluate first, then calibrate]\n\n\nExample 3: minMax and calibration, postEvaluate statement, no Evaluate statement\n\nResult: x = 2x - 1 [calibrate, then postEvaluate]\n\n\nExample 4: minMax and calibration, both Evaluate and postEvaluate statements\n\nResult: x = (x - 1)*2 - 1 [Evaluate, then calibrate, then postEvaluate]",
    "crumbs": [
      "Pipeline Documentation",
      "7. Troubleshooting and FAQ",
      "7.2 &nbsp; Recently Added Features"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_2_Recently_Added_Features.html#recently-added-features",
    "href": "PipelineDocumentation/7_2_Recently_Added_Features.html#recently-added-features",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This page contains significant features recently added to the data cleaning pipeline - most recent at the top.\n\n\n\nNew Third Stage YML template:\nWe improved error messaging for third stage cleaning, including passing more information to the log file and to the Matlab output display. You need to be using the most recent site-specific YML template for the third stage to run. The updated template is available in section 4.3 under “Download INI/config template files”.\nNew Overwrite feature:\nSometimes we need to overwrite multiple properties for one or more traces that have already been created e.g. in an include file. The global variables feature allows this, however, once this section becomes long with many trace property tweaks, it can become very hard to troubleshoot, and in these cases having all the information for one trace together is more desirable.\nInstead of using global variables you can duplicate the full trace ([Trace] ... [End]), and in your site-specific first stage INI file, put this duplicate after the line of code where the include file is called that contains your original trace. Note the additional Overwrite property highlighted in yellow:\n\nFigure 7.2A. Location within site-specific INI file to put duplicate trace for overwriting a trace previously defined in an include INI file. In this case, we want to overwrite the CH4_MIXING_RATIO trace that was originally defined in EddyPro_LI7700_FirstStage_include.ini. Yellow highlighting shows the syntax for the “overwrite” property.\nThere are three overwrite options:\n\n0 = do not overwrite with this trace. This is the default setting. If you do not include the Overwrite parameter, the pipeline assumes this option. If you have a duplicate trace you will get an error during cleaning (Example 1):\n\n1 = overwrite traces having the same variableName and also with Overwrite = 0 setting. This puts the duplicate data where the original data was, i.e., complete overwrite, first trace gone. Use this setting if you want your duplicate T2 available to use in a later variable such as T3, T4, T5, etc. (Example 2):\n\n2 = overwrite traces having the same variableName and with Overwrite = 0 setting. This takes advantage of the “position” of the duplicate trace. Use this setting if you want a later variable such as T3 or T4 available to use in T2.\n\n\n\nNew postEvaluate property:\nMore complex user-defined processing can be applied in the first stage to any trace using the very useful “Evaluate” and new “postEvaluate” options. Matlab functions (user-written or from Biomet.net) can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” or “postEvaluate” strings. They can be used to derive variables from available data, flag variables to remove bad data, or to calculate new useful variables. Here is an Evaluate example for removing outliers from a trace:\nEvaluate = 'wlen=24;thres=4;TA_1_1_1 = run_std_dev(TA_1_1_1,clean_tv,wlen,thres);'\nThe Evaluate property is executed for all traces before any other cleaning properties, e.g., minMax, calibration, etc.. In contrast, the newer postEvaluate property is executed in the first stage after all other cleaning is done.\nGenerally speaking, the order of operations is: Evaluate –&gt; other cleaning (e.g., minMax, calibration) –&gt; postEvaluate. This is regardless of the order that they appear  within [Trace] ... [End]. The following series of examples shows how the Evaluate and postEvaluate statements work in relation to the other cleaning properties. For each example the input (Original) is an array of ones, and both the “Original” and first-stage “Clean” data are plotted in the result.\nExample 1: Only minMax and calibration, no Evaluate or postEvaluate statements\n\nResult: x = 2x [calibrated]\n\n\nExample 2: Evaluate, minMax and calibration, no postEvaluate statement\n\nResult: x = (x - 1)*2 [Evaluate first, then calibrate]\n\n\nExample 3: minMax and calibration, postEvaluate statement, no Evaluate statement\n\nResult: x = 2x - 1 [calibrate, then postEvaluate]\n\n\nExample 4: minMax and calibration, both Evaluate and postEvaluate statements\n\nResult: x = (x - 1)*2 - 1 [Evaluate, then calibrate, then postEvaluate]",
    "crumbs": [
      "Pipeline Documentation",
      "7. Troubleshooting and FAQ",
      "7.2 &nbsp; Recently Added Features"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_5_Install_R_Studio.html",
    "href": "PipelineDocumentation/2_5_Install_R_Studio.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "As with all the other software installations for use in data cleaning, first check which versions are recommended at present.\n\nInstallation instructions for R on Windows and Mac: cran.rstudio.com.\nInstallation instructions for RStudio on Windows and Mac: posit.co/download/rstudio-desktop.\nAny packages that you need and do not already have will be automatically installed by our pipeline scripts.\nNext, install the following libraries in RStudio by running the code below. Note this code will only install packages that you don’t already have installed:\n\npackages &lt;- c(\"tidyverse\", \"caret\", \"REddyProc\", \"dplyr\", \"lubridate\", \"data.table\", \"fs\", \"yaml\", \"rlist\", \"zoo\", \"reshape2\", \"stringr\", \"ranger\", \"caret\", \"ggplot2\")\n  \ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n  if (any(installed_packages == FALSE)) {\n    install.packages(packages[!installed_packages])\n  }",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.5 &nbsp; Install Software: R/RStudio"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_5_Install_R_Studio.html#install-r-and-rstudio-on-your-local-computer",
    "href": "PipelineDocumentation/2_5_Install_R_Studio.html#install-r-and-rstudio-on-your-local-computer",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "As with all the other software installations for use in data cleaning, first check which versions are recommended at present.\n\nInstallation instructions for R on Windows and Mac: cran.rstudio.com.\nInstallation instructions for RStudio on Windows and Mac: posit.co/download/rstudio-desktop.\nAny packages that you need and do not already have will be automatically installed by our pipeline scripts.\nNext, install the following libraries in RStudio by running the code below. Note this code will only install packages that you don’t already have installed:\n\npackages &lt;- c(\"tidyverse\", \"caret\", \"REddyProc\", \"dplyr\", \"lubridate\", \"data.table\", \"fs\", \"yaml\", \"rlist\", \"zoo\", \"reshape2\", \"stringr\", \"ranger\", \"caret\", \"ggplot2\")\n  \ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n  if (any(installed_packages == FALSE)) {\n    install.packages(packages[!installed_packages])\n  }",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.5 &nbsp; Install Software: R/RStudio"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_1_Install_Git_Optional.html",
    "href": "PipelineDocumentation/2_1_Install_Git_Optional.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. It is useful if you think you may contribute to the Biomet.net library, which is a Git repository.\n\nDownload the latest version of Git for Windows or Mac here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\nNext, if you don’t already have a personal GitHub account, go to GitHub and set up one.\nOnce these tasks are complete, if you wish to contribute your own code to the Biomet.net library, start by creating a separate git branch to work on. Once you are happy with your code, create a pull request, then someone will review and approve the new files or contact you if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.1 &nbsp; Install Software: Git (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_1_Install_Git_Optional.html#install-git-and-create-personal-github-account-optional",
    "href": "PipelineDocumentation/2_1_Install_Git_Optional.html#install-git-and-create-personal-github-account-optional",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This step is optional. It is useful if you think you may contribute to the Biomet.net library, which is a Git repository.\n\nDownload the latest version of Git for Windows or Mac here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\nNext, if you don’t already have a personal GitHub account, go to GitHub and set up one.\nOnce these tasks are complete, if you wish to contribute your own code to the Biomet.net library, start by creating a separate git branch to work on. Once you are happy with your code, create a pull request, then someone will review and approve the new files or contact you if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.1 &nbsp; Install Software: Git (optional)"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_2_Full_Doc_Create_Database_From_Raw_Data_Inspect_Outputs.html",
    "href": "PipelineDocumentation/5_2_Full_Doc_Create_Database_From_Raw_Data_Inspect_Outputs.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section complements the quick-start set up in section 4.2.\nOn this page:\n\nBest practices\nUseful information on creating your database\nCreate Database From Multiple Input Files and Updates for Continuous Operational Sites\nCreate Database Using Data from Canadian Meteorological Stations\nVisualize and inspect your newly created database\n\n\n\n\n\n\nThe best-practice approach for each project is to have one “main” script for data processing (e.g., DataCleaning_Main.m). This script should contain all the steps that are needed to go from raw data to the final cleaned and gap-filled Ameriflux data set:\n\nObtaining/downloading the raw data (not covered here, we assume you have your own systems for obtaining datalogger output from your sites);\nCreating the database in the appropriate format (this current subsection);\nApplying the data-cleaning stages (subsections 5.3, 5.4, and 5.5).\n\n\nEach flux site is unique with its own sensors, data loggers, and data archiving formats (such as EddyPro output files, TOA5 Campbell Scientific files, Ameriflux CSV files, to name a few). The purpose of step 2 is to convert from such a site-specific dataset to a generic dataset ready to be processed using the standardised libraries (i.e., Biomet.net). These “generic” binary files can be read by most computer languages, by design, meaning they are suitable for the Biomet library which contains code written in Matlab, R, and Python.\nIf you need help getting started with your “main” computer program, a sample Matlab script is provided in the quick-start guide (section 4.2, under “Sample Code”). With that DataCleaning_Main.m script the entire data processing process is documented and it can be easily replicated. Insert as many informative comments as you can - it will help if the process needs to be reviewed later on or the project is handed over to another person. When working with your own data, especially for the first time using the pipeline, we still recommend following the steps in the tutorial as they also outline where to put your raw data files, and other important tasks.\n\n\n\n\n\n\n\nDetermine the data format/source of raw data in your Sites directory.\nUse standard existing function(s) from the Biomet.net library, which you have already cloned/downloaded, to convert your raw data and create the database in Database directory. Examples of functions that can read your data and be used in your “main” script are as follows:\n\ndb_struct2database (create database from Matlab structure; flux or met data);\nfr_read_EddyPro_file (create database from EddyPro output; flux data);\nfr_read_TOA5_file (create database from TOA5 Campbell Scientific ASCII files);\nfr_read_generic_data_file (create database from most file formats).\n\n\n\n\nThe relevant function, when run in matlab, will create subdirectories named by data year (yearIn), then by SITEID (i.e., SITEIDs are grouped by year), as follows:\n\nFigure 5.2A. Directory tree showing structure of Database folder for multiple years and multiple sites under one project.\nThe SITEID subdirectories will be populated with data (e.g., Flux and Met if these data types exist) for the appropriate year, stored as binary files ready for cleaning. The green highlighting in figure 5.2A shows an example with three years of data from two different sites.\nThe clean_tv file is automatically created, and will be located in each Raw/Clean folder. This a standardized time vector of 30-minute frequency. This formats the timestamp of each record in Local Standard Time (LST; as required by AmeriFlux). \n\n\nWe advise you to generalize your scripts as far as possible, based on the sites and data that you have, and remember to document your code thoroughly.\n\n\n\n\n\n\nIn real applications, it is likely that users will have multiple input files from Flux and Met stations (daily, monthly, or annual files). It is also very often the case with active sites that new files are continuously being downloaded from the site and added to the data folder.\nTo facilitate data processing in those cases we use specialized functions that can deal with multiple files and keep track of which files have changes since the last database update and update only those. That way raw data folders can contain thousands of files without affecting the speed of the database updates.\nUpdating Flux data\nHere is an example of how to update database from any files in the Sites/siteID/Flux folder that match a file pattern (in this case eddypro_TUT_*.csv).\n%% Processing multiple EddyPro files from the same folder \nprojectPath = 'E:\\Pipeline_Projects\\TUT Project'; \nstructProject = set_TAB_project(projectPath); \nsiteID = 'TUT'; \nFluxDatabase_Pth = fullfile(structProject.databasePath, 'yyyy', siteID,'Flux'); \nprogressListPth = fullfile(structProject.sitesPath,siteID,'Flux','EddyPro_progressList.mat'); \nfilePattern = fullfile(structProject.sitesPath,siteID,'Flux','eddypro_TUT_*.csv'); \noptionsFileRead.flagFileType = 'fulloutput'; \nmissingPointValue = NaN; \ntimeUnit = '30MIN'; \n[nFiles,nHHours]=fr_EddyPro_database(filePattern,progressListPth,FluxDatabase_Pth,[],timeUnit,missingPointValue,optionsFileRead); \nfprintf('  %s - Flux  Number of files processed = %d, Number of HHours = %d\\n',siteID,nFiles,nHHours); \n\n\nUpdating Met data\nIf we then wanted to process met data from Sites/siteID/Met folder (using the same projectPath, structProject and siteID) with the file-name pattern matching: TUT_MET.* we could run the following script:\n%% Processing multiple TOA5 files from the same folder \nMetDatabase_Pth = fullfile(structProject.databasePath, 'yyyy', siteID,'Met'); \nprogressListPth = fullfile(structProject.sitesPath,siteID,'Met','Met_progressList.mat'); \nfilePattern = fullfile(structProject.sitesPath,siteID,'Met','TUT_MET.*'); \nmissingPointValue = NaN; \ntimeUnit = '30MIN'; \n[nFiles,nHHours]=fr_TOA5_database(filePattern,progressListPth,MetDatabase_Pth,[],timeUnit,missingPointValue);  \nfprintf('  %s - Met  Number of files processed = %d, Number of HHours = %d\\n',siteID,nFiles,nHHours); \n\n If we run the same sequence twice (fr_TOA5_database or fr_EddyPro_database as shown above), the second time the number of files processed would be zero assuming that there were no new files added to the Sites folder between the two runs. If on the other hand, one of the files changed, the function would process it. That is how real-time data gets processed in this data pipeline.\nOther examples of Biomet.net functions that update data continuously, and can be written into a script as previously shown, are as follows:\n\nfr_HOBO_database (continuously create database from HOBO met data);\nfr_SmartFlux_database (continuously create database from SmartFlux output; flux data);\nfr_SoilFluxPro_database (continuously create database from SoilFluxPro output data).\n\n\n\n\n\n\n\n\nData from Canadian meteorological stations [operational climate stations run by Environment and Climate Change Canada (ECCC)] can be used for gap-filling met data for Canadian flux sites that are in close proximity. If you have gaps in your met data, you can search for nearby climate stations as follows:\n\nTo find the closest Environment ECCC stations to your flux site, first go to this website: https://climate.weather.gc.ca/historical_data/search_historic_data_e.html\nClick on the “Search by Proximity” tab (figure 5.2B), select “location coordinates in Decimal Degrees” and type in the coordinates of your flux site:\n\nFigure 5.2B. “Search by Proximity” tab on ECCC Historical Data website.\nWe advise selecting a station no more than 25 km away, but that is also up to the user’s judgement and their responsibility to justify.\nIt also helps to select a proper range of dates using either “with data available between” or “with data on” option.\nFrom the list of stations pick one that’s has “Hourly” data for the dates you need, and is closest to your location (see example in figure 5.2C):\n\nFigure 5.2C. Search results for selecting the station for gap-filling.\nIn this example, the closes would be “WINNIPEG THE FORKS”; click on “Go”, and make a note of the “Climate ID” number provided (figure 5.2D; in this case, the climate ID is 5023262).\n\nFigure 5.2D. Example of Hourly Data Report for Winnipeg The Forks, showing the Climate ID number.\nNext, you need the Station ID (which is different to the Climate ID). Go to this website: https://collaboration.cmc.ec.gc.ca/cmc/climate/Get_More_Data_Plus_de_donnees/; open the Station Inventory EN.csv file and search on the Climate ID number that you saved. The number in the column to the right of this is the Station ID, which in the Winnipeg example is 28051. This Station ID will be used for the data downloads.\nCreate a script using the snippet below and enter your Station ID and other relevant information (years and months). You can enter multiple station IDs at once.\n structProject = set_TAB_project(projectPath); \n stationIDs = [28051]; \n yearsIn = 2022:2023; \n monthsIn  = 1:12; \n for cntStations = 1:length(stationIDs) \n     sID = stationIDs(cntStations); \n     pathECCC = fullfile('yyyy','ECCC',num2str(sID)); \n     try \n         run_ECCC_climate_station_update(yearsIn,monthsIn,sID,structProject.databasePath)     \n     catch \n         fprintf('Error processing station: %d (year: %d, month: %d)\\n',sID,yearsIn,monthsIn(end)); \n     end \n end\nRunning this script will create the database for the given station ID, for two years (2022 and 2023) in the current project folder. The new database for the station 28051 is located here: &lt;projectPath&gt;/Database/2022/ECCC/28051/30min/ (and similarly for 2023).\nAlternatively, if a user wants to create the ECCC database just for the current year, or to keep the climate data base up to date by downloading all the months between Jan and the current month, they can use this function call: run_ECCC_climate_station_update([],1:month(datetime),28051,db_pth_root).\nOr, for example, to update only the last two months of the current year: run_ECCC_climate_station_update([],month(datetime)-1:month(datetime),28051,db_pth_root).\n\n\n\n\n\n\n\nOnce your database is created, we strongly advise you to review its contents using Biomet.net functions. Some of these are listed in the tutorial under section 4.2, and section 6 provides extensive details of visualization tools for inspecting and analyzing raw and cleaned data.\nWhen you have confirmed that all the traces you expect to see are present in your newly created Database, you are ready to start cleaning the data. First, you need to obtain the set of sample, configuration, and standardization files, described in section.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.2 &nbsp; Full Documentation: Create Database from Raw Data and Inspect Contents"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_2_Full_Doc_Create_Database_From_Raw_Data_Inspect_Outputs.html#full-documentation-create-database-from-raw-data-and-inspect-contents",
    "href": "PipelineDocumentation/5_2_Full_Doc_Create_Database_From_Raw_Data_Inspect_Outputs.html#full-documentation-create-database-from-raw-data-and-inspect-contents",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section complements the quick-start set up in section 4.2.\nOn this page:\n\nBest practices\nUseful information on creating your database\nCreate Database From Multiple Input Files and Updates for Continuous Operational Sites\nCreate Database Using Data from Canadian Meteorological Stations\nVisualize and inspect your newly created database\n\n\n\n\n\n\nThe best-practice approach for each project is to have one “main” script for data processing (e.g., DataCleaning_Main.m). This script should contain all the steps that are needed to go from raw data to the final cleaned and gap-filled Ameriflux data set:\n\nObtaining/downloading the raw data (not covered here, we assume you have your own systems for obtaining datalogger output from your sites);\nCreating the database in the appropriate format (this current subsection);\nApplying the data-cleaning stages (subsections 5.3, 5.4, and 5.5).\n\n\nEach flux site is unique with its own sensors, data loggers, and data archiving formats (such as EddyPro output files, TOA5 Campbell Scientific files, Ameriflux CSV files, to name a few). The purpose of step 2 is to convert from such a site-specific dataset to a generic dataset ready to be processed using the standardised libraries (i.e., Biomet.net). These “generic” binary files can be read by most computer languages, by design, meaning they are suitable for the Biomet library which contains code written in Matlab, R, and Python.\nIf you need help getting started with your “main” computer program, a sample Matlab script is provided in the quick-start guide (section 4.2, under “Sample Code”). With that DataCleaning_Main.m script the entire data processing process is documented and it can be easily replicated. Insert as many informative comments as you can - it will help if the process needs to be reviewed later on or the project is handed over to another person. When working with your own data, especially for the first time using the pipeline, we still recommend following the steps in the tutorial as they also outline where to put your raw data files, and other important tasks.\n\n\n\n\n\n\n\nDetermine the data format/source of raw data in your Sites directory.\nUse standard existing function(s) from the Biomet.net library, which you have already cloned/downloaded, to convert your raw data and create the database in Database directory. Examples of functions that can read your data and be used in your “main” script are as follows:\n\ndb_struct2database (create database from Matlab structure; flux or met data);\nfr_read_EddyPro_file (create database from EddyPro output; flux data);\nfr_read_TOA5_file (create database from TOA5 Campbell Scientific ASCII files);\nfr_read_generic_data_file (create database from most file formats).\n\n\n\n\nThe relevant function, when run in matlab, will create subdirectories named by data year (yearIn), then by SITEID (i.e., SITEIDs are grouped by year), as follows:\n\nFigure 5.2A. Directory tree showing structure of Database folder for multiple years and multiple sites under one project.\nThe SITEID subdirectories will be populated with data (e.g., Flux and Met if these data types exist) for the appropriate year, stored as binary files ready for cleaning. The green highlighting in figure 5.2A shows an example with three years of data from two different sites.\nThe clean_tv file is automatically created, and will be located in each Raw/Clean folder. This a standardized time vector of 30-minute frequency. This formats the timestamp of each record in Local Standard Time (LST; as required by AmeriFlux). \n\n\nWe advise you to generalize your scripts as far as possible, based on the sites and data that you have, and remember to document your code thoroughly.\n\n\n\n\n\n\nIn real applications, it is likely that users will have multiple input files from Flux and Met stations (daily, monthly, or annual files). It is also very often the case with active sites that new files are continuously being downloaded from the site and added to the data folder.\nTo facilitate data processing in those cases we use specialized functions that can deal with multiple files and keep track of which files have changes since the last database update and update only those. That way raw data folders can contain thousands of files without affecting the speed of the database updates.\nUpdating Flux data\nHere is an example of how to update database from any files in the Sites/siteID/Flux folder that match a file pattern (in this case eddypro_TUT_*.csv).\n%% Processing multiple EddyPro files from the same folder \nprojectPath = 'E:\\Pipeline_Projects\\TUT Project'; \nstructProject = set_TAB_project(projectPath); \nsiteID = 'TUT'; \nFluxDatabase_Pth = fullfile(structProject.databasePath, 'yyyy', siteID,'Flux'); \nprogressListPth = fullfile(structProject.sitesPath,siteID,'Flux','EddyPro_progressList.mat'); \nfilePattern = fullfile(structProject.sitesPath,siteID,'Flux','eddypro_TUT_*.csv'); \noptionsFileRead.flagFileType = 'fulloutput'; \nmissingPointValue = NaN; \ntimeUnit = '30MIN'; \n[nFiles,nHHours]=fr_EddyPro_database(filePattern,progressListPth,FluxDatabase_Pth,[],timeUnit,missingPointValue,optionsFileRead); \nfprintf('  %s - Flux  Number of files processed = %d, Number of HHours = %d\\n',siteID,nFiles,nHHours); \n\n\nUpdating Met data\nIf we then wanted to process met data from Sites/siteID/Met folder (using the same projectPath, structProject and siteID) with the file-name pattern matching: TUT_MET.* we could run the following script:\n%% Processing multiple TOA5 files from the same folder \nMetDatabase_Pth = fullfile(structProject.databasePath, 'yyyy', siteID,'Met'); \nprogressListPth = fullfile(structProject.sitesPath,siteID,'Met','Met_progressList.mat'); \nfilePattern = fullfile(structProject.sitesPath,siteID,'Met','TUT_MET.*'); \nmissingPointValue = NaN; \ntimeUnit = '30MIN'; \n[nFiles,nHHours]=fr_TOA5_database(filePattern,progressListPth,MetDatabase_Pth,[],timeUnit,missingPointValue);  \nfprintf('  %s - Met  Number of files processed = %d, Number of HHours = %d\\n',siteID,nFiles,nHHours); \n\n If we run the same sequence twice (fr_TOA5_database or fr_EddyPro_database as shown above), the second time the number of files processed would be zero assuming that there were no new files added to the Sites folder between the two runs. If on the other hand, one of the files changed, the function would process it. That is how real-time data gets processed in this data pipeline.\nOther examples of Biomet.net functions that update data continuously, and can be written into a script as previously shown, are as follows:\n\nfr_HOBO_database (continuously create database from HOBO met data);\nfr_SmartFlux_database (continuously create database from SmartFlux output; flux data);\nfr_SoilFluxPro_database (continuously create database from SoilFluxPro output data).\n\n\n\n\n\n\n\n\nData from Canadian meteorological stations [operational climate stations run by Environment and Climate Change Canada (ECCC)] can be used for gap-filling met data for Canadian flux sites that are in close proximity. If you have gaps in your met data, you can search for nearby climate stations as follows:\n\nTo find the closest Environment ECCC stations to your flux site, first go to this website: https://climate.weather.gc.ca/historical_data/search_historic_data_e.html\nClick on the “Search by Proximity” tab (figure 5.2B), select “location coordinates in Decimal Degrees” and type in the coordinates of your flux site:\n\nFigure 5.2B. “Search by Proximity” tab on ECCC Historical Data website.\nWe advise selecting a station no more than 25 km away, but that is also up to the user’s judgement and their responsibility to justify.\nIt also helps to select a proper range of dates using either “with data available between” or “with data on” option.\nFrom the list of stations pick one that’s has “Hourly” data for the dates you need, and is closest to your location (see example in figure 5.2C):\n\nFigure 5.2C. Search results for selecting the station for gap-filling.\nIn this example, the closes would be “WINNIPEG THE FORKS”; click on “Go”, and make a note of the “Climate ID” number provided (figure 5.2D; in this case, the climate ID is 5023262).\n\nFigure 5.2D. Example of Hourly Data Report for Winnipeg The Forks, showing the Climate ID number.\nNext, you need the Station ID (which is different to the Climate ID). Go to this website: https://collaboration.cmc.ec.gc.ca/cmc/climate/Get_More_Data_Plus_de_donnees/; open the Station Inventory EN.csv file and search on the Climate ID number that you saved. The number in the column to the right of this is the Station ID, which in the Winnipeg example is 28051. This Station ID will be used for the data downloads.\nCreate a script using the snippet below and enter your Station ID and other relevant information (years and months). You can enter multiple station IDs at once.\n structProject = set_TAB_project(projectPath); \n stationIDs = [28051]; \n yearsIn = 2022:2023; \n monthsIn  = 1:12; \n for cntStations = 1:length(stationIDs) \n     sID = stationIDs(cntStations); \n     pathECCC = fullfile('yyyy','ECCC',num2str(sID)); \n     try \n         run_ECCC_climate_station_update(yearsIn,monthsIn,sID,structProject.databasePath)     \n     catch \n         fprintf('Error processing station: %d (year: %d, month: %d)\\n',sID,yearsIn,monthsIn(end)); \n     end \n end\nRunning this script will create the database for the given station ID, for two years (2022 and 2023) in the current project folder. The new database for the station 28051 is located here: &lt;projectPath&gt;/Database/2022/ECCC/28051/30min/ (and similarly for 2023).\nAlternatively, if a user wants to create the ECCC database just for the current year, or to keep the climate data base up to date by downloading all the months between Jan and the current month, they can use this function call: run_ECCC_climate_station_update([],1:month(datetime),28051,db_pth_root).\nOr, for example, to update only the last two months of the current year: run_ECCC_climate_station_update([],month(datetime)-1:month(datetime),28051,db_pth_root).\n\n\n\n\n\n\n\nOnce your database is created, we strongly advise you to review its contents using Biomet.net functions. Some of these are listed in the tutorial under section 4.2, and section 6 provides extensive details of visualization tools for inspecting and analyzing raw and cleaned data.\nWhen you have confirmed that all the traces you expect to see are present in your newly created Database, you are ready to start cleaning the data. First, you need to obtain the set of sample, configuration, and standardization files, described in section.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.2 &nbsp; Full Documentation: Create Database from Raw Data and Inspect Contents"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_1_Matlab_plotApp.html",
    "href": "PipelineDocumentation/6_1_Matlab_plotApp.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "plotApp is a Matlab graphical user interface (GUI) made to facilitate visualization of database traces as:\n(i) Basic time series;\n(ii) Statistical summary;\n(iii) Comparison amongst traces; and\n(iv) Comparisons amongst stages.\nWe recommend using this plotApp during database creation and data cleaning.\n\n\n\nIn Matlab, run plotApp on the command line (figure 6.1A).\n\nFigure 6.1A. How to launch the plotApp GUI in Matlab.\nThe GUI in figure 6.1B will launch; specify the folder containing the database you would like to plot by typing the folder path or by clicking the folder button and navigating to the desired folder.\n\nYou need to specify the \\Database\\ folder (immediately beneath your project folder) rather than the individual year or site folder.\nThe Year and Site will be chosen from the drop-down menus which are automatically populated based on what is available in the folder.\nIf you type in the folder path manually or paste it into the Folder field, press ‘Enter’ when complete.\nOn subsequent uses of plotApp, it will remember the last Folder that was entered.\n\n\nFigure 6.1B. plotApp GUI showing where to enter the Database folder to view and analyze data.\nNext, select the Year, Site, and Data type (Flux, Met, Clean, etc.) that you want to plot.\nFlux and Met data types is associated with Stage2 and higher.\nOnce these selections are made, the ‘Traces’ list will populate; then click on a trace to select it and plot the time series (figure 6.1C).\n\nFigure 6.1C. Example of plotApp GUI showing time series plotted after selecting data.\nYou can use the Matlab window navigation tools (i.e., zoom, pan, data tips) to explore the traces.\nTwo traces can be plotted at once for quick and easy reference, or for qualitative visual comparisons.\n\n\n\n\nThe Trace analysis drop-down menu has four options:\n(i) Single trace;\n(ii) Plot parents;\n(iii) Compare traces; and\n(iv) Compare Stages (coming soon…).\n\nOptions (i), (ii), and (iv) operate on a single trace, which is defined by the ‘Trace’ selector  (i.e., 1 or 2, upper or lower panel, respectively).\nOption (iv) compares the selected trace with its previous stage.\nOptions (iii) operates on both Trace 1 and 2.\n\nSingle Trace\nA basic summary of a single trace can be plotted using the Trace analysis drop-down menu.\n\nFor the single trace summary, first select Trace ‘1’ (upper panel) or ‘2’ (lower panel) using the up/down buttons.\nFrom the Trace analysis drop-down menu, select ‘Single trace’.\nA new window will open with the trace summary (figure 6.1D).\n\nFigure 6.1D. Example of single-trace summary in plotApp GUI.\n\nThe four panel ’Single trace’ summary includes (i) upper left – raw half-hourly data and daily average; (ii) upper right – diurnal trend using boxplots on an hourly basis; (iii) lower left – Q-Q plot; and (iv) lower right – cumulative distribution with 1st and 99th percentile, mean, and median printed.\n\nPanels (ii)–(iv) are a summary of the raw data in panel (i).\nUsers can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\n\nCompare traces\nA basic comparison of Trace ‘1’ and ‘2’ can be plotted using the Trace analysis drop-down menu and selecting ‘Compare traces’.\n\nA new window will open with the trace comparison (figure 6.1E).\nAny two traces can be compared. There are no restrictions on Site, Data type or Level, but currently the Year must be the same for each trace.\nAn error message will appear if only one trace is plotted or if Year doesn’t match.\n\nFigure 6.1E. Example of ‘Compare trace’ summary in plotApp GUI.\n\nThe four panel ‘Plot comparison’ summary includes (i) upper left – raw half-hourly residuals and their daily average; (ii) upper right – normalized cross-covariance for +/- 48 measurement intervals; (iii) scatter plot comparing the two traces; and (iv) lower right – cumulative distribution of the residuals.\n\nAs for the single-trace analysis, users can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\nPanel (ii) displays the lag with the highest cross-covariance between the two traces for data visible in panel (i).\nPanel (iii) displays the slope and intercept of the linear regression between the two traces, along with the r2 value for data visible in panel (i).\nPanel (iv) displays the root mean squared error (RMSE), mean absolute deviation (MAD), mean, and median of the residuals in panel (i).\n\nPlot parents\n\nThe Trace analysis option is meant to aid the assessment of data cleaning amongst stages. For example, if more data is removed due to data cleaning than expected, Plot parents can be used to help identify which parent trace resulted in data removal.\nThe ‘Trace’ selector (i.e., 1 or 2) defines which trace will be analyzed and displays the analysis in a new window (figure 6.1F).\n\nFigure 6.1F. Example of ‘plot parents’ display in plotApp GUI.\nFor a particular stage, ‘Plot parents’ shows the data from the current stage (orange circles, upper panel) and contrasts with the previous stage (i.e. blue circles, upper panel).\nThe minMax=[] bounds for the trace defined in the .ini file for the particular site are shown in the upper panel in yellow and purple.\nThe lower panel shows all the parents of a particular trace (i.e., those traces which include that trace as a dependent=’’ in the INI file). Note, dependents are often called using ‘tags’.\nIn the lower panel, 0 - SELF refers to data which are missing before cleaning based on dependents.\nFor all other parents, a large dot indicates a point which was removed from the selected trace which was not missing in the previous stage. A small dot indicates the data from the selected trace would be removed based on the parent but was already missing from the previous stage.\nUsers can interact with the figure using the Matlab navigation tools (e.g., zoom and pan) to view a subset of the data.\n\nIt is recommended that you only zoom/pan the top panel.\nThe x-axis of the top and bottom panels is linked, so navigating in one panel will automatically update the x-axis on the corresponding panel.\n\n\nCompare traces\nThis option in Trace analysis is intended to quickly assess what changes occurred between stages for a particular trace.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization",
      "6.1 &nbsp; Matlab plotApp"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_1_Matlab_plotApp.html#matlab-plotapp-for-in-depth-analysis",
    "href": "PipelineDocumentation/6_1_Matlab_plotApp.html#matlab-plotapp-for-in-depth-analysis",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "plotApp is a Matlab graphical user interface (GUI) made to facilitate visualization of database traces as:\n(i) Basic time series;\n(ii) Statistical summary;\n(iii) Comparison amongst traces; and\n(iv) Comparisons amongst stages.\nWe recommend using this plotApp during database creation and data cleaning.\n\n\n\nIn Matlab, run plotApp on the command line (figure 6.1A).\n\nFigure 6.1A. How to launch the plotApp GUI in Matlab.\nThe GUI in figure 6.1B will launch; specify the folder containing the database you would like to plot by typing the folder path or by clicking the folder button and navigating to the desired folder.\n\nYou need to specify the \\Database\\ folder (immediately beneath your project folder) rather than the individual year or site folder.\nThe Year and Site will be chosen from the drop-down menus which are automatically populated based on what is available in the folder.\nIf you type in the folder path manually or paste it into the Folder field, press ‘Enter’ when complete.\nOn subsequent uses of plotApp, it will remember the last Folder that was entered.\n\n\nFigure 6.1B. plotApp GUI showing where to enter the Database folder to view and analyze data.\nNext, select the Year, Site, and Data type (Flux, Met, Clean, etc.) that you want to plot.\nFlux and Met data types is associated with Stage2 and higher.\nOnce these selections are made, the ‘Traces’ list will populate; then click on a trace to select it and plot the time series (figure 6.1C).\n\nFigure 6.1C. Example of plotApp GUI showing time series plotted after selecting data.\nYou can use the Matlab window navigation tools (i.e., zoom, pan, data tips) to explore the traces.\nTwo traces can be plotted at once for quick and easy reference, or for qualitative visual comparisons.\n\n\n\n\nThe Trace analysis drop-down menu has four options:\n(i) Single trace;\n(ii) Plot parents;\n(iii) Compare traces; and\n(iv) Compare Stages (coming soon…).\n\nOptions (i), (ii), and (iv) operate on a single trace, which is defined by the ‘Trace’ selector  (i.e., 1 or 2, upper or lower panel, respectively).\nOption (iv) compares the selected trace with its previous stage.\nOptions (iii) operates on both Trace 1 and 2.\n\nSingle Trace\nA basic summary of a single trace can be plotted using the Trace analysis drop-down menu.\n\nFor the single trace summary, first select Trace ‘1’ (upper panel) or ‘2’ (lower panel) using the up/down buttons.\nFrom the Trace analysis drop-down menu, select ‘Single trace’.\nA new window will open with the trace summary (figure 6.1D).\n\nFigure 6.1D. Example of single-trace summary in plotApp GUI.\n\nThe four panel ’Single trace’ summary includes (i) upper left – raw half-hourly data and daily average; (ii) upper right – diurnal trend using boxplots on an hourly basis; (iii) lower left – Q-Q plot; and (iv) lower right – cumulative distribution with 1st and 99th percentile, mean, and median printed.\n\nPanels (ii)–(iv) are a summary of the raw data in panel (i).\nUsers can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\n\nCompare traces\nA basic comparison of Trace ‘1’ and ‘2’ can be plotted using the Trace analysis drop-down menu and selecting ‘Compare traces’.\n\nA new window will open with the trace comparison (figure 6.1E).\nAny two traces can be compared. There are no restrictions on Site, Data type or Level, but currently the Year must be the same for each trace.\nAn error message will appear if only one trace is plotted or if Year doesn’t match.\n\nFigure 6.1E. Example of ‘Compare trace’ summary in plotApp GUI.\n\nThe four panel ‘Plot comparison’ summary includes (i) upper left – raw half-hourly residuals and their daily average; (ii) upper right – normalized cross-covariance for +/- 48 measurement intervals; (iii) scatter plot comparing the two traces; and (iv) lower right – cumulative distribution of the residuals.\n\nAs for the single-trace analysis, users can interact with panel (i) using the Matlab navigation tools (zoom and pan only) to analyze a subset of the data.\nAfter zooming/panning, panels (ii)–(iv) will automatically update.\nTo restore the view to the whole data set, with zoom out selected, double-click panel (i). Alternately, right click on panel (i) and select ‘Restore view’ from the drop-down.\nPanel (ii) displays the lag with the highest cross-covariance between the two traces for data visible in panel (i).\nPanel (iii) displays the slope and intercept of the linear regression between the two traces, along with the r2 value for data visible in panel (i).\nPanel (iv) displays the root mean squared error (RMSE), mean absolute deviation (MAD), mean, and median of the residuals in panel (i).\n\nPlot parents\n\nThe Trace analysis option is meant to aid the assessment of data cleaning amongst stages. For example, if more data is removed due to data cleaning than expected, Plot parents can be used to help identify which parent trace resulted in data removal.\nThe ‘Trace’ selector (i.e., 1 or 2) defines which trace will be analyzed and displays the analysis in a new window (figure 6.1F).\n\nFigure 6.1F. Example of ‘plot parents’ display in plotApp GUI.\nFor a particular stage, ‘Plot parents’ shows the data from the current stage (orange circles, upper panel) and contrasts with the previous stage (i.e. blue circles, upper panel).\nThe minMax=[] bounds for the trace defined in the .ini file for the particular site are shown in the upper panel in yellow and purple.\nThe lower panel shows all the parents of a particular trace (i.e., those traces which include that trace as a dependent=’’ in the INI file). Note, dependents are often called using ‘tags’.\nIn the lower panel, 0 - SELF refers to data which are missing before cleaning based on dependents.\nFor all other parents, a large dot indicates a point which was removed from the selected trace which was not missing in the previous stage. A small dot indicates the data from the selected trace would be removed based on the parent but was already missing from the previous stage.\nUsers can interact with the figure using the Matlab navigation tools (e.g., zoom and pan) to view a subset of the data.\n\nIt is recommended that you only zoom/pan the top panel.\nThe x-axis of the top and bottom panels is linked, so navigating in one panel will automatically update the x-axis on the corresponding panel.\n\n\nCompare traces\nThis option in Trace analysis is intended to quickly assess what changes occurred between stages for a particular trace.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization",
      "6.1 &nbsp; Matlab plotApp"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_Data_Visualization.html",
    "href": "PipelineDocumentation/6_Data_Visualization.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Previously, we briefly mentioned some ways to visualize your data (section 4.2). This current section gives details on our recommended methods for viewing and analyzing your data, both during cleaning, and once you have real-time data flowing.\n\n\nThere are several Matlab functions for visualizing data:\n\nplotApp: a Matlab graphical user interface (GUI) which we recommend using during database set up and cleaning of your data (see section 4.2 for details).\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons. A working example is provided in section 4.2.\n\nguiPlotTraces: this is an older function with less utility than the plotApp GUI, but it still displays your data for quick viewing.\n\n\n\n\nThere is also an R Shiny App which is useful for viewing your flux data in real-time (see section 6.2 for details). You can host this on an RShiny server to make your data publicly viewable.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_Data_Visualization.html#data-visualization",
    "href": "PipelineDocumentation/6_Data_Visualization.html#data-visualization",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Previously, we briefly mentioned some ways to visualize your data (section 4.2). This current section gives details on our recommended methods for viewing and analyzing your data, both during cleaning, and once you have real-time data flowing.\n\n\nThere are several Matlab functions for visualizing data:\n\nplotApp: a Matlab graphical user interface (GUI) which we recommend using during database set up and cleaning of your data (see section 4.2 for details).\n\ngui_Browse_Folder: given a filepath, this function will open a new figure window and provide a dropdown containing all traces located at that filepath location. You can pick one or scroll through using the arrow buttons. A working example is provided in section 4.2.\n\nguiPlotTraces: this is an older function with less utility than the plotApp GUI, but it still displays your data for quick viewing.\n\n\n\n\nThere is also an R Shiny App which is useful for viewing your flux data in real-time (see section 6.2 for details). You can host this on an RShiny server to make your data publicly viewable.",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html",
    "href": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Before completing this step, you must have the Biomet.net repository cloned to your local computer (section 2.2). This step is important for ensuring the pipeline scripts and tools run successfully on your local computer.\n\n\n\nOnce Biomet.net is successfully cloned, open Matlab.\nIn the “Home” tab, click “Set Path” menu option. Figure 2.4A shows what your screen should be displaying when you do this, for both Windows and Mac users:\n\nFigure 2.4A. View of the Set Path menu option, for (a) Windows and (b) Mac.\n\nSelect “Add Folder…” and navigate to Biomet.net/matlab/Startup (Figure 2.4B). Click “Select Folder” (PC) or “Open” (Mac):\n\nFigure 2.4B. View of Add Folder to Path menu option, adding the Startup directory, for (a) Windows and (b) Mac.\n\nAt this point the Matlab path should look very similar to Figure 2.4C: only one folder in the path does not belong to Matlab nor the /user/Documents/Matlab folder, and that is the Startup folder you just added.\nNext, click “Save” to save these settings for the future. We recommend that no other folders are saved in the default path to avoid unintentional consequences when running the data cleaning. If the saving fails, if possible, you should restart Matlab in admin mode and repeat the process (it is often a permissions issue at this point; see Troubleshooting section 7 for more information).\n\nFigure 2.4C. View of Matlab path after adding the Startup directory but before restarting Matlab, for (a) Windows and (b) Mac.\n\nNow, you can restart Matlab. Following the restart, check the Matlab path (click “Set Path”) and it should have the Biomet.net library included, as shown in Figure 2.4D. In some instances you might also see “UBC_PC_Setup”, but this may not appear in all installations - this will be automatically added only when required. Once confirmed, you can close this window, taking care not to change anything further.\n\nFigure 2.4D. View of Matlab path after adding the Startup directory, and after restarting Matlab, for (a) Windows and (b) Mac.\n\n\nMatlab is now ready to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.4 &nbsp; Configure Matlab for Biomet.net"
    ]
  },
  {
    "objectID": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html#configure-matlab-for-biomet.net-library",
    "href": "PipelineDocumentation/2_4_Configure_Matlab_For_Biomet.html#configure-matlab-for-biomet.net-library",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "Before completing this step, you must have the Biomet.net repository cloned to your local computer (section 2.2). This step is important for ensuring the pipeline scripts and tools run successfully on your local computer.\n\n\n\nOnce Biomet.net is successfully cloned, open Matlab.\nIn the “Home” tab, click “Set Path” menu option. Figure 2.4A shows what your screen should be displaying when you do this, for both Windows and Mac users:\n\nFigure 2.4A. View of the Set Path menu option, for (a) Windows and (b) Mac.\n\nSelect “Add Folder…” and navigate to Biomet.net/matlab/Startup (Figure 2.4B). Click “Select Folder” (PC) or “Open” (Mac):\n\nFigure 2.4B. View of Add Folder to Path menu option, adding the Startup directory, for (a) Windows and (b) Mac.\n\nAt this point the Matlab path should look very similar to Figure 2.4C: only one folder in the path does not belong to Matlab nor the /user/Documents/Matlab folder, and that is the Startup folder you just added.\nNext, click “Save” to save these settings for the future. We recommend that no other folders are saved in the default path to avoid unintentional consequences when running the data cleaning. If the saving fails, if possible, you should restart Matlab in admin mode and repeat the process (it is often a permissions issue at this point; see Troubleshooting section 7 for more information).\n\nFigure 2.4C. View of Matlab path after adding the Startup directory but before restarting Matlab, for (a) Windows and (b) Mac.\n\nNow, you can restart Matlab. Following the restart, check the Matlab path (click “Set Path”) and it should have the Biomet.net library included, as shown in Figure 2.4D. In some instances you might also see “UBC_PC_Setup”, but this may not appear in all installations - this will be automatically added only when required. Once confirmed, you can close this window, taking care not to change anything further.\n\nFigure 2.4D. View of Matlab path after adding the Startup directory, and after restarting Matlab, for (a) Windows and (b) Mac.\n\n\nMatlab is now ready to work with the Biomet.net library.",
    "crumbs": [
      "Pipeline Documentation",
      "2. Software Installation",
      "2.4 &nbsp; Configure Matlab for Biomet.net"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_1_Full_Doc_Setup_Project_Directory_Structure_Details.html",
    "href": "PipelineDocumentation/5_1_Full_Doc_Setup_Project_Directory_Structure_Details.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides the same instructions as the quick-start set up in section 4.1, but in much greater detail.\nOn this page:\n\nSetting up your project directory structure and configure Matlab to work with Biomet.net\nConfigure Matlab for directory structure\n\n\n\n\n\n\nEach user should have at least one Project directory which encompasses data cleaning/analysis/research for a group of similar flux sites, e.g., FLUXNET-Canada-V2.0. You will need the full file path to this Project directory, and also you will need to create a site IDs(s) for your flux site(s), if they do not already exist. Site IDs are usually acronyms based on the site name.\nImportant notes:\n\nWe enforce using uppercase for site IDs, to avoid problems with running data cleaning on Mac vs. Windows. Examples: the Delta Salt Marsh site ID is DSM; the Turkey Point Agriculture site ID is TPAG.\nSite IDs must also be a valid Matlab variable name: they should start with a letter, can only be alphanumeric, and are allowed to contain underscores but no other special characters (no hyphens!), e.g., PE_TNR.\nYour project directory must be on your local computer: avoid putting it on an external drive or cloud drive (like OneDrive or GDrive) as this will cause problems in the steps that follow. Additionally, avoid putting it in the Biomet.net directory.\nYour &lt;projectPath&gt;, which you will need when following the instructions, is the full filepath to your Project root directory. E.g., if your project is called My_Micromet, the project path would be similar to /Users/&lt;username&gt;/Projects/My_Micromet/.\n\nWe provide functions within the Biomet library to help you set up the project directory structure and configure Matlab:\n\n\n\nIf you have multiple flux sites, choose one site to begin working with. Using the Matlab command line, define your projectPath and siteID as in the following example:\nFor example, if the name of your project is My_Micromet, and your chosen siteID is SITEID1, you would enter the following commands in the Matlab command window:\nMac example:\n projectPath = '/Users/&lt;username&gt;/Projects/My_Micromet/';\n siteID = 'SITEID1';\nPC example:\n projectPath = 'C:\\Projects\\My_Micromet\\';\n siteID = 'SITEID1';\nNext, in Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned or downloaded), as follows:\n create_TAB_ProjectFolders(projectPath,siteID)\nBoth input arguments are of type “string”. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info. \n\nInstructions continue below (Configure Matlab for directory structure) following these details on the create_TAB_ProjectFolders function, which takes care of several steps for you, outlined next:\n\nIn your Project root directory (My_Micromet in figure 5.1A), you should now see three directories with the following names: (1) Database, (2) Matlab, (3) Sites. In this directory tree example, there are three project root directories to illustrate the structure in case you have more than one project: My_Micromet (highlighted in green), CH4_V2.0, and FLUXNET-Canada-V2.0; although you may only have one.\n\nFigure 5.1A Directory tree showing contents of My_Micromet project folder after running the create_TAB_ProjectFolders Matlab function; also shown is the structure if there are multiple projects, e.g., My_Micromet, CH4_V2.0, FLUXNET-Canada-V2.0.\n\nIf you do not see these contents, and/or Matlab gives you an error on running create_TAB_ProjectFolders, you should check which version of Matlab you are using. If you are working with an earlier version of Matlab than 2023b, this is likely the cause of the issue. In this case, download this zip file, unzip, and put the contents of “My_Micromet_Folder” within your project directory (underneath My_Micromet); then make sure your directory structure looks like figure 4.1.\nNOTE: The directory information given next is simply descriptions of the contents of these folders and their purpose. The next instructions to follow are below under “Configure Matlab for Directory Structure”.\n\n\n\nWithin the new Database directory that you just made, you will find another new directory called Calculation_Procedures, and within Calculation_Procedures, there should be two subdirectories: AmeriFlux and TraceAnalysis_ini. Eventually your initial database and cleaned data will go here (instructions are in future sections).\n\nIn the Ameriflux folder, the create_TAB_ProjectFolders.m function has cloned a git repository containing necessary files (figure 5.1B, highlighted in blue) that will be needed later if you want to convert your cleaned data into AmeriFlux format.\n\nFigure 5.1B. Directory tree showing contents of AmeriFlux directory after running the create_TAB_ProjectFolders Matlab function.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your SITEID (SITEID1 in this example, figure 5.1C), as follows:\n\nFigure 5.1C. Directory tree showing contents of TraceAnalysis_ini directory after running the create_TAB_ProjectFolders Matlab function.\nNow, if you navigate to that SITEID1 directory, you should see two more folders: Derived_Variables and log (figure 5.1D). Your site-specific INI files will eventually go into this SITEID1 directory.\n\nFigure 5.1D. Directory tree showing contents of SITEID1 directory within TraceAnalysis_ini, after running the create_TAB_ProjectFolders Matlab function.\nWithin the TraceAnalysis_ini folder, you should also find a series of “include” files (that were cloned by the create_TAB_ProjectFolders function; figure 5.1C); these are important, and their purpose is described in detail in section 5.3.\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory (Figure 5.1E), under the appropriate siteID. The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 5.1E. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created “behind the scenes” in step 2 above;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes and may also be useful to you later when you want to visualize your data.\nOnce you have confirmed all files and folders appear as described above, you can now move on to the next step which configures Matlab for the directory structure you just created.\n\n\n\n\n\n\n\n\n\n(Step 3 in quick-start guide; section 4.1) Run\nset_TAB_project(projectPath)\n\nThis process sets up the Biomet.net toolbox to work with your project.\nIf you wish to, you can add the name of your project, for documentation purposes only, to the new file get_TAB_project_configuration.m which was created when running set_TAB_project(projectPath) in step 3.\n\nOptional final step: open (but do not run) get_TAB_project_configuration.m, located in /&lt;projectPath&gt;/Matlab/ and enter your projectName (user-preferred name, this is just for documentation purposes and not used for processing). The file should look like this:\nfunction structProject = get_TAB_project_configuration  (projectPath)\nprojectName = 'My_Micromet project is the best';\nstructProject.projectName   = projectName;\nstructProject.path          = fullfile(projectPath);\nstructProject.databasePath  = fullfile(structProject.path,'Database');\nstructProject.sitesPath     = fullfile(structProject.path,'Sites');\nstructProject.matlabPath    = fullfile(structProject.path,'Matlab');\n[In case you do not have this script, you can simply copy this code and edit the string assigned to projectName.]\nMatlab is now ready to run data processing and cleaning for that particular project.\n\nAt this point, your Matlab directory should contain the following files:\n\nThe two files not previously discussed, i.e., biomet_database_default.m and biomet_sites_default.m were automatically generated by set_TAB_project.m. If you run either of these on your command line in Matlab, you will see that it returns the full file path to either your Database or your Sites directory, respectively.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.1 &nbsp; Full Documentation: Project Directory Structure and Matlab Configuration"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_1_Full_Doc_Setup_Project_Directory_Structure_Details.html#full-documentation-project-directory-structure-and-matlab-configuration",
    "href": "PipelineDocumentation/5_1_Full_Doc_Setup_Project_Directory_Structure_Details.html#full-documentation-project-directory-structure-and-matlab-configuration",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides the same instructions as the quick-start set up in section 4.1, but in much greater detail.\nOn this page:\n\nSetting up your project directory structure and configure Matlab to work with Biomet.net\nConfigure Matlab for directory structure\n\n\n\n\n\n\nEach user should have at least one Project directory which encompasses data cleaning/analysis/research for a group of similar flux sites, e.g., FLUXNET-Canada-V2.0. You will need the full file path to this Project directory, and also you will need to create a site IDs(s) for your flux site(s), if they do not already exist. Site IDs are usually acronyms based on the site name.\nImportant notes:\n\nWe enforce using uppercase for site IDs, to avoid problems with running data cleaning on Mac vs. Windows. Examples: the Delta Salt Marsh site ID is DSM; the Turkey Point Agriculture site ID is TPAG.\nSite IDs must also be a valid Matlab variable name: they should start with a letter, can only be alphanumeric, and are allowed to contain underscores but no other special characters (no hyphens!), e.g., PE_TNR.\nYour project directory must be on your local computer: avoid putting it on an external drive or cloud drive (like OneDrive or GDrive) as this will cause problems in the steps that follow. Additionally, avoid putting it in the Biomet.net directory.\nYour &lt;projectPath&gt;, which you will need when following the instructions, is the full filepath to your Project root directory. E.g., if your project is called My_Micromet, the project path would be similar to /Users/&lt;username&gt;/Projects/My_Micromet/.\n\nWe provide functions within the Biomet library to help you set up the project directory structure and configure Matlab:\n\n\n\nIf you have multiple flux sites, choose one site to begin working with. Using the Matlab command line, define your projectPath and siteID as in the following example:\nFor example, if the name of your project is My_Micromet, and your chosen siteID is SITEID1, you would enter the following commands in the Matlab command window:\nMac example:\n projectPath = '/Users/&lt;username&gt;/Projects/My_Micromet/';\n siteID = 'SITEID1';\nPC example:\n projectPath = 'C:\\Projects\\My_Micromet\\';\n siteID = 'SITEID1';\nNext, in Matlab, run the create_TAB_ProjectFolders function (which is in the Biomet.net library that you cloned or downloaded), as follows:\n create_TAB_ProjectFolders(projectPath,siteID)\nBoth input arguments are of type “string”. You can use single '' or double \"\" quotation marks for these purposes in Matlab, although technically, these mean slightly different things; see this link for more info. \n\nInstructions continue below (Configure Matlab for directory structure) following these details on the create_TAB_ProjectFolders function, which takes care of several steps for you, outlined next:\n\nIn your Project root directory (My_Micromet in figure 5.1A), you should now see three directories with the following names: (1) Database, (2) Matlab, (3) Sites. In this directory tree example, there are three project root directories to illustrate the structure in case you have more than one project: My_Micromet (highlighted in green), CH4_V2.0, and FLUXNET-Canada-V2.0; although you may only have one.\n\nFigure 5.1A Directory tree showing contents of My_Micromet project folder after running the create_TAB_ProjectFolders Matlab function; also shown is the structure if there are multiple projects, e.g., My_Micromet, CH4_V2.0, FLUXNET-Canada-V2.0.\n\nIf you do not see these contents, and/or Matlab gives you an error on running create_TAB_ProjectFolders, you should check which version of Matlab you are using. If you are working with an earlier version of Matlab than 2023b, this is likely the cause of the issue. In this case, download this zip file, unzip, and put the contents of “My_Micromet_Folder” within your project directory (underneath My_Micromet); then make sure your directory structure looks like figure 4.1.\nNOTE: The directory information given next is simply descriptions of the contents of these folders and their purpose. The next instructions to follow are below under “Configure Matlab for Directory Structure”.\n\n\n\nWithin the new Database directory that you just made, you will find another new directory called Calculation_Procedures, and within Calculation_Procedures, there should be two subdirectories: AmeriFlux and TraceAnalysis_ini. Eventually your initial database and cleaned data will go here (instructions are in future sections).\n\nIn the Ameriflux folder, the create_TAB_ProjectFolders.m function has cloned a git repository containing necessary files (figure 5.1B, highlighted in blue) that will be needed later if you want to convert your cleaned data into AmeriFlux format.\n\nFigure 5.1B. Directory tree showing contents of AmeriFlux directory after running the create_TAB_ProjectFolders Matlab function.\nWithin TraceAnalysis_ini, you will see a subdirectory named using your SITEID (SITEID1 in this example, figure 5.1C), as follows:\n\nFigure 5.1C. Directory tree showing contents of TraceAnalysis_ini directory after running the create_TAB_ProjectFolders Matlab function.\nNow, if you navigate to that SITEID1 directory, you should see two more folders: Derived_Variables and log (figure 5.1D). Your site-specific INI files will eventually go into this SITEID1 directory.\n\nFigure 5.1D. Directory tree showing contents of SITEID1 directory within TraceAnalysis_ini, after running the create_TAB_ProjectFolders Matlab function.\nWithin the TraceAnalysis_ini folder, you should also find a series of “include” files (that were cloned by the create_TAB_ProjectFolders function; figure 5.1C); these are important, and their purpose is described in detail in section 5.3.\n\n\n\n\n\nRaw, uncleaned data from your site(s) will be stored in the &lt;projectPath&gt;/Sites directory (Figure 5.1E), under the appropriate siteID. The data in this directory should remain untouched since we always want to preserve a copy of the raw data.\n\nFigure 5.1E. Directory tree showing contents of Sites folder after running the create_TAB_ProjectFolders Matlab function.\n\n\n\n\n\nThis directory should now contain three matlab files:\n\n\nget_TAB_project_configuration.m which was created “behind the scenes” in step 2 above;\n\n\nbiomet_database_default.m; and\n\n\nbiomet_sites_default.m.\n\n\n\nThe latter two functions are used in the pipeline behind the scenes and may also be useful to you later when you want to visualize your data.\nOnce you have confirmed all files and folders appear as described above, you can now move on to the next step which configures Matlab for the directory structure you just created.\n\n\n\n\n\n\n\n\n\n(Step 3 in quick-start guide; section 4.1) Run\nset_TAB_project(projectPath)\n\nThis process sets up the Biomet.net toolbox to work with your project.\nIf you wish to, you can add the name of your project, for documentation purposes only, to the new file get_TAB_project_configuration.m which was created when running set_TAB_project(projectPath) in step 3.\n\nOptional final step: open (but do not run) get_TAB_project_configuration.m, located in /&lt;projectPath&gt;/Matlab/ and enter your projectName (user-preferred name, this is just for documentation purposes and not used for processing). The file should look like this:\nfunction structProject = get_TAB_project_configuration  (projectPath)\nprojectName = 'My_Micromet project is the best';\nstructProject.projectName   = projectName;\nstructProject.path          = fullfile(projectPath);\nstructProject.databasePath  = fullfile(structProject.path,'Database');\nstructProject.sitesPath     = fullfile(structProject.path,'Sites');\nstructProject.matlabPath    = fullfile(structProject.path,'Matlab');\n[In case you do not have this script, you can simply copy this code and edit the string assigned to projectName.]\nMatlab is now ready to run data processing and cleaning for that particular project.\n\nAt this point, your Matlab directory should contain the following files:\n\nThe two files not previously discussed, i.e., biomet_database_default.m and biomet_sites_default.m were automatically generated by set_TAB_project.m. If you run either of these on your command line in Matlab, you will see that it returns the full file path to either your Database or your Sites directory, respectively.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.1 &nbsp; Full Documentation: Project Directory Structure and Matlab Configuration"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_4_Full_Doc_Second_Stage_INI_Files.html",
    "href": "PipelineDocumentation/5_4_Full_Doc_Second_Stage_INI_Files.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nGeneral outline for creating your second stage INI file\nRunning second stage cleaning in Matlab\nProperties and parameters: second stage INI\nSecond stage main features\nProgramming syntax rules for first and second stage INI files\n\n\n\n\n\n\nAs with the first stage cleaning, if you are doing this for the first time, we recommend that you obtain the simplified template files from the quick-start instructions in section 4 and follow the tutorial. The steps to create your second stage INI are very similar to the first stage:\n\nUsing your duplicated second stage INI file or downloaded template file, rename it using your unique measurement site ID. For example, the first stage INI file for a site with siteID = ‘DSM’ is named DSM_FirstStage.ini.\nEdit second stage INI file adding just a few variables at a time (as with the first stage) and then testing, so it’s easy to diagnose errors/typos. Pay attention to the output in your Matlab command window; it is informative and highlights any issues and where they occur. Recall the data cleaning principles previously outlined and make sure your traces align with the second stage principles.\n\n\n\n\n\n\n\nOnce you have added a few variables to the second stage INI, test it using the same Matlab command you used for the first stage cleaning, but entering ‘2’ as the final argument (instead of ‘1’):\nfr_automated_cleaning(yearIn,'SITEID',2)\nEventually, you can run the first and second stage together, as follows:\nfr_automated_cleaning(yearIn,'SITEID',[1 2])\n…but initially this makes it harder to troubleshoot.\n\n\n\n\n\n\nTable 5.2. Second stage INI file properties and parameters.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nSite_name\nName of the site. Any text can go here.\n\n\nSiteID\nThis is the name attributed to the site in the database (e.g., DSM or BB).\n\n\ninput_path\nThis can stay blank.\n\n\noutput_path\nThe local output path.\n\n\nhigh_level_path\nLeave blank:{}. It used to indicate Met/Flux, etc.\n\n\nsearchPath\nAll traces on this path(s) will be loaded up and available in the SecondStage cleaning. Syntax: use ‘auto’ or use specific folders to limit or to expand the (example: ‘Flux,Met,Flags’). When option ‘auto’ is used, all the traces created by the FirstStage cleaning will be automatically loaded before the SecondStage cleaning starts.\n\n\n[Trace]\nMarks the beginning of a new variable. The section has to end with the keyword &lt;[End]&gt;.\n\n\nvariableName\nName of variable for second stage, again following Ameriflux format. The variable with the name created here will show up in the /Database/yyyy/SITEID/Clean/SecondStage folder, where yyyy is the year that the data is valid for.\n\n\nEvaluate\nUser defined function. If no function is applied, default input will just pass the variable from the first stage to the second stage, e.g., Evaluate = 'TKE = TKE;'. Use the calc_avg_trace function (described above) to average or gap-fill met variables (such as air temperature) with values from secondary measurements or nearby sites (you will need to load these into the first stage before using them in this second stage). See sample file DSM_SecondStage.ini for more use cases.\n\n\ntitle\nDescriptive variable name for plots/visualization.\n\n\nunits\nUnits for this current trace, e.g. 'W/m^2'\n\n\n\n\n\n\n\n\n\nCombining multiple sensor measurements into one trace. This can be done in different ways using calc_avg_trace.m, which combines two or more traces from the first stage (i.e., already deemed “good” sensor measurements). You can also combine these three methods:\n\n\nAveraging multiple sensors to remove variability;\nUsing one sensor as the best (most accurate value) and using the other sensor(s) only to fill in the missing values. A relationship can be created between the “best” sensor and its “replacement” and that relationship can be applied to the “replacement” values to improve the accuracy;\nUsing sensors from another near-by site to fill in the missing values at the current site.\nArguments for calc_avg_trace function:\n\n\n\n\n\n\n\n\nField\nDescription\nType\n\n\n\n\ntv\ninput the time vector clean_tv that has already been created in the first stage cleaning\nn/a\n\n\ndata_in\nthe name of the trace output from stage one, to be filled or “improved” by combining/averaging with another trace or traces\nstring uppercase\n\n\ndata_fill\nthe name of the traces used to average/gap-fill “data_in” trace; these “data_fill” traces must have been included in the first stage\ninteger\n\n\navg_period\n\n\n\n\n\n\n\nAs with the first stage, more complex user-defined processing can be applied to the trace using the “Evaluate” option. User written Matlab functions can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” string. \n\n\n\n\n\n\n\nSome programming rules that you must follow for the first and second stage INI files to be successfully read by the pipeline scripts:\n\n1. We enforce using uppercase for site IDs to avoid problems with running data cleaning on Mac vs. Windows.\n2. All traces must be enclosed in [Trace] and [End] blocks.\n3. All assignments can be on multiple lines but should be enclosed in single quotes.\n4. Comments must begin with a percentage sign (%).\n5. All fields must be in Matlab format.\n6. All parameter assignments must be to strings in single quotes, or numeric expressions, e.g., threshold_const = 6, threshold_const = [6], variableName = 'Some Name'.\n7. For the first stage, the partial path must be included with the inputFileName when you locate the raw data trace in the database. (Using biomet_path function only returns the path: /year/SITEID/measType/)\n8. First stage necessary fields are: variableName, inputFileName, measurementType, units, title, and minMax.\n9. Second stage necessary fields are: variableName, title, units.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.4 &nbsp; Full Documentation: Second Stage INI Files"
    ]
  },
  {
    "objectID": "PipelineDocumentation/5_4_Full_Doc_Second_Stage_INI_Files.html#full-documentation-second-stage-ini-files",
    "href": "PipelineDocumentation/5_4_Full_Doc_Second_Stage_INI_Files.html#full-documentation-second-stage-ini-files",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nGeneral outline for creating your second stage INI file\nRunning second stage cleaning in Matlab\nProperties and parameters: second stage INI\nSecond stage main features\nProgramming syntax rules for first and second stage INI files\n\n\n\n\n\n\nAs with the first stage cleaning, if you are doing this for the first time, we recommend that you obtain the simplified template files from the quick-start instructions in section 4 and follow the tutorial. The steps to create your second stage INI are very similar to the first stage:\n\nUsing your duplicated second stage INI file or downloaded template file, rename it using your unique measurement site ID. For example, the first stage INI file for a site with siteID = ‘DSM’ is named DSM_FirstStage.ini.\nEdit second stage INI file adding just a few variables at a time (as with the first stage) and then testing, so it’s easy to diagnose errors/typos. Pay attention to the output in your Matlab command window; it is informative and highlights any issues and where they occur. Recall the data cleaning principles previously outlined and make sure your traces align with the second stage principles.\n\n\n\n\n\n\n\nOnce you have added a few variables to the second stage INI, test it using the same Matlab command you used for the first stage cleaning, but entering ‘2’ as the final argument (instead of ‘1’):\nfr_automated_cleaning(yearIn,'SITEID',2)\nEventually, you can run the first and second stage together, as follows:\nfr_automated_cleaning(yearIn,'SITEID',[1 2])\n…but initially this makes it harder to troubleshoot.\n\n\n\n\n\n\nTable 5.2. Second stage INI file properties and parameters.\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nSite_name\nName of the site. Any text can go here.\n\n\nSiteID\nThis is the name attributed to the site in the database (e.g., DSM or BB).\n\n\ninput_path\nThis can stay blank.\n\n\noutput_path\nThe local output path.\n\n\nhigh_level_path\nLeave blank:{}. It used to indicate Met/Flux, etc.\n\n\nsearchPath\nAll traces on this path(s) will be loaded up and available in the SecondStage cleaning. Syntax: use ‘auto’ or use specific folders to limit or to expand the (example: ‘Flux,Met,Flags’). When option ‘auto’ is used, all the traces created by the FirstStage cleaning will be automatically loaded before the SecondStage cleaning starts.\n\n\n[Trace]\nMarks the beginning of a new variable. The section has to end with the keyword &lt;[End]&gt;.\n\n\nvariableName\nName of variable for second stage, again following Ameriflux format. The variable with the name created here will show up in the /Database/yyyy/SITEID/Clean/SecondStage folder, where yyyy is the year that the data is valid for.\n\n\nEvaluate\nUser defined function. If no function is applied, default input will just pass the variable from the first stage to the second stage, e.g., Evaluate = 'TKE = TKE;'. Use the calc_avg_trace function (described above) to average or gap-fill met variables (such as air temperature) with values from secondary measurements or nearby sites (you will need to load these into the first stage before using them in this second stage). See sample file DSM_SecondStage.ini for more use cases.\n\n\ntitle\nDescriptive variable name for plots/visualization.\n\n\nunits\nUnits for this current trace, e.g. 'W/m^2'\n\n\n\n\n\n\n\n\n\nCombining multiple sensor measurements into one trace. This can be done in different ways using calc_avg_trace.m, which combines two or more traces from the first stage (i.e., already deemed “good” sensor measurements). You can also combine these three methods:\n\n\nAveraging multiple sensors to remove variability;\nUsing one sensor as the best (most accurate value) and using the other sensor(s) only to fill in the missing values. A relationship can be created between the “best” sensor and its “replacement” and that relationship can be applied to the “replacement” values to improve the accuracy;\nUsing sensors from another near-by site to fill in the missing values at the current site.\nArguments for calc_avg_trace function:\n\n\n\n\n\n\n\n\nField\nDescription\nType\n\n\n\n\ntv\ninput the time vector clean_tv that has already been created in the first stage cleaning\nn/a\n\n\ndata_in\nthe name of the trace output from stage one, to be filled or “improved” by combining/averaging with another trace or traces\nstring uppercase\n\n\ndata_fill\nthe name of the traces used to average/gap-fill “data_in” trace; these “data_fill” traces must have been included in the first stage\ninteger\n\n\navg_period\n\n\n\n\n\n\n\nAs with the first stage, more complex user-defined processing can be applied to the trace using the “Evaluate” option. User written Matlab functions can be called from this statement. Multiple Matlab statements can be called from within the “Evaluate” string. \n\n\n\n\n\n\n\nSome programming rules that you must follow for the first and second stage INI files to be successfully read by the pipeline scripts:\n\n1. We enforce using uppercase for site IDs to avoid problems with running data cleaning on Mac vs. Windows.\n2. All traces must be enclosed in [Trace] and [End] blocks.\n3. All assignments can be on multiple lines but should be enclosed in single quotes.\n4. Comments must begin with a percentage sign (%).\n5. All fields must be in Matlab format.\n6. All parameter assignments must be to strings in single quotes, or numeric expressions, e.g., threshold_const = 6, threshold_const = [6], variableName = 'Some Name'.\n7. For the first stage, the partial path must be included with the inputFileName when you locate the raw data trace in the database. (Using biomet_path function only returns the path: /year/SITEID/measType/)\n8. First stage necessary fields are: variableName, inputFileName, measurementType, units, title, and minMax.\n9. Second stage necessary fields are: variableName, title, units.",
    "crumbs": [
      "Pipeline Documentation",
      "5. Full Documentation: Features, Details, and Other Useful Information for Advanced Users",
      "5.4 &nbsp; Full Documentation: Second Stage INI Files"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_2_RShiny_App.html",
    "href": "PipelineDocumentation/6_2_RShiny_App.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "We recommend using this R-Shiny app for viewing your data in real-time, and if you wish to display your data publicly.\n\n\nInstructions for using the RShiny_flux_plots.R app (under /Biomet.net/R/RShiny_flux_plots/). To run the R-Shiny app, either locally or on an R-Shiny server, you will need to download, edit, rename and save two files.\n\nDownload the Excel sheet below to input your site coordinates (you can include multiple sites, each as a separate line):\nExample site coordinate file\n\nNote that the standard meridian is equal to your [UTC offset (without daylight savings) x 15]. For example, Vancouver is UTC-8 so the standard meridian is -120.\nAlso, latitude and longitude are in decimal degrees, and negative for West and South coordinates.\n\nRename the Excel file to include your specific Project Name (in place of “TUT” in this example file).\nCreate a new folder called RShiny_flux_plots_ini within &lt;projectPath&gt;/Database/Calculation_Procedures/ and save your Excel site coordinates file within this new folder.\nNext, download this R-Shiny ini file example:\nExample R-Shiny ini file\nEdit the ini file, renaming the two paths at the top of the file, to (a) the Biomet.net library and (b) your Database folder, so they match your local computer.\nSelect which level(s) of data you would like to plot. You can plot ThirdStage data by specifying level &lt;- \"Clean/ThirdStage\", or multiple levels of data using the concatenate command, e.g., level &lt;- c(\"Clean/ThirdStage\",\"Met\").\nAs with the Excel sheet, rename the ini file to include your specific Project Name and save it in under &lt;projectPath&gt;/Database/Calculation_Procedures/RShiny_flux_plots_ini/ (same location as the Excel file).\nNext, copy the code block below to a new R script, again saving it in your new RShiny_flux_plots_ini folder. You can name it as you wish; in this example it is named run_RShiny.R. Edit the filepaths at the top of your new script, as with your INI file, and also enter your RShiny ini filename. Alternatively, you can run these lines of code on the command line, but you must remember to edit the file paths and ini filename.\n # Edit these two file paths to point to biomet.net library and your database directory\n biomet_dir &lt;- \"/Users/rosie/Documents/Micromet/Biomet.net\"  # path to Biomet.net directory \n main_dir &lt;- \"/Users/rosie/Documents/Micromet/Projects/My_Micromet/Database\"   # database path \n\n # Edit to match your RShiny ini filename\n ini_filename &lt;- \"TUT_RShiny_ini.R\"\n\n source(file.path(main_dir,\"Calculation_Procedures/RShiny_flux_plots_ini\",ini_filename))\n source(file.path(biomet_dir,\"R/RShiny_flux_plots/load_save_data.R\"))\n shiny::runApp(file.path(biomet_dir,\"R/RShiny_flux_plots/RShiny_flux_plots.R\"))\nYou can now run the R-Shiny app by running your new script (run_RShiny.R), or as mentioned in step 9, you can run the individual lines of code within the R console. Importantly, remember to change the file paths to match those on your local computer.\n\n\nNote: if you get an error installing the package imager, you may need to try:\nFor imager, you may need to download quartz to be able to run this library properly (https://www.xquartz.org) & then use install.packages(\"igraph\", type=\"binary\")",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization",
      "6.2 &nbsp; R-Shiny App"
    ]
  },
  {
    "objectID": "PipelineDocumentation/6_2_RShiny_App.html#r-shiny-app-for-viewing-data-in-real-time",
    "href": "PipelineDocumentation/6_2_RShiny_App.html#r-shiny-app-for-viewing-data-in-real-time",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "We recommend using this R-Shiny app for viewing your data in real-time, and if you wish to display your data publicly.\n\n\nInstructions for using the RShiny_flux_plots.R app (under /Biomet.net/R/RShiny_flux_plots/). To run the R-Shiny app, either locally or on an R-Shiny server, you will need to download, edit, rename and save two files.\n\nDownload the Excel sheet below to input your site coordinates (you can include multiple sites, each as a separate line):\nExample site coordinate file\n\nNote that the standard meridian is equal to your [UTC offset (without daylight savings) x 15]. For example, Vancouver is UTC-8 so the standard meridian is -120.\nAlso, latitude and longitude are in decimal degrees, and negative for West and South coordinates.\n\nRename the Excel file to include your specific Project Name (in place of “TUT” in this example file).\nCreate a new folder called RShiny_flux_plots_ini within &lt;projectPath&gt;/Database/Calculation_Procedures/ and save your Excel site coordinates file within this new folder.\nNext, download this R-Shiny ini file example:\nExample R-Shiny ini file\nEdit the ini file, renaming the two paths at the top of the file, to (a) the Biomet.net library and (b) your Database folder, so they match your local computer.\nSelect which level(s) of data you would like to plot. You can plot ThirdStage data by specifying level &lt;- \"Clean/ThirdStage\", or multiple levels of data using the concatenate command, e.g., level &lt;- c(\"Clean/ThirdStage\",\"Met\").\nAs with the Excel sheet, rename the ini file to include your specific Project Name and save it in under &lt;projectPath&gt;/Database/Calculation_Procedures/RShiny_flux_plots_ini/ (same location as the Excel file).\nNext, copy the code block below to a new R script, again saving it in your new RShiny_flux_plots_ini folder. You can name it as you wish; in this example it is named run_RShiny.R. Edit the filepaths at the top of your new script, as with your INI file, and also enter your RShiny ini filename. Alternatively, you can run these lines of code on the command line, but you must remember to edit the file paths and ini filename.\n # Edit these two file paths to point to biomet.net library and your database directory\n biomet_dir &lt;- \"/Users/rosie/Documents/Micromet/Biomet.net\"  # path to Biomet.net directory \n main_dir &lt;- \"/Users/rosie/Documents/Micromet/Projects/My_Micromet/Database\"   # database path \n\n # Edit to match your RShiny ini filename\n ini_filename &lt;- \"TUT_RShiny_ini.R\"\n\n source(file.path(main_dir,\"Calculation_Procedures/RShiny_flux_plots_ini\",ini_filename))\n source(file.path(biomet_dir,\"R/RShiny_flux_plots/load_save_data.R\"))\n shiny::runApp(file.path(biomet_dir,\"R/RShiny_flux_plots/RShiny_flux_plots.R\"))\nYou can now run the R-Shiny app by running your new script (run_RShiny.R), or as mentioned in step 9, you can run the individual lines of code within the R console. Importantly, remember to change the file paths to match those on your local computer.\n\n\nNote: if you get an error installing the package imager, you may need to try:\nFor imager, you may need to download quartz to be able to run this library properly (https://www.xquartz.org) & then use install.packages(\"igraph\", type=\"binary\")",
    "crumbs": [
      "Pipeline Documentation",
      "6. Data Visualization",
      "6.2 &nbsp; R-Shiny App"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_Troubleshooting_FAQ.html",
    "href": "PipelineDocumentation/7_Troubleshooting_FAQ.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nTroubleshooting: some general tips\nFAQ\nSpecial Cases\n\nThis section presents some general troubleshooting tips outlining things to check to make sure the pipeline scripts will run smoothly. There are also some FAQ, and some guidance on special cases and how to handle them.\nNote that this section is a work in progress and will continue to be updated as the pipeline is further developed and tested.\n\n\n\n\n\n\nBefore running data cleaning, first make sure you have the most recent version of the Biomet.net library (see section 2.2 for details). The code is updated regularly and you should be updating your local repository at least once per week. If you downloaded the Biomet.net library (rather than cloned it), make sure that you renamed it exactly as Biomet.net since the download includes “main” in the folder name.\nWhen running data cleaning (any stage), pay very close attention to the output on your screen. It tells you when everything runs smoothly, and if something goes wrong, in most cases it will be informative as to why and whereabouts things went wrong.\nAvoid having extra white space in your INI file between [TRACE] and [END]. Each parameter must be defined on a new line, with no wraparound. We recommend using a text editor that has line numbers (such as VS Code) to help you avoid and/or diagnose this issue.\n\n\n\n\n\n\n\n\nI have multiple files containing data from one site, e.g., daily, monthly, or annual files. How do I create one database from all my files?\n\nSee section 5.2: subsection “Create Database from Multiple Input Files and Updates for Continuous Operational Sites”.\n\nI have multiple flux sites. Once I’ve added one site and created my database with data from that site (and cleaned it, etc.), what are the steps to move on to my next site?\n\nSee section …\n\nHow do I incorporate data from other sources such as nearby climate stations, e.g., for gap-filling, into my database?\n\nSee section 5.2: subsection “Create Database Using Data from Canadian Meteorological Stations”.\n\nWhen I run the third stage cleaning, why does it finish so quickly (less than a minute), and/or why is there is no data output in the Database directory?\n\nCheck the log file that is produced automatically when running the third stage (SITEID1_ThirdStageCleaning.log). It is informative and will usually tell you the issue; it is located here:\n\nScreenshot showing the location of the third stage log file.\nCheck that all R packages are installed and loaded. Refer to the code provided in section 2.5 for this.\nCheck that after you downloaded and unzipped the Biomet.net library, you renamed it from Biomet.net-main to Biomet.net.\n\n\n\n\n\n\n\n\nThis section outlines some recurring special cases and how to deal with them:\n\nThere is a variable defined in an include INI file that I have no raw input data for.\nIn this case, you can add the following code to the global variables section of your first stage INI file. Remember to change the name of the “dummyVariable” to match the name of the variable you do not have.\n %--&gt;Avoiding errors due to missing input files \n dateRangeNoData = [datenum(1900,1,1) datenum(1900,12,31)]\n globalVars.Trace.dummyVariable.inputFileName_dates = dateRangeNoData\nE.g., if you are running data cleaning for the year 2023, this code essentially tells the pipeline that for this “dummyVariable”, no data exists for 2023 (and only exists for the year 1900), and the program will continue smoothly with no errors.\nI am using an earlier version of Matlab than 2023b, and I’m getting an error when running the create_TAB_ProjectFolders function. How do I fix this?\nDownload this zip file, unzip, and put the contents of the unzipped directory within your own project directory; make sure your directory structure looks like figure 4.1 in section 4.\nThe gitclone Matlab function is used within the create_TAB_ProjectFolders function to transfer (clone) the directory structure and files within. However, gitclone was only added to Matlab 2023b, so you need to download this project directory structure directly.\nIf it is an error related to gitclone, the error will occur on the line that gitclone is called, so you can check this in the error message, e.g., it may look like this:\nError: File: create_TAB_ProjectFolders.m Line: 66 Column: 32\n Incorrect use of '=' operator. To assign a value to a variable, use '='. To compare values for equality, use '=='.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Troubleshooting and FAQ"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_Troubleshooting_FAQ.html#troubleshooting-faq-and-special-cases",
    "href": "PipelineDocumentation/7_Troubleshooting_FAQ.html#troubleshooting-faq-and-special-cases",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "On this page:\n\nTroubleshooting: some general tips\nFAQ\nSpecial Cases\n\nThis section presents some general troubleshooting tips outlining things to check to make sure the pipeline scripts will run smoothly. There are also some FAQ, and some guidance on special cases and how to handle them.\nNote that this section is a work in progress and will continue to be updated as the pipeline is further developed and tested.\n\n\n\n\n\n\nBefore running data cleaning, first make sure you have the most recent version of the Biomet.net library (see section 2.2 for details). The code is updated regularly and you should be updating your local repository at least once per week. If you downloaded the Biomet.net library (rather than cloned it), make sure that you renamed it exactly as Biomet.net since the download includes “main” in the folder name.\nWhen running data cleaning (any stage), pay very close attention to the output on your screen. It tells you when everything runs smoothly, and if something goes wrong, in most cases it will be informative as to why and whereabouts things went wrong.\nAvoid having extra white space in your INI file between [TRACE] and [END]. Each parameter must be defined on a new line, with no wraparound. We recommend using a text editor that has line numbers (such as VS Code) to help you avoid and/or diagnose this issue.\n\n\n\n\n\n\n\n\nI have multiple files containing data from one site, e.g., daily, monthly, or annual files. How do I create one database from all my files?\n\nSee section 5.2: subsection “Create Database from Multiple Input Files and Updates for Continuous Operational Sites”.\n\nI have multiple flux sites. Once I’ve added one site and created my database with data from that site (and cleaned it, etc.), what are the steps to move on to my next site?\n\nSee section …\n\nHow do I incorporate data from other sources such as nearby climate stations, e.g., for gap-filling, into my database?\n\nSee section 5.2: subsection “Create Database Using Data from Canadian Meteorological Stations”.\n\nWhen I run the third stage cleaning, why does it finish so quickly (less than a minute), and/or why is there is no data output in the Database directory?\n\nCheck the log file that is produced automatically when running the third stage (SITEID1_ThirdStageCleaning.log). It is informative and will usually tell you the issue; it is located here:\n\nScreenshot showing the location of the third stage log file.\nCheck that all R packages are installed and loaded. Refer to the code provided in section 2.5 for this.\nCheck that after you downloaded and unzipped the Biomet.net library, you renamed it from Biomet.net-main to Biomet.net.\n\n\n\n\n\n\n\n\nThis section outlines some recurring special cases and how to deal with them:\n\nThere is a variable defined in an include INI file that I have no raw input data for.\nIn this case, you can add the following code to the global variables section of your first stage INI file. Remember to change the name of the “dummyVariable” to match the name of the variable you do not have.\n %--&gt;Avoiding errors due to missing input files \n dateRangeNoData = [datenum(1900,1,1) datenum(1900,12,31)]\n globalVars.Trace.dummyVariable.inputFileName_dates = dateRangeNoData\nE.g., if you are running data cleaning for the year 2023, this code essentially tells the pipeline that for this “dummyVariable”, no data exists for 2023 (and only exists for the year 1900), and the program will continue smoothly with no errors.\nI am using an earlier version of Matlab than 2023b, and I’m getting an error when running the create_TAB_ProjectFolders function. How do I fix this?\nDownload this zip file, unzip, and put the contents of the unzipped directory within your own project directory; make sure your directory structure looks like figure 4.1 in section 4.\nThe gitclone Matlab function is used within the create_TAB_ProjectFolders function to transfer (clone) the directory structure and files within. However, gitclone was only added to Matlab 2023b, so you need to download this project directory structure directly.\nIf it is an error related to gitclone, the error will occur on the line that gitclone is called, so you can check this in the error message, e.g., it may look like this:\nError: File: create_TAB_ProjectFolders.m Line: 66 Column: 32\n Incorrect use of '=' operator. To assign a value to a variable, use '='. To compare values for equality, use '=='.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Troubleshooting and FAQ"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_5_Quick_Start_Third_Stage_Cleaning_And_Ameriflux_Output.html",
    "href": "PipelineDocumentation/4_5_Quick_Start_Third_Stage_Cleaning_And_Ameriflux_Output.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The third stage cleaning generally requires the least amount of work by the user, but is usually the most computationally intensive stage, as it includes running models for gap-filling fluxes and flux partitioning. The given example assumes you have already completed first and second stage cleaning for one site.\nOn this page:\n\nThird stage cleaning instructions\nThird stage flux variable definitions\nCreating Ameriflux output\n\n\n\n\n\n\n\nOpen your site-specific SITEID1_config.yml for editing (figure 4.5A):\n\nFigure 4.5A. Directory tree showing location of third stage custom YAML file that must be copied (green highlighted text) and edited (yellow highlighted text).\nAt the top of your site-specific configuration file (i.e., SITEID1_config.yml), input the site ID, the year that measurements at the site began, and the metadata for your site (figure 4.5B; yellow highlighted text). The northOffset can be found in your site GHG biomet file and this information is used for filtering data by wind direction, to comply with eddy-covariance measurement standards.\nThe peach highlighted text indicates parts of the file that should be checked in relation to your site data:\n\nFirst, check that the flux variables listed are actually measured at your site, otherwise replace the right hand side with NULL.\nSecond, check the met variable names for used for gap-filling are named the same as those output by your second stage cleaning.\nThird, if you wish to, you can edit the wind sector and precipitation filter values, but we recommend the values already written in the file.\n\n\nFigure 4.5B. Third stage site-specific custom YAML file showing which fields to edit in yellow highlighted text, and optional text to check or edit in peach highlighting.\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the Biomet.net library and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs; these site-specific settings will overwrite those in the global_config.yml if they are also defined there. More information on this is provided in the full documentation in section 5.5.\n\n\n\nNext, test the third stage data cleaning in Matlab; remember that it can take a lot longer to run than first and second stages. Note that the cleaning stage argument for third stage cleaning is 7 (not  3; this is a legacy artifact), as follows:\n fr_automated_cleaning(yearIn,siteID,7)  % third stage\nThe output will appear in two directories: ThirdStage and ThirdStage_Default_Ustar within the Clean directory, where the second stage output is; again, we recommend that you inspect this data using the visualization tools. \n\n\n\n\n\n\n\nThe standalone flux variable names (i.e., FCH4, FC, H, LE) are copied directly from the second stage output, then wind sector and precipitation filters are applied to comply with eddy-covariance measurement theory, with no change to the variable name. For the variable names with suffixes following the flux variables, these suffixes represent different algorithms that we have applied sequentially, in the order that they appear. This description only provides definitions, and more detailed information on each output variable is provided in the full documentation in section 5.5.\n\n\n\n\n\n\n\nSuffix\nDefinition\n\n\n\n\nNo suffix\nStandard cleaning (wind direction and precipitation filtering)\n\n\n_PI_SC\nStorage flux Correction\n\n\n_PI_SC_JSZ\nPlus z-score filter\n\n\n_PI_SC_JSZ_MAD\nPlus Median of Absolute Deviation (about the median) filter\n\n\n_PI_SC_JSZ_MAD_RP\nPlus REddyProc applied (u-star filtering)\n\n\n\nThis link provides descriptions of suffixes applied to REddyProc output, e.g., _uStar, U95, _orig, and _f.\n\n\n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux:\nfr_automated_cleaning(yearIn,SITEID,8)  % Ameriflux CSV output\nThe output will appear in an Ameriflux directory within the Clean directory, where the second and third stage output is.\nNote that you can run all stages at once (or a subset, provided the previous stages to the subset have already been run, i.e., the data exists):\nfr_automated_cleaning(yearIn,SITEID,[1 2 7 8])  % all three stages plus Ameriflux output",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.5 &nbsp; Quick Start Tutorial: Third Stage Cleaning and Converting to Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_5_Quick_Start_Third_Stage_Cleaning_And_Ameriflux_Output.html#quick-start-tutorial-third-stage-cleaning-and-converting-to-ameriflux-output",
    "href": "PipelineDocumentation/4_5_Quick_Start_Third_Stage_Cleaning_And_Ameriflux_Output.html#quick-start-tutorial-third-stage-cleaning-and-converting-to-ameriflux-output",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "The third stage cleaning generally requires the least amount of work by the user, but is usually the most computationally intensive stage, as it includes running models for gap-filling fluxes and flux partitioning. The given example assumes you have already completed first and second stage cleaning for one site.\nOn this page:\n\nThird stage cleaning instructions\nThird stage flux variable definitions\nCreating Ameriflux output\n\n\n\n\n\n\n\nOpen your site-specific SITEID1_config.yml for editing (figure 4.5A):\n\nFigure 4.5A. Directory tree showing location of third stage custom YAML file that must be copied (green highlighted text) and edited (yellow highlighted text).\nAt the top of your site-specific configuration file (i.e., SITEID1_config.yml), input the site ID, the year that measurements at the site began, and the metadata for your site (figure 4.5B; yellow highlighted text). The northOffset can be found in your site GHG biomet file and this information is used for filtering data by wind direction, to comply with eddy-covariance measurement standards.\nThe peach highlighted text indicates parts of the file that should be checked in relation to your site data:\n\nFirst, check that the flux variables listed are actually measured at your site, otherwise replace the right hand side with NULL.\nSecond, check the met variable names for used for gap-filling are named the same as those output by your second stage cleaning.\nThird, if you wish to, you can edit the wind sector and precipitation filter values, but we recommend the values already written in the file.\n\n\nFigure 4.5B. Third stage site-specific custom YAML file showing which fields to edit in yellow highlighted text, and optional text to check or edit in peach highlighting.\nThe main configuration file (global_config.yml) for running third stage cleaning is located in the Biomet.net library and generally speaking this should not be edited. The custom SITEID1_config.yml file can be used to add parameters/inputs; these site-specific settings will overwrite those in the global_config.yml if they are also defined there. More information on this is provided in the full documentation in section 5.5.\n\n\n\nNext, test the third stage data cleaning in Matlab; remember that it can take a lot longer to run than first and second stages. Note that the cleaning stage argument for third stage cleaning is 7 (not  3; this is a legacy artifact), as follows:\n fr_automated_cleaning(yearIn,siteID,7)  % third stage\nThe output will appear in two directories: ThirdStage and ThirdStage_Default_Ustar within the Clean directory, where the second stage output is; again, we recommend that you inspect this data using the visualization tools. \n\n\n\n\n\n\n\nThe standalone flux variable names (i.e., FCH4, FC, H, LE) are copied directly from the second stage output, then wind sector and precipitation filters are applied to comply with eddy-covariance measurement theory, with no change to the variable name. For the variable names with suffixes following the flux variables, these suffixes represent different algorithms that we have applied sequentially, in the order that they appear. This description only provides definitions, and more detailed information on each output variable is provided in the full documentation in section 5.5.\n\n\n\n\n\n\n\nSuffix\nDefinition\n\n\n\n\nNo suffix\nStandard cleaning (wind direction and precipitation filtering)\n\n\n_PI_SC\nStorage flux Correction\n\n\n_PI_SC_JSZ\nPlus z-score filter\n\n\n_PI_SC_JSZ_MAD\nPlus Median of Absolute Deviation (about the median) filter\n\n\n_PI_SC_JSZ_MAD_RP\nPlus REddyProc applied (u-star filtering)\n\n\n\nThis link provides descriptions of suffixes applied to REddyProc output, e.g., _uStar, U95, _orig, and _f.\n\n\n\n\n\n\nFinally, once you have inspected your clean data and are happy with your INI files, you can output the data to a CSV file formatted for submission to Ameriflux:\nfr_automated_cleaning(yearIn,SITEID,8)  % Ameriflux CSV output\nThe output will appear in an Ameriflux directory within the Clean directory, where the second and third stage output is.\nNote that you can run all stages at once (or a subset, provided the previous stages to the subset have already been run, i.e., the data exists):\nfr_automated_cleaning(yearIn,SITEID,[1 2 7 8])  % all three stages plus Ameriflux output",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.5 &nbsp; Quick Start Tutorial: Third Stage Cleaning and Converting to Ameriflux Output"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_1_Recommended_Software_Versions.html",
    "href": "PipelineDocumentation/7_1_Recommended_Software_Versions.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "All three stages of data cleaning (plus conversion to Ameriflux output if needed) can be run from Matlab. Our plotApp for data visualization and analysis during data cleaning is run from Matlab too.\nNote that the third stage Matlab script invokes an R-script, which you will also need installed on your local computer. \nAdditionally, our R-Shiny data visualization tool uses R/R-Studio, and we recommend this app for viewing your data in real-time.\nThe scripts and tools that run the data cleaning are kept in a Git repository (called Biomet.net). These can be downloaded without having Git installed.\nOptionally, you can install Git and create your own GitHub account, in case you wish to contribute code to the Biomet.net library. \nSome users may also need Python installed, for example, if you are processing high-frequency data. \n\n\n\n\nUpdated on: 5 November 2024\n\n\n\n\n\n\n\nSoftware\nRecommended version as of above date\n\n\n\n\nMatlab\n2024a (at least 2023b)\n\n\nR\nv4.3.3\n\n\nRStudio\n2024.09.0-375\n\n\nPython\n3.9.6 for Mac\n\n\nGit\nLatest available \n\n\n\nYour computer/system administrators should be able to help you with all these installations if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Troubleshooting and FAQ",
      "7.1 &nbsp; Recommended Software Versions"
    ]
  },
  {
    "objectID": "PipelineDocumentation/7_1_Recommended_Software_Versions.html#software-current-recommended-versions",
    "href": "PipelineDocumentation/7_1_Recommended_Software_Versions.html#software-current-recommended-versions",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "All three stages of data cleaning (plus conversion to Ameriflux output if needed) can be run from Matlab. Our plotApp for data visualization and analysis during data cleaning is run from Matlab too.\nNote that the third stage Matlab script invokes an R-script, which you will also need installed on your local computer. \nAdditionally, our R-Shiny data visualization tool uses R/R-Studio, and we recommend this app for viewing your data in real-time.\nThe scripts and tools that run the data cleaning are kept in a Git repository (called Biomet.net). These can be downloaded without having Git installed.\nOptionally, you can install Git and create your own GitHub account, in case you wish to contribute code to the Biomet.net library. \nSome users may also need Python installed, for example, if you are processing high-frequency data. \n\n\n\n\nUpdated on: 5 November 2024\n\n\n\n\n\n\n\nSoftware\nRecommended version as of above date\n\n\n\n\nMatlab\n2024a (at least 2023b)\n\n\nR\nv4.3.3\n\n\nRStudio\n2024.09.0-375\n\n\nPython\n3.9.6 for Mac\n\n\nGit\nLatest available \n\n\n\nYour computer/system administrators should be able to help you with all these installations if necessary.",
    "crumbs": [
      "Pipeline Documentation",
      "7. Troubleshooting and FAQ",
      "7.1 &nbsp; Recommended Software Versions"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_2_Quick_Start_Create_Database_Visualize_Contents.html",
    "href": "PipelineDocumentation/4_2_Quick_Start_Create_Database_Visualize_Contents.html",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a generic, quick-start example that converts (1) EddyPro raw data output, and (2) Campbell Scientific TOA5 output, to a formatted database ready to be processed using standardized libraries (i.e., Biomet.net). We also present some examples of how to quickly check the contents of your new database.\nOn this page:\n\nSample data for tutorial\nCreate database\nVisualize and inspect database\nSample code\n\n\n\n\n\nIf you wish to use our sample raw flux and met data to work through the example, click the following “Sample_Data_For_Tutorial” link, unzip the contents, and keep them handy:\nSample_Data_For_Tutorial\n\n\n\n\n\n\nAs previously mentioned, we will follow an example that focuses on flux-related EddyPro and met-related Campbell Scientific output data. If your data has a different format, we recommend first carrying out the steps below using our sample data to see how the process works. Section 5.2 provides some examples of functions to deal with different data formats.\n\n\nFirst, in your newly created Sites directory, within the relevant SITEID directory, create a new Flux folder (figure 4.2A).\n\nFigure 4.2A. Directory tree showing file path to the location where raw flux data for site with SITEID1 should be stored.\nCopy the EddyPro raw output data to this Flux folder. This data will remain untouched so that you always have a local copy of the original data.\nNow, create a Met folder within the same SITEID directory (SITEID1 in this example), and copy the Campbell Scientific TOA5 data for this site to this Met folder.\nIn your &lt;projectPath&gt;/Matlab folder, create a new “main” Matlab file that will act as a “do-it-all” script. You can name this file however you like; we advise making it meaningful and including the word “Main”. The example given here (figure 4.2B) is named DataCleaning_Main.m (you can make your filename less generic) — it will first create the database, and later you can add the code for data cleaning. This script can be copied section by section from the code block at the bottom of this page.\n\nFigure 4.2B. Matlab code to create database from raw EddyPro output and Campbell Scientific logger meteorological data. Yellow highlighted text should be edited.\nNext, run your “Main” Matlab program; you should see some data output in your Database directory. Data is grouped by year, then by site, then by data type, e.g., Flux or Met (figure 4.2C; Met data not shown here).\n\nFigure 4.2C. Directory tree showing file path to output in Database directory following database conversion.\n\nYour data is now in a format ready for cleaning using the pipeline.\n\n\n\n\n\n\nHere are some quick tips to inspect the data in your newly created database, all within Matlab:\n\nplotApp function:\nSimply type plotApp on the Matlab command line, and it will open an app that can compare traces from the same and different cleaning stages (once you have completed those), databases, and also produce statistical plots and outputs. More details of this and other visualization tools are described at length in section 7.\ngui_Browse_folder function:\n pth = biomet_path(yearIn, 'SITEID1', 'Flux')  % define path to folder you wish to browse\n gui_Browse_Folder(pth)\n \nThis function opens a Matlab app that looks in the Flux folder for SITEID1 for a specific year (as defined in the biomet_path function input parameters) and plots each variable in turn. You can scroll through or use the dropdown in order to check that your data looks reasonable and as expected.\nLoad one trace, e.g., co2_mixing_ratio and plot it.\n %% Load one trace and plot it\n pth = biomet_path(yearIn,'SITEID1','Flux');   \n tv = read_bor(fullfile(pth,'clean_tv'),8);      % load the time vector (Matlab's datenum format)\n tv_dt = datetime(tv,'convertfrom','datenum');   % convert to Matlab's datetime object\n x = read_bor(fullfile(pth,'co2_mixing_ratio')); % load co2_mixing_ratio from SITEID1/Flux folder\n plot(tv_dt,x)                                   % plot data\n grid on;zoom on\n \n\n\n\n\n\nDataCleaning_Main.m template script for copying one section at a time (see Figure 4.2B for necessary edits, highlighted yellow):\n%% Main function for My_Micromet data processing\n% Created by &lt;author&gt; on &lt;date&gt;\n% \n% ============================\n% Setup the project and siteID\nprojectPath = '/Users/&lt;username&gt;/Project/My_Micromet';\nstructProject=set_TAB_project(projectPath);\nsiteID = 'SITEID1';\n\n% Create database from raw data\n%% Flux data from EddyPro output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Flux','MY_EDDYPRO_OUTPUT.csv');\n\n% Read the file \noptionsFileRead.flagFileType = 'fulloutput';    % select fulloutput, biomet, or summary\n[~, ~,tv,outStruct] = fr_read_EddyPro_file(fileName,[],[],optionsFileRead);\n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Flux'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1); \n\n%% Met data from Campbell Scientific TOA5 output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Met','MY_CS_TOA5_OUTPUT.dat');\n\n% Read the file \n[~,~,~,outStruct] = fr_read_TOA5_file(fileName); \n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Met'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1);",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.2 &nbsp; Quick Start Tutorial: Create Database from Raw Data and Visualize Contents"
    ]
  },
  {
    "objectID": "PipelineDocumentation/4_2_Quick_Start_Create_Database_Visualize_Contents.html#quick-start-tutorial-create-database-from-raw-data-and-visualize-contents",
    "href": "PipelineDocumentation/4_2_Quick_Start_Create_Database_Visualize_Contents.html#quick-start-tutorial-create-database-from-raw-data-and-visualize-contents",
    "title": "EcoFlux Lab",
    "section": "",
    "text": "This section provides a generic, quick-start example that converts (1) EddyPro raw data output, and (2) Campbell Scientific TOA5 output, to a formatted database ready to be processed using standardized libraries (i.e., Biomet.net). We also present some examples of how to quickly check the contents of your new database.\nOn this page:\n\nSample data for tutorial\nCreate database\nVisualize and inspect database\nSample code\n\n\n\n\n\nIf you wish to use our sample raw flux and met data to work through the example, click the following “Sample_Data_For_Tutorial” link, unzip the contents, and keep them handy:\nSample_Data_For_Tutorial\n\n\n\n\n\n\nAs previously mentioned, we will follow an example that focuses on flux-related EddyPro and met-related Campbell Scientific output data. If your data has a different format, we recommend first carrying out the steps below using our sample data to see how the process works. Section 5.2 provides some examples of functions to deal with different data formats.\n\n\nFirst, in your newly created Sites directory, within the relevant SITEID directory, create a new Flux folder (figure 4.2A).\n\nFigure 4.2A. Directory tree showing file path to the location where raw flux data for site with SITEID1 should be stored.\nCopy the EddyPro raw output data to this Flux folder. This data will remain untouched so that you always have a local copy of the original data.\nNow, create a Met folder within the same SITEID directory (SITEID1 in this example), and copy the Campbell Scientific TOA5 data for this site to this Met folder.\nIn your &lt;projectPath&gt;/Matlab folder, create a new “main” Matlab file that will act as a “do-it-all” script. You can name this file however you like; we advise making it meaningful and including the word “Main”. The example given here (figure 4.2B) is named DataCleaning_Main.m (you can make your filename less generic) — it will first create the database, and later you can add the code for data cleaning. This script can be copied section by section from the code block at the bottom of this page.\n\nFigure 4.2B. Matlab code to create database from raw EddyPro output and Campbell Scientific logger meteorological data. Yellow highlighted text should be edited.\nNext, run your “Main” Matlab program; you should see some data output in your Database directory. Data is grouped by year, then by site, then by data type, e.g., Flux or Met (figure 4.2C; Met data not shown here).\n\nFigure 4.2C. Directory tree showing file path to output in Database directory following database conversion.\n\nYour data is now in a format ready for cleaning using the pipeline.\n\n\n\n\n\n\nHere are some quick tips to inspect the data in your newly created database, all within Matlab:\n\nplotApp function:\nSimply type plotApp on the Matlab command line, and it will open an app that can compare traces from the same and different cleaning stages (once you have completed those), databases, and also produce statistical plots and outputs. More details of this and other visualization tools are described at length in section 7.\ngui_Browse_folder function:\n pth = biomet_path(yearIn, 'SITEID1', 'Flux')  % define path to folder you wish to browse\n gui_Browse_Folder(pth)\n \nThis function opens a Matlab app that looks in the Flux folder for SITEID1 for a specific year (as defined in the biomet_path function input parameters) and plots each variable in turn. You can scroll through or use the dropdown in order to check that your data looks reasonable and as expected.\nLoad one trace, e.g., co2_mixing_ratio and plot it.\n %% Load one trace and plot it\n pth = biomet_path(yearIn,'SITEID1','Flux');   \n tv = read_bor(fullfile(pth,'clean_tv'),8);      % load the time vector (Matlab's datenum format)\n tv_dt = datetime(tv,'convertfrom','datenum');   % convert to Matlab's datetime object\n x = read_bor(fullfile(pth,'co2_mixing_ratio')); % load co2_mixing_ratio from SITEID1/Flux folder\n plot(tv_dt,x)                                   % plot data\n grid on;zoom on\n \n\n\n\n\n\nDataCleaning_Main.m template script for copying one section at a time (see Figure 4.2B for necessary edits, highlighted yellow):\n%% Main function for My_Micromet data processing\n% Created by &lt;author&gt; on &lt;date&gt;\n% \n% ============================\n% Setup the project and siteID\nprojectPath = '/Users/&lt;username&gt;/Project/My_Micromet';\nstructProject=set_TAB_project(projectPath);\nsiteID = 'SITEID1';\n\n% Create database from raw data\n%% Flux data from EddyPro output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Flux','MY_EDDYPRO_OUTPUT.csv');\n\n% Read the file \noptionsFileRead.flagFileType = 'fulloutput';    % select fulloutput, biomet, or summary\n[~, ~,tv,outStruct] = fr_read_EddyPro_file(fileName,[],[],optionsFileRead);\n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Flux'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1); \n\n%% Met data from Campbell Scientific TOA5 output files\n%\n% Input file name\nfileName = fullfile(structProject.sitesPath,siteID,'Met','MY_CS_TOA5_OUTPUT.dat');\n\n% Read the file \n[~,~,~,outStruct] = fr_read_TOA5_file(fileName); \n\n% set database path \ndatabasePath = fullfile(db_pth_root,'yyyy',siteID,'Met'); \n\n% Convert outStruct into database \nmissingPointValue = NaN; \ntimeUnit= '30MIN'; \nstructType = 1; \ndb_struct2database(outStruct,databasePath,0,[],timeUnit,missingPointValue,structType,1);",
    "crumbs": [
      "Pipeline Documentation",
      "4. Quick Start Tutorial - Recommended for First-Time Users",
      "4.2 &nbsp; Quick Start Tutorial: Create Database from Raw Data and Visualize Contents"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the EcoFlux Lab!",
    "section": "",
    "text": "Contact Information Dr. Sara Knox sara.knox@mcgill.ca\n\n\n\n\n\nOur research group explores the physical, biological and chemical processes that control trace gas, water and energy fluxes between the land surface and the atmosphere. We investigate how land‑atmosphere exchanges of greenhouse gas fluxes respond to a changing climate and disturbances, and how we can modify land management practices for climate change adaptation and mitigation. We combine micrometeorological measurements with remote sensing and modelling to understand soil-plant-atmosphere interactions across a range of spatial and temporal scales. We collaborate with a broad group of researchers and institutions to help inform and advance climate policy.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Russell, S. J., Windham-Myers, L., Stuart-Haëntjens, E. J., Bergamaschi, B. A., Anderson, F., Oikawa, P., & Knox, S. H. (2023). Increased salinity decreases annual gross primary productivity at a Northern California brackish tidal marsh. Environmental Research Letters, 18(3), 034045. https://doi.org/10.1088/1748-9326/acbbdf\n\n\nQuan, N., Lee, S.-C., Chopra, C., Nesic, Z., Porto, P., Pow, P., et al. (2023). Estimating Net Carbon and Greenhouse Gas Balances of Potato and Pea Crops on a Conventional Farm in Western Canada. Journal of Geophysical Research: Biogeosciences, 128(3), e2022JG007113. https://doi.org/10.1029/2022JG007113\n\n\nLee, S.-C., Knox, S. H., McKendry, I., & Black, T. A. (2022). Biogeochemical and biophysical responses to episodes of wildfire smoke from natural ecosystems in southwestern British Columbia, Canada. Atmospheric Chemistry and Physics, 22(4), 2333–2349. https://doi.org/10.5194/acp-22-2333-2022\n\n\nNyberg, M., Black, T. A., Ketler, R., Lee, S.-C., Johnson, M., Merkens, M., et al. (2022). Impacts of Active Versus Passive Re-Wetting on the Carbon Balance of a Previously Drained Bog. Journal of Geophysical Research: Biogeosciences, 127(9), e2022JG006881. https://doi.org/10.1029/2022JG006881\n\n\nZhang, Z., Poulter, B., Knox, S. H., Stavert, A., McNicol, G., Fluet-Chouinard, E., et al. (2021). Anthropogenic emission is the main contributor to the rise of atmospheric methane during 1993–2017. National Science Review, 9(5). https://doi.org/10.1093/nsr/nwab200\n\n\nFisher, J. B., Keenan, T. F., Buechner, C., Shirkey, G., Perez-Quezada, J. F., Knox, S. H., et al. (2021). Once Upon a Time, in AmeriFlux. Journal of Geophysical Research: Biogeosciences, 126(1), e2020JG006148. https://doi.org/10.1029/2020JG006148\n\n\nDelwiche, K. B., Knox, S. H., Malhotra, A., Fluet-Chouinard, E., McNicol, G., Feron, S., et al. (2021). FLUXNET-CH₄: a global, multi-ecosystem dataset and analysis of methane seasonality from freshwater wetlands. Earth System Science Data, 13(7), 3607–3689. https://doi.org/10.5194/essd-13-3607-2021\n\n\nDronova, I., Taddeo, S., Hemes, K. S., Knox, S. H., Valach, A., Oikawa, P. Y., et al. (2021). Remotely sensed phenological heterogeneity of restored wetlands: linking vegetation structure and function. Agricultural and Forest Meteorology, 296, 108215. https://doi.org/10.1016/j.agrformet.2020.108215\n\n\nKnox, S. H., Bansal, S., McNicol, G., Schafer, K., Sturtevant, C., Ueyama, M., et al. (2021). Identifying dominant environmental predictors of freshwater wetland methane fluxes across diurnal to seasonal time scales. Global Change Biology, 27(15), 3582–3604. https://doi.org/10.1111/gcb.15661\n\n\nLee, S.-C., Black, T. A., Nyberg, M., Merkens, M., Nesic, Z., Ng, D., & Knox, S. H. (2021). Biophysical Impacts of Historical Disturbances, Restoration Strategies, and Vegetation Types in a Peatland Ecosystem. Journal of Geophysical Research: Biogeosciences, 126(10), e2021JG006532. https://doi.org/10.1029/2021JG006532\n\n\nMiller, G. J., Dronova, I., Oikawa, P. Y., Knox, S. H., Windham-Myers, L., Shahan, J., & Stuart-Haëntjens, E. (2021). The Potential of Satellite Remote Sensing Time Series to Uncover Wetland Phenology under Unique Challenges of Tidal Setting. Remote Sensing, 13(18), 3589. https://doi.org/10.3390/rs13183589\n\n\nBogard, M. J., Bergamaschi, B. A., Butman, D. E., Anderson, F., Knox, S. H., & Windham-Myers, L. (2020). Hydrologic Export Is a Major Component of Coastal Wetland Carbon Budgets. Global Biogeochemical Cycles, 34(8), e2019GB006430. https://doi.org/10.1029/2019GB006430\n\n\nKim, Y., Johnson, M. S., Knox, S. H., Black, T. A., Dalmagro, H. J., Kang, M., et al. (2019). Gap-filling approaches for eddy covariance methane fluxes: A comparison of three machine learning algorithms and a traditional method with principal component analysis. Global Change Biology, 26(3), 1499–1518. https://doi.org/10.1111/gcb.14845\n\n\nKnox, S. H., Jackson, R. B., Poulter, B., McNicol, G., Fluet-Chouinard, E., Zhang, Z., et al. (2019). FLUXNET-CH4 Synthesis Activity: Objectives, Observations, and Future Directions. Bulletin of the American Meteorological Society, 100(12), 2607–2632. https://doi.org/10.1175/BAMS-D-18-0268.1\n\n\nMcNicol, G., Knox, S. H., Guilderson, T. P., Baldocchi, D. D., & Silver, W. L. (2019). Where old meets new: An ecosystem study of methanogenesis in a reflooded agricultural peatland. Global Change Biology, 26(2), 772–785. https://doi.org/10.1111/gcb.14916",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#peer-reviewed-journal-articles",
    "href": "Publications.html#peer-reviewed-journal-articles",
    "title": "Publications",
    "section": "",
    "text": "Russell, S. J., Windham-Myers, L., Stuart-Haëntjens, E. J., Bergamaschi, B. A., Anderson, F., Oikawa, P., & Knox, S. H. (2023). Increased salinity decreases annual gross primary productivity at a Northern California brackish tidal marsh. Environmental Research Letters, 18(3), 034045. https://doi.org/10.1088/1748-9326/acbbdf\n\n\nQuan, N., Lee, S.-C., Chopra, C., Nesic, Z., Porto, P., Pow, P., et al. (2023). Estimating Net Carbon and Greenhouse Gas Balances of Potato and Pea Crops on a Conventional Farm in Western Canada. Journal of Geophysical Research: Biogeosciences, 128(3), e2022JG007113. https://doi.org/10.1029/2022JG007113\n\n\nLee, S.-C., Knox, S. H., McKendry, I., & Black, T. A. (2022). Biogeochemical and biophysical responses to episodes of wildfire smoke from natural ecosystems in southwestern British Columbia, Canada. Atmospheric Chemistry and Physics, 22(4), 2333–2349. https://doi.org/10.5194/acp-22-2333-2022\n\n\nNyberg, M., Black, T. A., Ketler, R., Lee, S.-C., Johnson, M., Merkens, M., et al. (2022). Impacts of Active Versus Passive Re-Wetting on the Carbon Balance of a Previously Drained Bog. Journal of Geophysical Research: Biogeosciences, 127(9), e2022JG006881. https://doi.org/10.1029/2022JG006881\n\n\nZhang, Z., Poulter, B., Knox, S. H., Stavert, A., McNicol, G., Fluet-Chouinard, E., et al. (2021). Anthropogenic emission is the main contributor to the rise of atmospheric methane during 1993–2017. National Science Review, 9(5). https://doi.org/10.1093/nsr/nwab200\n\n\nFisher, J. B., Keenan, T. F., Buechner, C., Shirkey, G., Perez-Quezada, J. F., Knox, S. H., et al. (2021). Once Upon a Time, in AmeriFlux. Journal of Geophysical Research: Biogeosciences, 126(1), e2020JG006148. https://doi.org/10.1029/2020JG006148\n\n\nDelwiche, K. B., Knox, S. H., Malhotra, A., Fluet-Chouinard, E., McNicol, G., Feron, S., et al. (2021). FLUXNET-CH₄: a global, multi-ecosystem dataset and analysis of methane seasonality from freshwater wetlands. Earth System Science Data, 13(7), 3607–3689. https://doi.org/10.5194/essd-13-3607-2021\n\n\nDronova, I., Taddeo, S., Hemes, K. S., Knox, S. H., Valach, A., Oikawa, P. Y., et al. (2021). Remotely sensed phenological heterogeneity of restored wetlands: linking vegetation structure and function. Agricultural and Forest Meteorology, 296, 108215. https://doi.org/10.1016/j.agrformet.2020.108215\n\n\nKnox, S. H., Bansal, S., McNicol, G., Schafer, K., Sturtevant, C., Ueyama, M., et al. (2021). Identifying dominant environmental predictors of freshwater wetland methane fluxes across diurnal to seasonal time scales. Global Change Biology, 27(15), 3582–3604. https://doi.org/10.1111/gcb.15661\n\n\nLee, S.-C., Black, T. A., Nyberg, M., Merkens, M., Nesic, Z., Ng, D., & Knox, S. H. (2021). Biophysical Impacts of Historical Disturbances, Restoration Strategies, and Vegetation Types in a Peatland Ecosystem. Journal of Geophysical Research: Biogeosciences, 126(10), e2021JG006532. https://doi.org/10.1029/2021JG006532\n\n\nMiller, G. J., Dronova, I., Oikawa, P. Y., Knox, S. H., Windham-Myers, L., Shahan, J., & Stuart-Haëntjens, E. (2021). The Potential of Satellite Remote Sensing Time Series to Uncover Wetland Phenology under Unique Challenges of Tidal Setting. Remote Sensing, 13(18), 3589. https://doi.org/10.3390/rs13183589\n\n\nBogard, M. J., Bergamaschi, B. A., Butman, D. E., Anderson, F., Knox, S. H., & Windham-Myers, L. (2020). Hydrologic Export Is a Major Component of Coastal Wetland Carbon Budgets. Global Biogeochemical Cycles, 34(8), e2019GB006430. https://doi.org/10.1029/2019GB006430\n\n\nKim, Y., Johnson, M. S., Knox, S. H., Black, T. A., Dalmagro, H. J., Kang, M., et al. (2019). Gap-filling approaches for eddy covariance methane fluxes: A comparison of three machine learning algorithms and a traditional method with principal component analysis. Global Change Biology, 26(3), 1499–1518. https://doi.org/10.1111/gcb.14845\n\n\nKnox, S. H., Jackson, R. B., Poulter, B., McNicol, G., Fluet-Chouinard, E., Zhang, Z., et al. (2019). FLUXNET-CH4 Synthesis Activity: Objectives, Observations, and Future Directions. Bulletin of the American Meteorological Society, 100(12), 2607–2632. https://doi.org/10.1175/BAMS-D-18-0268.1\n\n\nMcNicol, G., Knox, S. H., Guilderson, T. P., Baldocchi, D. D., & Silver, W. L. (2019). Where old meets new: An ecosystem study of methanogenesis in a reflooded agricultural peatland. Global Change Biology, 26(2), 772–785. https://doi.org/10.1111/gcb.14916",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#theses",
    "href": "Publications.html#theses",
    "title": "Publications",
    "section": "Theses",
    "text": "Theses\n\n\nSatriawan, T. (2022). Interannual variability of carbon dioxide (CO₂) and methane (CH₄) fluxes in a temperate bog over a 5-year period (Master’s Thesis). University of British Columbia. https://doi.org/10.14288/1.0416306\n\n\nNyberg, M. (2021). Impacts of restoration and climate variability on peatland GHG fluxes (Master’s Thesis). University of British Columbia. https://doi.org/10.14288/1.0401463\n\n\nRussell, S. J. (2021). Increased salinity decreases annual gross primary productivity of a Northern California brackish wetland (Master’s Thesis). University of British Columbia. https://doi.org/10.14288/1.0406272",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#research-talks-poster-presentations",
    "href": "Publications.html#research-talks-poster-presentations",
    "title": "Publications",
    "section": "Research Talks & Poster Presentations",
    "text": "Research Talks & Poster Presentations\n\n\n\n\n\n\n\nSkeeter, J., & Knox, S. H. (2023, April). Ongoing and Proposed Research in the Burns Bog Ecological Conservancy Area.\n\n\nKnox, S. H., & Skeeter, J. (2023, March). UBC Micrometeorology Lab Studies Review.\n\n\nLu, T.-Y., Russell, S. J., Skeeter, J., Lee, S., Oikawa, P., & Knox, S. H. (2022, December). Investigating environmental controls on carbon exchange and predicting gaseous carbon fluxes at a salt marsh in British Columbia.\n\n\nNg, D., & Knox, S. H. (2022, December). Characterizing within-footprint spatial heterogeneity of CH4 emissions in freshwater wetlands through remote sensing and Footprint-weighted Flux Maps.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2022, September). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2022, June). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2021, December). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.\n\n\nNyberg, M., Knox, S. H., Black, T., Johnson, M., Ketler, R., & Nesic, Z. (2020, December). Impacts of restoration and climate variability on peatland greenhouse gas fluxes.\n\n\nSatriawan, T., Nyberg, M., Lee, S.-C., Nesic, Z., Black, T., Johnson, M., & Knox, S. H. (2020, December). Interannual Variability of Carbon Dioxide (CO2) and Methane (CH4) Fluxes in a Rewetted Temperate Bog over a 5-Year Period.",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "OutreachNews.html",
    "href": "OutreachNews.html",
    "title": "Outreach and News",
    "section": "",
    "text": "A new short-term flux tower was installed at the Burns Bog Seedling site to monitor CO2 fluxes from a 7-year old lodge-pole pine stand that sprouted following a fire in Burns Bog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJune and I presented at the CGU Annual meeting which was held in Banff this year."
  },
  {
    "objectID": "OutreachNews.html#section",
    "href": "OutreachNews.html#section",
    "title": "Outreach and News",
    "section": "",
    "text": "A new short-term flux tower was installed at the Burns Bog Seedling site to monitor CO2 fluxes from a 7-year old lodge-pole pine stand that sprouted following a fire in Burns Bog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJune and I presented at the CGU Annual meeting which was held in Banff this year."
  },
  {
    "objectID": "OutreachNews.html#section-1",
    "href": "OutreachNews.html#section-1",
    "title": "Outreach and News",
    "section": "2022",
    "text": "2022\n\nDecember\nA recent talk I gave during the UBC IRES Seminar Series on ‘Wetlands in a changing world: processes, feedbacks, and the climate benefits of wetlands’\n\n\nNovember\nLearn more about our recently funded work looking into the role of wetlands as nature-based climate solutions. Excited to work with this great team of researchers! \nHonoured and thrilled to be named CRC in Eco-Meteorology! This is really a group accomplishment that reflects my amazing team (past and present) and all the wonderful and talented colleagues and mentors I’ve had the opportunity to worth with.\n\n\n\nSeptember\nThanks @Let’s talk science for featuring our work in your career profiles! See the accompanying interview here.\n\nMarion’s paper was featured in EOS:"
  },
  {
    "objectID": "OutreachNews.html#section-2",
    "href": "OutreachNews.html#section-2",
    "title": "Outreach and News",
    "section": "2021",
    "text": "2021\n\nJune\nCoverage of our 2021 Global Change Biology Paper:"
  },
  {
    "objectID": "OutreachNews.html#section-3",
    "href": "OutreachNews.html#section-3",
    "title": "Outreach and News",
    "section": "2019",
    "text": "2019\n\nAugust\nThe UBC Geography department highlighting our 2019 BAMS paper:\n\nAn AmeriFlux blog post highlighting our FLUXNET-CH4 work:\n\n\n\nJune\nA blog post I wrote describing life as a new faculty member:"
  },
  {
    "objectID": "OutreachNews.html#and-earlier",
    "href": "OutreachNews.html#and-earlier",
    "title": "Outreach and News",
    "section": "2018 and Earlier",
    "text": "2018 and Earlier\n\nJune (2016)\nCoverage of our work on wetland restoration in the Sacramento-San Joaquin Delta:\n\n\n\nOctober (2016)\nAn interview I did as part of STEM week at Los Altos High School in Los Altos, California:"
  }
]